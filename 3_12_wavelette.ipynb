{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f5dc684-c804-4307-8c00-7979a48fef2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pywavelets in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.8.0.76)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (8.1.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from pywavelets) (1.26.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pywavelets opencv-python matplotlib tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36150fd-08d1-4b09-9c83-d67c36335f58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total frames: 1548, FPS: 30, Resolution: 1280x720\n",
      "Training model on 1548 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:   0%|          | 0/125 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 15.72 GiB of which 181.06 MiB is free. Process 2807253 has 8.38 GiB memory in use. Process 2805731 has 7.16 GiB memory in use. Of the allocated memory 6.54 GiB is allocated by PyTorch, and 419.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 506\u001b[0m\n\u001b[1;32m    504\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/processed_video.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 506\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 482\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(video_path, output_path, mode)\u001b[0m\n\u001b[1;32m    478\u001b[0m model \u001b[38;5;241m=\u001b[39m ReflectionUNet(in_channels\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mCHANNELS)\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# Process video with trained model\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     process_video(model, frame_data, output_path, config, fps\u001b[38;5;241m=\u001b[39mvideo_fps)\n",
      "Cell \u001b[0;32mIn[13], line 302\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, frame_data, config)\u001b[0m\n\u001b[1;32m    299\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m    305\u001b[0m loss, (recon_loss, coherence_loss, edge_loss) \u001b[38;5;241m=\u001b[39m criterion(outputs, frames, masks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 207\u001b[0m, in \u001b[0;36mReflectionUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# Encoder path\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(x1)\n\u001b[1;32m    209\u001b[0m     x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(x2)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 154\u001b[0m, in \u001b[0;36mUnetDoubleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:87\u001b[0m, in \u001b[0;36m_InstanceNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_no_batch_dim():\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_no_batch_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_instance_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/instancenorm.py:36\u001b[0m, in \u001b[0;36m_InstanceNorm._apply_instance_norm\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_instance_norm\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:2523\u001b[0m, in \u001b[0;36minstance_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, use_input_stats, momentum, eps)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_input_stats:\n\u001b[1;32m   2522\u001b[0m     _verify_spatial_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_input_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 15.72 GiB of which 181.06 MiB is free. Process 2807253 has 8.38 GiB memory in use. Process 2805731 has 7.16 GiB memory in use. Of the allocated memory 6.54 GiB is allocated by PyTorch, and 419.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration class\n",
    "class Config:\n",
    "    # Video parameters\n",
    "    VIDEO_WIDTH = 512\n",
    "    VIDEO_HEIGHT = 512\n",
    "    CHANNELS = 3  # RGB\n",
    "\n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 30\n",
    "\n",
    "    # Device configuration\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Paths\n",
    "    OUTPUT_DIR = \"output\"\n",
    "    MODEL_DIR = \"models\"\n",
    "\n",
    "    # Visualization parameters\n",
    "    VISUALIZE_EVERY = 20  # Save sample images every N batches\n",
    "\n",
    "    def __init__(self):\n",
    "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "        os.makedirs(self.MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Enhanced frame extraction with original resolution and color\n",
    "def extract_frames(video_path, output_dir=\"frames\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Total frames: {frame_count}, FPS: {fps}, Resolution: {width}x{height}\")\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_path = f\"{output_dir}/frame_{i:04d}.png\"\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frames.append((frame_path, (height, width)))\n",
    "    cap.release()\n",
    "    return frames, fps\n",
    "\n",
    "# Specialized dataset for reflection removal\n",
    "class ReflectionDataset(Dataset):\n",
    "    def __init__(self, frame_data, config, validation=False):\n",
    "        self.frame_data = frame_data\n",
    "        self.config = config\n",
    "        self.validation = validation\n",
    "        \n",
    "        # Use a subset for validation\n",
    "        if validation:\n",
    "            self.frame_data = random.sample(frame_data, min(50, len(frame_data)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_data)\n",
    "    \n",
    "    def detect_reflections(self, frame):\n",
    "        \"\"\"Advanced reflection detection specialized for endoscopy\"\"\"\n",
    "        # Convert to multiple color spaces\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Extract channels\n",
    "        s_channel = hsv[:,:,1]\n",
    "        v_channel = hsv[:,:,2]\n",
    "        \n",
    "        # Calculate adaptive thresholds\n",
    "        v_thresh = np.percentile(v_channel, 97)  # Very bright\n",
    "        s_thresh = np.percentile(s_channel, 15)  # Low saturation\n",
    "        \n",
    "        # Main reflection mask - high brightness and low saturation\n",
    "        reflection_mask = ((v_channel > v_thresh) & (s_channel < s_thresh)).astype(np.uint8)\n",
    "        \n",
    "        # Use local contrast to refine (reflections have high local contrast)\n",
    "        kernel_size = 9\n",
    "        local_mean = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)\n",
    "        local_contrast = cv2.absdiff(gray, local_mean)\n",
    "        contrast_thresh = np.percentile(local_contrast, 90)\n",
    "        contrast_mask = (local_contrast > contrast_thresh).astype(np.uint8)\n",
    "        \n",
    "        # Combine masks\n",
    "        combined_mask = cv2.bitwise_or(reflection_mask, contrast_mask)\n",
    "        \n",
    "        # Clean up with morphological operations\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        cleaned_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Connected component filtering to remove small noise regions\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(cleaned_mask, 4)\n",
    "        min_size = 20\n",
    "        for i in range(1, num_labels):\n",
    "            if stats[i, cv2.CC_STAT_AREA] < min_size:\n",
    "                cleaned_mask[labels == i] = 0\n",
    "                \n",
    "        # Dilate slightly to ensure we cover the full reflection\n",
    "        final_mask = cv2.dilate(cleaned_mask, kernel, iterations=1)\n",
    "        \n",
    "        return final_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame_path, _ = self.frame_data[idx]\n",
    "        frame = cv2.imread(frame_path)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize for consistent processing\n",
    "        frame = cv2.resize(frame, (self.config.VIDEO_WIDTH, self.config.VIDEO_HEIGHT))\n",
    "        \n",
    "        # Detect reflections\n",
    "        mask = self.detect_reflections(frame)\n",
    "        mask = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convert to tensors\n",
    "        frame_tensor = torch.from_numpy(frame.transpose(2, 0, 1)).float() / 255.0\n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        return frame_tensor, mask_tensor\n",
    "\n",
    "# Define U-Net model\n",
    "class UnetDoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            UnetDoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down(x)\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = UnetDoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Adjust dimensions if needed\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class ReflectionUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.inc = UnetDoubleConv(in_channels, 64)\n",
    "        self.down1 = UnetDown(64, 128)\n",
    "        self.down2 = UnetDown(128, 256)\n",
    "        self.down3 = UnetDown(256, 512)\n",
    "        self.down4 = UnetDown(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = UnetUp(1024, 512)\n",
    "        self.up2 = UnetUp(512, 256)\n",
    "        self.up3 = UnetUp(256, 128)\n",
    "        self.up4 = UnetUp(128, 64)\n",
    "        \n",
    "        # Final output\n",
    "        self.outc = nn.Conv2d(64, in_channels, kernel_size=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoder path\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        \n",
    "        # Final output\n",
    "        logits = self.outc(x)\n",
    "        return self.tanh(logits)\n",
    "\n",
    "# Loss function for reflection removal\n",
    "class ReflectionRemovalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReflectionRemovalLoss, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, outputs, targets, masks):\n",
    "        # Reconstruction loss for non-reflection areas (should be identical to input)\n",
    "        non_reflection_area = 1 - masks\n",
    "        reconstruction_loss = self.l1_loss(outputs * non_reflection_area, targets * non_reflection_area) * 10.0\n",
    "        \n",
    "        # Coherence loss for reflection areas\n",
    "        reflection_area = masks\n",
    "        coherence_loss = self.l1_loss(outputs * reflection_area, targets * reflection_area) * 5.0\n",
    "        \n",
    "        # Edge preservation loss\n",
    "        # Create edge maps using Sobel filters\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).to(outputs.device)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).to(outputs.device)\n",
    "        \n",
    "        sobel_x = sobel_x.reshape(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
    "        sobel_y = sobel_y.reshape(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
    "        \n",
    "        # Calculate gradients for outputs and targets\n",
    "        edge_x_output = F.conv2d(outputs, sobel_x, padding=1, groups=3)\n",
    "        edge_y_output = F.conv2d(outputs, sobel_y, padding=1, groups=3)\n",
    "        \n",
    "        edge_x_target = F.conv2d(targets, sobel_x, padding=1, groups=3)\n",
    "        edge_y_target = F.conv2d(targets, sobel_y, padding=1, groups=3)\n",
    "        \n",
    "        # Calculate edge consistency loss\n",
    "        edge_loss = (self.l1_loss(edge_x_output, edge_x_target) + \n",
    "                    self.l1_loss(edge_y_output, edge_y_target)) * 2.0\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = reconstruction_loss + coherence_loss + edge_loss\n",
    "        \n",
    "        return total_loss, (reconstruction_loss, coherence_loss, edge_loss)\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, frame_data, config):\n",
    "    \"\"\"Train the model for reflection removal\"\"\"\n",
    "    print(f\"Training model on {len(frame_data)} frames...\")\n",
    "    \n",
    "    # Create datasets for training and validation\n",
    "    train_frames = random.sample(frame_data, min(500, len(frame_data)))\n",
    "    train_dataset = ReflectionDataset(train_frames, config)\n",
    "    val_dataset = ReflectionDataset(frame_data, config, validation=True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = ReflectionRemovalLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Create output directories\n",
    "    samples_dir = os.path.join(config.OUTPUT_DIR, 'samples')\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS} [Train]\")\n",
    "        for batch_idx, (frames, masks) in enumerate(progress_bar):\n",
    "            frames = frames.to(config.DEVICE)\n",
    "            masks = masks.to(config.DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(frames)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, (recon_loss, coherence_loss, edge_loss) = criterion(outputs, frames, masks)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'recon': f\"{recon_loss.item():.4f}\",\n",
    "                'edge': f\"{edge_loss.item():.4f}\"\n",
    "            })\n",
    "            \n",
    "            # Save sample images periodically\n",
    "            if batch_idx % config.VISUALIZE_EVERY == 0:\n",
    "                save_sample_images(frames, outputs, masks, epoch, batch_idx, samples_dir)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS} [Val]\")\n",
    "            for batch_idx, (frames, masks) in enumerate(progress_bar):\n",
    "                frames = frames.to(config.DEVICE)\n",
    "                masks = masks.to(config.DEVICE)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(frames)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss, _ = criterion(outputs, frames, masks)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Print epoch stats\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{config.NUM_EPOCHS} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Time: {elapsed:.1f}s, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss\n",
    "            }, os.path.join(config.MODEL_DIR, 'best_model.pth'))\n",
    "            print(f\"New best model saved with val loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Save latest model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_val_loss\n",
    "        }, os.path.join(config.MODEL_DIR, 'latest_model.pth'))\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return model\n",
    "\n",
    "# Function to save sample images\n",
    "def save_sample_images(inputs, outputs, masks, epoch, batch_idx, output_dir):\n",
    "    \"\"\"Save sample images showing original, processed, and mask\"\"\"\n",
    "    # Select the first image from the batch\n",
    "    input_img = inputs[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    output_img = outputs[0].detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    mask_img = masks[0][0].detach().cpu().numpy()\n",
    "    \n",
    "    # Clip to valid range [0, 1]\n",
    "    input_img = np.clip(input_img, 0, 1)\n",
    "    output_img = np.clip(output_img, 0, 1)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot original image\n",
    "    axes[0].imshow(input_img)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot processed image\n",
    "    axes[1].imshow(output_img)\n",
    "    axes[1].set_title('Reflection Removed')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Plot mask\n",
    "    axes[2].imshow(mask_img, cmap='gray')\n",
    "    axes[2].set_title('Reflection Mask')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'epoch_{epoch+1}_batch_{batch_idx}.png'))\n",
    "    plt.close(fig)\n",
    "\n",
    "# Function to process a video with a trained model\n",
    "def process_video(model, frame_data, output_path, config, fps=30):\n",
    "    \"\"\"Process a video with a trained model\"\"\"\n",
    "    output_dir = os.path.join(config.OUTPUT_DIR, 'processed_frames')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create dataset for full video\n",
    "    full_dataset = ReflectionDataset(frame_data, config)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (frame_path, original_size) in enumerate(tqdm(frame_data, desc=\"Processing video frames\")):\n",
    "            # Load and process frame\n",
    "            orig_frame = cv2.imread(frame_path)\n",
    "            orig_height, orig_width = orig_frame.shape[:2]\n",
    "            \n",
    "            # Get processed frame from dataset\n",
    "            frame_tensor, mask_tensor = full_dataset[i]\n",
    "            \n",
    "            # Skip processing if no reflections detected\n",
    "            if torch.sum(mask_tensor) == 0:\n",
    "                print(f\"No reflections in frame {i}, skipping\")\n",
    "                cv2.imwrite(f\"{output_dir}/frame_{i:04d}.png\", orig_frame)\n",
    "                continue\n",
    "            \n",
    "            # Process the frame\n",
    "            frame_tensor = frame_tensor.unsqueeze(0).to(config.DEVICE)\n",
    "            output = model(frame_tensor)\n",
    "            \n",
    "            # Apply mask to blend original and generated content\n",
    "            # Create a smoothed mask for better blending\n",
    "            mask_smooth = F.avg_pool2d(F.avg_pool2d(mask_tensor, 3, stride=1, padding=1), 3, stride=1, padding=1)\n",
    "            mask_smooth = mask_smooth.unsqueeze(0).to(config.DEVICE)\n",
    "            \n",
    "            # Blend\n",
    "            blended = frame_tensor * (1 - mask_smooth) + output * mask_smooth\n",
    "            \n",
    "            # Convert output tensor to image\n",
    "            output_img = blended[0].cpu().numpy().transpose(1, 2, 0) * 255\n",
    "            output_img = np.clip(output_img, 0, 255).astype(np.uint8)\n",
    "            output_img = cv2.cvtColor(output_img, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Resize back to original dimensions\n",
    "            output_img = cv2.resize(output_img, (orig_width, orig_height))\n",
    "            \n",
    "            # Save processed frame\n",
    "            cv2.imwrite(f\"{output_dir}/frame_{i:04d}.png\", output_img)\n",
    "    \n",
    "    # Combine frames back into video\n",
    "    os.system(f\"ffmpeg -framerate {fps} -i {output_dir}/frame_%04d.png -c:v libx264 -pix_fmt yuv420p -crf 18 {output_path}\")\n",
    "    print(f\"Processed video saved as {output_path}\")\n",
    "\n",
    "# Main execution function\n",
    "def main(video_path, output_path, mode='train'):\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Initialize configuration\n",
    "    config = Config()\n",
    "    \n",
    "    # Extract frames from video\n",
    "    frame_data, video_fps = extract_frames(video_path)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ReflectionUNet(in_channels=config.CHANNELS).to(config.DEVICE)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        # Train model\n",
    "        model = train_model(model, frame_data, config)\n",
    "        \n",
    "        # Process video with trained model\n",
    "        process_video(model, frame_data, output_path, config, fps=video_fps)\n",
    "    \n",
    "    elif mode == 'inference':\n",
    "        # Load pre-trained model if available\n",
    "        model_path = os.path.join(config.MODEL_DIR, 'best_model.pth')\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path, map_location=config.DEVICE)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            print(f\"Loaded pre-trained model from {model_path}\")\n",
    "        else:\n",
    "            print(\"No pre-trained model found, using untrained model\")\n",
    "        \n",
    "        # Process video with model\n",
    "        process_video(model, frame_data, output_path, config, fps=video_fps)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Use 'train' or 'inference'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"video.mp4\"\n",
    "    output_path = \"output/processed_video.mp4\"\n",
    "    main(video_path, output_path, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38331a-87a4-448d-9c8d-95a0c7e05f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
