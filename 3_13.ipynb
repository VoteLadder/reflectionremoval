{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f2ef74-226d-45be-8915-cb4494233069",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset loaded - Total: 48, Train: 38, Val: 10\n",
      "Model initialized with 7,767,140 trainable parameters\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafa31529fea4c71b54bc5164e14842d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.png\n",
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_213348_frame682.png\n",
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_212419_frame804.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c719da9061f14562bcf867c28deb4175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 1/30 - Train Loss: nan, Val Loss: nan, Time: 2.9s, LR: 0.001000\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/matplotlib/cm.py:478: RuntimeWarning: invalid value encountered in cast\n",
      "  xx = (xx * 255).astype(np.uint8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefba3b4396340d4b2cca65dda319c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_212419_frame804.png\n",
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n",
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_213348_frame682.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5045696f43f54b51bf506bcfdbb36a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 2/30 - Train Loss: nan, Val Loss: nan, Time: 2.3s, LR: 0.001000\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.pngWarning: Mask not found for 1__20250313_213841_frame557.png\n",
      "\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.pngWarning: Mask not found for 1__20250313_213522_frame162.png\n",
      "\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.pngWarning: Mask not found for 1__20250313_212304_frame754.png\n",
      "\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b09d0cef5bd45e1943aaeb2dea783ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_212419_frame804.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n",
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.png\n",
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n",
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_213348_frame682.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf9bc59a3434b4da7c5352a2467981d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 3/30 - Train Loss: nan, Val Loss: nan, Time: 2.3s, LR: 0.001000\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19f9b8360464689a645b57f1393dc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_213348_frame682.png\n",
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_212419_frame804.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.pngWarning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.png\n",
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8673538eb8494987e3c0d97477273d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 4/30 - Train Loss: nan, Val Loss: nan, Time: 2.3s, LR: 0.001000\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e21107bca254df6ab2e711d804d70fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_212419_frame804.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.pngWarning: Mask not found for 1__20250313_213348_frame682.png\n",
      "\n",
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.png\n",
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n",
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a181c2d2e6ac4f17b70ef4adb4d75917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.pngWarning: Mask not found for 1__20250313_214323_frame779.png\n",
      "\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 5/30 - Train Loss: nan, Val Loss: nan, Time: 2.4s, LR: 0.001000\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ba8b2575cc437b8c62c1780873e736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212419_frame804.png\n",
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.png\n",
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_213348_frame682.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d48d93471241a191d30288c2dcca5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6/30 - Train Loss: nan, Val Loss: nan, Time: 2.4s, LR: 0.000500\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9f96930af54f199f4d004f7bad7b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.pngWarning: Mask not found for 1__20250313_212419_frame804.png\n",
      "\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_213348_frame682.png\n",
      "Warning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n",
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ed383ccc8e406087c9c032bfd0dd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 7/30 - Train Loss: nan, Val Loss: nan, Time: 2.4s, LR: 0.000500\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad1b24a718048db9291389a951db922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_211906_frame544.png\n",
      "Warning: Mask not found for 1__20250313_213731_frame245.png\n",
      "Warning: Mask not found for 1__20250313_212419_frame804.png\n",
      "Warning: Mask not found for 1__20250313_212459_frame844.png\n",
      "Warning: Mask not found for 1__20250313_214040_frame687.png\n",
      "Warning: Mask not found for 1__20250313_213152_frame1457.png\n",
      "Warning: Mask not found for 1__20250313_212832_frame1064.png\n",
      "Warning: Mask not found for 1__20250313_212033_frame694.png\n",
      "Warning: Mask not found for 1__20250313_213348_frame682.png\n",
      "Warning: Mask not found for 1__20250313_213314_frame762.png\n",
      "Warning: Mask not found for 1__20250313_212122_frame704.png\n",
      "Warning: Mask not found for 1__20250313_213801_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213750_frame525.png\n",
      "Warning: Mask not found for 1__20250313_213956_frame626.png\n",
      "Warning: Mask not found for 1__20250313_214019_frame669.png\n",
      "Warning: Mask not found for 1__20250313_214349_frame818.png\n",
      "Warning: Mask not found for 1__20250313_213634_frame120.png\n",
      "Warning: Mask not found for 1__20250313_213920_frame590.png\n",
      "Warning: Mask not found for 1__20250313_212339_frame764.png\n",
      "Warning: Mask not found for 1__20250313_213422_frame502.png\n",
      "Warning: Mask not found for 1__20250313_211956_frame564.png\n",
      "Warning: Mask not found for 1__20250313_214256_frame771.png\n",
      "Warning: Mask not found for 1__20250313_212643_frame964.png\n",
      "Warning: Mask not found for 1__20250313_213946_frame618.png\n",
      "Warning: Mask not found for 1__20250313_213910_frame576.png\n",
      "Warning: Mask not found for 1__20250313_211426_frame4.png\n",
      "Warning: Mask not found for 1__20250313_213816_frame536.png\n",
      "Warning: Mask not found for 1__20250313_214138_frame701.png\n",
      "Warning: Mask not found for 1__20250313_212154_frame714.png\n",
      "Warning: Mask not found for 1__20250313_212717_frame974.png\n",
      "Warning: Mask not found for 1__20250313_212919_frame1204.png\n",
      "Warning: Mask not found for 1__20250313_213417_frame572.png\n",
      "Warning: Mask not found for 1__20250313_214208_frame721.png\n",
      "Warning: Mask not found for 1__20250313_213250_frame957.png\n",
      "Warning: Mask not found for 1__20250313_212804_frame1014.png\n",
      "Warning: Mask not found for 1__20250313_213933_frame602.png\n",
      "Warning: Mask not found for 1__20250313_213606_frame46.png\n",
      "Warning: Mask not found for 1__20250313_211708_frame264.png\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823330a4e1a445f7b94f379f4f6e9c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30 [Val]:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n",
      "Warning: Mask not found for 1__20250313_214235_frame756.png\n",
      "Warning: Mask not found for 1__20250313_213023_frame1234.png\n",
      "Epoch 8/30 - Train Loss: nan, Val Loss: nan, Time: 2.4s, LR: 0.000500\n",
      "Warning: Mask not found for 1__20250313_214246_frame764.png\n",
      "Warning: Mask not found for 1__20250313_212534_frame864.png\n",
      "Warning: Mask not found for 1__20250313_213841_frame557.png\n",
      "Warning: Mask not found for 1__20250313_214323_frame779.png\n",
      "Warning: Mask not found for 1__20250313_213804_frame534.png\n",
      "Warning: Mask not found for 1__20250313_213522_frame162.png\n",
      "Warning: Mask not found for 1__20250313_213455_frame342.png\n",
      "Warning: Mask not found for 1__20250313_212304_frame754.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 909\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# Run the main function when script is executed directly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 909\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 715\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 715\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m    724\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_tissue_emulation_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 541\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;66;03m# Visualize a sample after each epoch\u001b[39;00m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loader:\n\u001b[0;32m--> 541\u001b[0m         visualize_sample(model, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader)), epoch, device)\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py:947\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    944\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 947\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Modified Dataset class that focuses on teaching tissue emulation\n",
    "class TissueEmulationDataset(Dataset):\n",
    "    def __init__(self, originals_dir, masks_dir, img_size=(512, 512), \n",
    "                 darkness_threshold=0.15, context_radius=3):\n",
    "        self.originals_dir = originals_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.img_size = img_size\n",
    "        self.darkness_threshold = darkness_threshold  # Threshold to identify aperture\n",
    "        self.context_radius = context_radius  # Radius for sampling surrounding tissue\n",
    "        \n",
    "        # Get all files from originals directory\n",
    "        self.image_filenames = [f for f in os.listdir(originals_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load original image\n",
    "        img_path = os.path.join(self.originals_dir, self.image_filenames[idx])\n",
    "        original_img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Find corresponding mask\n",
    "        mask_filename = self.image_filenames[idx].replace('.png', '_mask.png')\n",
    "        mask_path = os.path.join(self.masks_dir, mask_filename)\n",
    "        \n",
    "        # If mask doesn't exist with _mask suffix, try without it\n",
    "        if not os.path.exists(mask_path):\n",
    "            mask_path = os.path.join(self.masks_dir, self.image_filenames[idx])\n",
    "        \n",
    "        # Load and convert mask to binary\n",
    "        if os.path.exists(mask_path):\n",
    "            mask_img = Image.open(mask_path).convert('L')\n",
    "        else:\n",
    "            # Create an empty mask if the mask file doesn't exist\n",
    "            print(f\"Warning: Mask not found for {self.image_filenames[idx]}\")\n",
    "            mask_img = Image.new('L', original_img.size, 0)\n",
    "        \n",
    "        # Resize to target dimensions\n",
    "        original_img = original_img.resize(self.img_size, Image.BILINEAR)\n",
    "        mask_img = mask_img.resize(self.img_size, Image.NEAREST)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        original_array = np.array(original_img) / 255.0\n",
    "        mask_array = np.array(mask_img) / 255.0\n",
    "        \n",
    "        # Convert mask to binary (threshold)\n",
    "        mask_array = (mask_array > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Create aperture mask (identify dark regions to ignore)\n",
    "        # Average RGB channels to get brightness\n",
    "        brightness = np.mean(original_array, axis=2)\n",
    "        aperture_mask = (brightness < self.darkness_threshold).astype(np.float32)\n",
    "        \n",
    "        # Create valid tissue mask (neither reflection nor aperture)\n",
    "        valid_tissue_mask = (1.0 - mask_array) * (1.0 - aperture_mask)\n",
    "        \n",
    "        # For each reflection pixel, find the nearest valid tissue pixels\n",
    "        # This is a simplified version - in practice you might want a more efficient algorithm\n",
    "        h, w = mask_array.shape\n",
    "        context_map = np.zeros_like(original_array)\n",
    "        \n",
    "        # For pixels in the reflection (mask_array == 1)\n",
    "        mask_indices = np.where(mask_array == 1)\n",
    "        for i, j in zip(mask_indices[0], mask_indices[1]):\n",
    "            # Define region around the pixel\n",
    "            min_i = max(0, i - self.context_radius)\n",
    "            max_i = min(h, i + self.context_radius + 1)\n",
    "            min_j = max(0, j - self.context_radius)\n",
    "            max_j = min(w, j + self.context_radius + 1)\n",
    "            \n",
    "            # Extract region\n",
    "            region_valid = valid_tissue_mask[min_i:max_i, min_j:max_j]\n",
    "            region_orig = original_array[min_i:max_i, min_j:max_j, :]\n",
    "            \n",
    "            # If there are valid tissue pixels in the region\n",
    "            if np.sum(region_valid) > 0:\n",
    "                # Average color of valid tissue\n",
    "                for c in range(3):\n",
    "                    valid_colors = region_orig[:, :, c][region_valid > 0]\n",
    "                    if len(valid_colors) > 0:\n",
    "                        context_map[i, j, c] = np.mean(valid_colors)\n",
    "            else:\n",
    "                # If no valid tissue nearby, use global average of valid tissue\n",
    "                for c in range(3):\n",
    "                    valid_colors = original_array[:, :, c][valid_tissue_mask > 0]\n",
    "                    if len(valid_colors) > 0:\n",
    "                        context_map[i, j, c] = np.mean(valid_colors)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        original_tensor = torch.from_numpy(original_array.transpose(2, 0, 1)).float()\n",
    "        mask_tensor = torch.from_numpy(mask_array).float().unsqueeze(0)\n",
    "        aperture_tensor = torch.from_numpy(aperture_mask).float().unsqueeze(0)\n",
    "        context_tensor = torch.from_numpy(context_map.transpose(2, 0, 1)).float()\n",
    "        \n",
    "        # Create input tensor by concatenating image, mask, and aperture mask\n",
    "        input_tensor = torch.cat([original_tensor, mask_tensor, aperture_tensor], dim=0)\n",
    "        \n",
    "        # Model should learn to use context_map values in masked areas\n",
    "        # Target is original image for non-masked areas, and context_map for masked areas\n",
    "        target_tensor = original_tensor * (1.0 - mask_tensor) + context_tensor * mask_tensor\n",
    "        \n",
    "        return {\n",
    "            'input': input_tensor,           # [C+2, H, W] - RGB + Reflection mask + Aperture mask\n",
    "            'target': target_tensor,         # [C, H, W] - RGB with reflections replaced by context\n",
    "            'mask': mask_tensor,             # [1, H, W] - Reflection mask\n",
    "            'aperture_mask': aperture_tensor, # [1, H, W] - Aperture mask\n",
    "            'filename': self.image_filenames[idx]\n",
    "        }\n",
    "\n",
    "# Modified U-Net with awareness of aperture and focus on surrounding tissue emulation\n",
    "class TissueEmulationUNet(nn.Module):\n",
    "    def __init__(self, in_channels=5, out_channels=3):\n",
    "        super(TissueEmulationUNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Pooling\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Bottleneck with attention\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism to focus on relevant tissue patterns\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.upconv1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.upconv2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.upconv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.upconv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output layers - RGB channels\n",
    "        self.outconv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract components of the input\n",
    "        img = x[:, :3]           # RGB channels\n",
    "        mask = x[:, 3:4]         # Reflection mask\n",
    "        aperture_mask = x[:, 4:5] # Aperture mask (dark areas)\n",
    "        \n",
    "        # Create a combined invalid area mask (reflections + aperture)\n",
    "        invalid_mask = torch.clamp(mask + aperture_mask, 0, 1)\n",
    "        \n",
    "        # Encoder path\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.maxpool(x1)\n",
    "        \n",
    "        x2 = self.down2(x2)\n",
    "        x3 = self.maxpool(x2)\n",
    "        \n",
    "        x3 = self.down3(x3)\n",
    "        x4 = self.maxpool(x3)\n",
    "        \n",
    "        x4 = self.down4(x4)\n",
    "        x5 = self.maxpool(x4)\n",
    "        \n",
    "        # Bottleneck with attention\n",
    "        x5 = self.bottleneck(x5)\n",
    "        attention_map = self.attention(x5)\n",
    "        x5 = x5 * attention_map\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        x = self.up1(x5)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = self.upconv1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.upconv2(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.upconv3(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.upconv4(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.outconv(x)\n",
    "        x = torch.sigmoid(x)  # Scale to [0,1]\n",
    "        \n",
    "        # Only replace the reflection areas, keep the original elsewhere\n",
    "        output = img * (1.0 - mask) + x * mask\n",
    "        \n",
    "        # Ensure we don't modify the aperture areas\n",
    "        output = output * (1.0 - aperture_mask) + img * aperture_mask\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Custom loss function focused on tissue emulation\n",
    "class TissueEmulationLoss(nn.Module):\n",
    "    def __init__(self, texture_weight=2.0, edge_weight=1.0):\n",
    "        super(TissueEmulationLoss, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.texture_weight = texture_weight\n",
    "        self.edge_weight = edge_weight\n",
    "        \n",
    "    def edge_loss(self, pred, target, mask):\n",
    "        # Sobel operators for edge detection\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], device=pred.device).float().view(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], device=pred.device).float().view(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
    "        \n",
    "        # Expand mask to match number of channels\n",
    "        expanded_mask = mask.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        # Detect edges\n",
    "        pred_edges_x = F.conv2d(pred, sobel_x, padding=1, groups=3)\n",
    "        pred_edges_y = F.conv2d(pred, sobel_y, padding=1, groups=3)\n",
    "        target_edges_x = F.conv2d(target, sobel_x, padding=1, groups=3)\n",
    "        target_edges_y = F.conv2d(target, sobel_y, padding=1, groups=3)\n",
    "        \n",
    "        # Calculate edge magnitude\n",
    "        pred_edges = torch.sqrt(pred_edges_x**2 + pred_edges_y**2)\n",
    "        target_edges = torch.sqrt(target_edges_x**2 + target_edges_y**2)\n",
    "        \n",
    "        # Only consider edges in masked areas\n",
    "        edge_loss = self.l1_loss(pred_edges * expanded_mask, target_edges * expanded_mask)\n",
    "        \n",
    "        return edge_loss\n",
    "    \n",
    "    def texture_loss(self, pred, target, mask):\n",
    "        # Simple texture loss using local patches\n",
    "        # For a more advanced implementation, consider using Gram matrices\n",
    "        # or other texture descriptors\n",
    "        \n",
    "        # Apply average pooling to get local features\n",
    "        pred_pooled = F.avg_pool2d(pred, kernel_size=3, stride=1, padding=1)\n",
    "        target_pooled = F.avg_pool2d(target, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Only consider texture in masked areas\n",
    "        expanded_mask = mask.repeat(1, 3, 1, 1)\n",
    "        texture_loss = self.l1_loss(pred_pooled * expanded_mask, target_pooled * expanded_mask)\n",
    "        \n",
    "        return texture_loss\n",
    "    \n",
    "    def forward(self, pred, target, mask, aperture_mask):\n",
    "        # Calculate valid mask (not aperture)\n",
    "        valid_mask = 1.0 - aperture_mask\n",
    "        \n",
    "        # Only evaluate loss in non-aperture regions\n",
    "        pred_valid = pred * valid_mask\n",
    "        target_valid = target * valid_mask\n",
    "        mask_valid = mask * valid_mask\n",
    "        \n",
    "        # Pixel-wise L1 loss\n",
    "        pixel_loss = self.l1_loss(pred_valid * mask_valid, target_valid * mask_valid)\n",
    "        \n",
    "        # Edge consistency loss\n",
    "        edge = self.edge_loss(pred_valid, target_valid, mask_valid) * self.edge_weight\n",
    "        \n",
    "        # Texture matching loss\n",
    "        texture = self.texture_loss(pred_valid, target_valid, mask_valid) * self.texture_weight\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = pixel_loss + edge + texture\n",
    "        \n",
    "        return total_loss, {\n",
    "            'pixel': pixel_loss.item(),\n",
    "            'edge': edge.item(),\n",
    "            'texture': texture.item()\n",
    "        }\n",
    "\n",
    "# Function to visualize results\n",
    "def visualize_sample(model, batch, epoch, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get input, target and masks\n",
    "        inputs = batch['input'].to(device)\n",
    "        targets = batch['target'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        aperture_masks = batch['aperture_mask'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Convert tensors to numpy for visualization\n",
    "        input_img = inputs[0, :3].cpu().numpy().transpose(1, 2, 0)  # RGB channels only\n",
    "        mask_img = masks[0, 0].cpu().numpy()\n",
    "        aperture_img = aperture_masks[0, 0].cpu().numpy()\n",
    "        target_img = targets[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        output_img = outputs[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Plot input image\n",
    "        axes[0, 0].imshow(input_img)\n",
    "        axes[0, 0].set_title('Input Image')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Plot reflection mask\n",
    "        axes[0, 1].imshow(mask_img, cmap='gray')\n",
    "        axes[0, 1].set_title('Reflection Mask')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Plot aperture mask\n",
    "        axes[0, 2].imshow(aperture_img, cmap='gray')\n",
    "        axes[0, 2].set_title('Aperture Mask')\n",
    "        axes[0, 2].axis('off')\n",
    "        \n",
    "        # Plot target image\n",
    "        axes[1, 0].imshow(target_img)\n",
    "        axes[1, 0].set_title('Target Image')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Plot output image\n",
    "        axes[1, 1].imshow(output_img)\n",
    "        axes[1, 1].set_title('Output Image')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        # Plot difference image\n",
    "        diff_img = np.abs(output_img - target_img)\n",
    "        diff_img = diff_img / np.max(diff_img) if np.max(diff_img) > 0 else diff_img\n",
    "        axes[1, 2].imshow(diff_img)\n",
    "        axes[1, 2].set_title('Difference')\n",
    "        axes[1, 2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/sample_epoch_{epoch+1}.png')\n",
    "        plt.close()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30, lr=0.001):\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = TissueEmulationLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        \n",
    "        for batch in train_bar:\n",
    "            # Get input, target, masks\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            aperture_masks = batch['aperture_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, loss_components = criterion(outputs, targets, masks, aperture_masks)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update training loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'pixel': f\"{loss_components['pixel']:.4f}\",\n",
    "                'texture': f\"{loss_components['texture']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Progress bar for validation\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            \n",
    "            for batch in val_bar:\n",
    "                inputs = batch['input'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                masks = batch['mask'].to(device)\n",
    "                aperture_masks = batch['aperture_mask'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, _ = criterion(outputs, targets, masks, aperture_masks)\n",
    "                \n",
    "                # Update validation loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_bar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Update history\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Time: {epoch_time:.1f}s, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_val_loss\n",
    "            }, 'best_tissue_emulation_model.pth')\n",
    "            print(f\"Saved new best model with val loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Visualize a sample after each epoch\n",
    "        if val_loader:\n",
    "            visualize_sample(model, next(iter(val_loader)), epoch, device)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Video processing function\n",
    "def process_video(model, video_path, output_path, darkness_threshold=0.15):\n",
    "    # Make sure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create output video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Load the model\n",
    "    model.eval()\n",
    "    \n",
    "    # Create transform for preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Function to detect specular reflections and aperture\n",
    "    def detect_features(frame):\n",
    "        # Convert to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to HSV for better reflection detection\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Extract value channel (brightness)\n",
    "        v_channel = hsv[:,:,2]\n",
    "        \n",
    "        # Create a mask for very bright areas (specular reflections)\n",
    "        _, reflection_mask = cv2.threshold(v_channel, 220, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Create a mask for dark areas (outside the endoscope view)\n",
    "        brightness = np.mean(frame_rgb / 255.0, axis=2)\n",
    "        aperture_mask = (brightness < darkness_threshold).astype(np.float32)\n",
    "        \n",
    "        # Clean up reflection mask with morphological operations\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        reflection_mask = cv2.morphologyEx(reflection_mask, cv2.MORPH_CLOSE, kernel)\n",
    "        reflection_mask = cv2.morphologyEx(reflection_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        return frame_rgb, reflection_mask / 255.0, aperture_mask\n",
    "    \n",
    "    # Process each frame\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(frame_count), desc=\"Processing video\"):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Detect reflection and aperture\n",
    "            frame_rgb, reflection_mask, aperture_mask = detect_features(frame)\n",
    "            \n",
    "            # Check if reflections found\n",
    "            if np.max(reflection_mask) > 0:\n",
    "                # Preprocess\n",
    "                frame_tensor = preprocess(frame_rgb).unsqueeze(0)\n",
    "                reflection_tensor = torch.from_numpy(reflection_mask).float().unsqueeze(0).unsqueeze(0)\n",
    "                reflection_tensor = F.interpolate(reflection_tensor, size=(512, 512), mode='nearest')\n",
    "                aperture_tensor = torch.from_numpy(aperture_mask).float().unsqueeze(0).unsqueeze(0)\n",
    "                aperture_tensor = F.interpolate(aperture_tensor, size=(512, 512), mode='nearest')\n",
    "                \n",
    "                # Create input by concatenating frame, reflection mask, and aperture mask\n",
    "                input_tensor = torch.cat([frame_tensor, reflection_tensor, aperture_tensor], dim=1).to(device)\n",
    "                \n",
    "                # Process with model\n",
    "                output = model(input_tensor)\n",
    "                \n",
    "                # Convert output to numpy\n",
    "                output_np = output[0].cpu().numpy().transpose(1, 2, 0)\n",
    "                output_np = (output_np * 255).astype(np.uint8)\n",
    "                \n",
    "                # Resize back to original dimensions\n",
    "                output_np = cv2.resize(output_np, (width, height))\n",
    "                \n",
    "                # Convert RGB to BGR for OpenCV\n",
    "                output_bgr = cv2.cvtColor(output_np, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "                # Create the mask at original resolution\n",
    "                orig_mask = cv2.resize(reflection_mask, (width, height))\n",
    "                orig_mask = np.expand_dims(orig_mask, axis=2).repeat(3, axis=2)\n",
    "                \n",
    "                # Blend original and processed frames\n",
    "                blended = frame * (1 - orig_mask) + output_bgr * orig_mask\n",
    "                blended = blended.astype(np.uint8)\n",
    "                \n",
    "                # Write frame\n",
    "                out.write(blended)\n",
    "            else:\n",
    "                # No reflections, use original frame\n",
    "                out.write(frame)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Processed video saved to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create results directory\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    # Parameters\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 0.001\n",
    "    IMG_SIZE = (512, 512)\n",
    "    DARKNESS_THRESHOLD = 0.15  # Adjust based on your images\n",
    "    \n",
    "    # Path to your data directories\n",
    "    originals_dir = 'originals'  # Directory with original images\n",
    "    masks_dir = 'masks'         # Directory with reflection masks\n",
    "    \n",
    "    # Create the dataset\n",
    "    full_dataset = TissueEmulationDataset(\n",
    "        originals_dir=originals_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        img_size=IMG_SIZE,\n",
    "        darkness_threshold=DARKNESS_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Split into train and validation sets (80% train, 20% validation)\n",
    "    dataset_size = len(full_dataset)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = dataset_size - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded - Total: {dataset_size}, Train: {train_size}, Val: {val_size}\")\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = TissueEmulationUNet(in_channels=5, out_channels=3).to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model initialized with {total_params:,} trainable parameters\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        lr=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save(model.state_dict(), 'final_tissue_emulation_model.pth')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['epoch_times'])\n",
    "    plt.title('Epoch Times')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Time (s)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/training_history.png')\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Process a video if available\n",
    "    video_path = input(\"Enter the path to a video to process (or press Enter to skip): \")\n",
    "    if video_path and os.path.exists(video_path):\n",
    "        output_path = 'results/processed_video.mp4'\n",
    "        print(f\"Processing video {video_path}...\")\n",
    "        process_video(model, video_path, output_path, darkness_threshold=DARKNESS_THRESHOLD)\n",
    "        print(f\"Video processing completed!\")\n",
    "\n",
    "# Additionally, add functions to use a pre-trained model for inference\n",
    "\n",
    "def load_trained_model(model_path, device=None):\n",
    "    \"\"\"Load a pre-trained model for inference\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = TissueEmulationUNet(in_channels=5, out_channels=3).to(device)\n",
    "    \n",
    "    # Load the saved weights\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Check if we have a full checkpoint or just state_dict\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def process_single_image(model, image_path, output_path=None, darkness_threshold=0.15):\n",
    "    \"\"\"Process a single image to remove reflections\"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not open image {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Detect reflection and aperture\n",
    "    frame_rgb, reflection_mask, aperture_mask = detect_features(image, darkness_threshold)\n",
    "    \n",
    "    # Check if reflections found\n",
    "    if np.max(reflection_mask) > 0:\n",
    "        # Create preprocessing transform\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        # Preprocess\n",
    "        frame_tensor = preprocess(frame_rgb).unsqueeze(0)\n",
    "        reflection_tensor = torch.from_numpy(reflection_mask).float().unsqueeze(0).unsqueeze(0)\n",
    "        reflection_tensor = F.interpolate(reflection_tensor, size=(512, 512), mode='nearest')\n",
    "        aperture_tensor = torch.from_numpy(aperture_mask).float().unsqueeze(0).unsqueeze(0)\n",
    "        aperture_tensor = F.interpolate(aperture_tensor, size=(512, 512), mode='nearest')\n",
    "        \n",
    "        # Create input by concatenating frame, reflection mask, and aperture mask\n",
    "        input_tensor = torch.cat([frame_tensor, reflection_tensor, aperture_tensor], dim=1).to(device)\n",
    "        \n",
    "        # Process with model\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        # Convert output to numpy\n",
    "        output_np = output[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        output_np = (output_np * 255).astype(np.uint8)\n",
    "        \n",
    "        # Resize back to original dimensions\n",
    "        h, w = image.shape[:2]\n",
    "        output_np = cv2.resize(output_np, (w, h))\n",
    "        \n",
    "        # Convert RGB to BGR for OpenCV\n",
    "        output_bgr = cv2.cvtColor(output_np, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Create the mask at original resolution\n",
    "        orig_mask = cv2.resize(reflection_mask, (w, h))\n",
    "        orig_mask = np.expand_dims(orig_mask, axis=2).repeat(3, axis=2)\n",
    "        \n",
    "        # Blend original and processed frames\n",
    "        blended = image * (1 - orig_mask) + output_bgr * orig_mask\n",
    "        processed_image = blended.astype(np.uint8)\n",
    "        \n",
    "        # Save if output path provided\n",
    "        if output_path:\n",
    "            cv2.imwrite(output_path, processed_image)\n",
    "            print(f\"Processed image saved to {output_path}\")\n",
    "        \n",
    "        return processed_image\n",
    "    else:\n",
    "        print(\"No reflections detected in the image.\")\n",
    "        if output_path:\n",
    "            cv2.imwrite(output_path, image)\n",
    "        return image\n",
    "\n",
    "def detect_features(frame, darkness_threshold=0.15):\n",
    "    \"\"\"Helper function to detect specular reflections and aperture in an image\"\"\"\n",
    "    # Convert to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert to HSV for better reflection detection\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Extract value channel (brightness)\n",
    "    v_channel = hsv[:,:,2]\n",
    "    \n",
    "    # Create a mask for very bright areas (specular reflections)\n",
    "    _, reflection_mask = cv2.threshold(v_channel, 220, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Create a mask for dark areas (outside the endoscope view)\n",
    "    brightness = np.mean(frame_rgb / 255.0, axis=2)\n",
    "    aperture_mask = (brightness < darkness_threshold).astype(np.float32)\n",
    "    \n",
    "    # Clean up reflection mask with morphological operations\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    reflection_mask = cv2.morphologyEx(reflection_mask, cv2.MORPH_CLOSE, kernel)\n",
    "    reflection_mask = cv2.morphologyEx(reflection_mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return frame_rgb, reflection_mask / 255.0, aperture_mask\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model performance on a test dataset\"\"\"\n",
    "    model.eval()\n",
    "    criterion = TissueEmulationLoss()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    component_losses = {'pixel': 0.0, 'edge': 0.0, 'texture': 0.0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating model\"):\n",
    "            inputs = batch['input'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            aperture_masks = batch['aperture_mask'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss, loss_components = criterion(outputs, targets, masks, aperture_masks)\n",
    "            \n",
    "            # Update losses\n",
    "            total_loss += loss.item()\n",
    "            for key, value in loss_components.items():\n",
    "                component_losses[key] += value\n",
    "    \n",
    "    # Average losses\n",
    "    num_batches = len(test_loader)\n",
    "    avg_total_loss = total_loss / num_batches\n",
    "    avg_component_losses = {k: v / num_batches for k, v in component_losses.items()}\n",
    "    \n",
    "    print(f\"Evaluation results:\")\n",
    "    print(f\"  Total loss: {avg_total_loss:.4f}\")\n",
    "    for key, value in avg_component_losses.items():\n",
    "        print(f\"  {key.capitalize()} loss: {value:.4f}\")\n",
    "    \n",
    "    return avg_total_loss, avg_component_losses\n",
    "\n",
    "# Run the main function when script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b069854-1582-4c37-a3f5-11cec5a9822b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
