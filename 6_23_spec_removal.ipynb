{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d490bed-9a4d-401c-8bcc-66498e7ef0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model file models/specular_removal_model_epoch_50.pth not found. Training a new model...\n",
      "Starting fallback training...\n",
      "Training epoch 1/50 completed. Average Loss: 0.153508\n",
      "Training epoch 2/50 completed. Average Loss: 0.072186\n",
      "Training epoch 3/50 completed. Average Loss: 0.034875\n",
      "Training epoch 4/50 completed. Average Loss: 0.017543\n",
      "Training epoch 5/50 completed. Average Loss: 0.009831\n",
      "Training epoch 6/50 completed. Average Loss: 0.006028\n",
      "Training epoch 7/50 completed. Average Loss: 0.003942\n",
      "Training epoch 8/50 completed. Average Loss: 0.002702\n",
      "Training epoch 9/50 completed. Average Loss: 0.001942\n",
      "Training epoch 10/50 completed. Average Loss: 0.001452\n",
      "Training epoch 11/50 completed. Average Loss: 0.001126\n",
      "Training epoch 12/50 completed. Average Loss: 0.000904\n",
      "Training epoch 13/50 completed. Average Loss: 0.000741\n",
      "Training epoch 14/50 completed. Average Loss: 0.000626\n",
      "Training epoch 15/50 completed. Average Loss: 0.000535\n",
      "Training epoch 16/50 completed. Average Loss: 0.000469\n",
      "Training epoch 17/50 completed. Average Loss: 0.000419\n",
      "Training epoch 18/50 completed. Average Loss: 0.000370\n",
      "Training epoch 19/50 completed. Average Loss: 0.000335\n",
      "Training epoch 20/50 completed. Average Loss: 0.000308\n",
      "Training epoch 21/50 completed. Average Loss: 0.000288\n",
      "Training epoch 22/50 completed. Average Loss: 0.000275\n",
      "Training epoch 23/50 completed. Average Loss: 0.000254\n",
      "Training epoch 24/50 completed. Average Loss: 0.000237\n",
      "Training epoch 25/50 completed. Average Loss: 0.000220\n",
      "Training epoch 26/50 completed. Average Loss: 0.000206\n",
      "Training epoch 27/50 completed. Average Loss: 0.000198\n",
      "Training epoch 28/50 completed. Average Loss: 0.000195\n",
      "Training epoch 29/50 completed. Average Loss: 0.000185\n",
      "Training epoch 30/50 completed. Average Loss: 0.000181\n",
      "Training epoch 31/50 completed. Average Loss: 0.000171\n",
      "Training epoch 32/50 completed. Average Loss: 0.000165\n",
      "Training epoch 33/50 completed. Average Loss: 0.000160\n",
      "Training epoch 34/50 completed. Average Loss: 0.000151\n",
      "Training epoch 35/50 completed. Average Loss: 0.000150\n",
      "Training epoch 36/50 completed. Average Loss: 0.000149\n",
      "Training epoch 37/50 completed. Average Loss: 0.000146\n",
      "Training epoch 38/50 completed. Average Loss: 0.000140\n",
      "Training epoch 39/50 completed. Average Loss: 0.000136\n",
      "Training epoch 40/50 completed. Average Loss: 0.000134\n",
      "Training epoch 41/50 completed. Average Loss: 0.000131\n",
      "Training epoch 42/50 completed. Average Loss: 0.000132\n",
      "Training epoch 43/50 completed. Average Loss: 0.000130\n",
      "Training epoch 44/50 completed. Average Loss: 0.000122\n",
      "Training epoch 45/50 completed. Average Loss: 0.000122\n",
      "Training epoch 46/50 completed. Average Loss: 0.000122\n",
      "Training epoch 47/50 completed. Average Loss: 0.000119\n",
      "Training epoch 48/50 completed. Average Loss: 0.000119\n",
      "Training epoch 49/50 completed. Average Loss: 0.000119\n",
      "Training epoch 50/50 completed. Average Loss: 0.000115\n",
      "Trained and saved model to models/specular_removal_model_epoch_50.pth\n",
      "Scanning initial frames for best keyframe...\n",
      "Initial keyframe found with specular score: 0.0035\n",
      "  -> Rejected keyframe update due to low sharpness (608.07 vs 910.38)\n",
      "Processed initial frame 1/150\n",
      "  -> Rejected keyframe update due to low sharpness (609.25 vs 910.38)\n",
      "Processed initial frame 2/150\n",
      "  -> Rejected keyframe update due to low sharpness (630.74 vs 910.38)\n",
      "Processed initial frame 3/150\n",
      "  -> Rejected keyframe update due to low sharpness (624.74 vs 910.38)\n",
      "Processed initial frame 4/150\n",
      "  -> Rejected keyframe update due to low sharpness (627.70 vs 910.38)\n",
      "Processed initial frame 5/150\n",
      "  -> Rejected keyframe update due to low sharpness (598.36 vs 910.38)\n",
      "Processed initial frame 6/150\n",
      "  -> Rejected keyframe update due to low sharpness (608.77 vs 910.38)\n",
      "Processed initial frame 7/150\n",
      "  -> Rejected keyframe update due to low sharpness (652.90 vs 910.38)\n",
      "Processed initial frame 8/150\n",
      "  -> Rejected keyframe update due to low sharpness (626.81 vs 910.38)\n",
      "Processed initial frame 9/150\n",
      "  -> Rejected keyframe update due to low sharpness (619.27 vs 910.38)\n",
      "Processed initial frame 10/150\n",
      "  -> Rejected keyframe update due to low sharpness (638.51 vs 910.38)\n",
      "Processed initial frame 11/150\n",
      "  -> Rejected keyframe update due to low sharpness (614.15 vs 910.38)\n",
      "Processed initial frame 12/150\n",
      "  -> Rejected keyframe update due to low sharpness (653.18 vs 910.38)\n",
      "Processed initial frame 13/150\n",
      "  -> Rejected keyframe update due to low sharpness (626.00 vs 910.38)\n",
      "Processed initial frame 14/150\n",
      "  -> Rejected keyframe update due to low sharpness (575.10 vs 910.38)\n",
      "Processed initial frame 15/150\n",
      "  -> Rejected keyframe update due to low sharpness (590.68 vs 910.38)\n",
      "Processed initial frame 16/150\n",
      "  -> Rejected keyframe update due to low sharpness (617.45 vs 910.38)\n",
      "Processed initial frame 17/150\n",
      "  -> Rejected keyframe update due to low sharpness (662.04 vs 910.38)\n",
      "Processed initial frame 18/150\n",
      "  -> Rejected keyframe update due to low sharpness (681.87 vs 910.38)\n",
      "Processed initial frame 19/150\n",
      "  -> Rejected keyframe update due to low sharpness (620.51 vs 910.38)\n",
      "Processed initial frame 20/150\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "deque index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 416\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Output saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minpainted_output\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 388\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Process the frames already in the buffer first\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(initial_frames):\n\u001b[0;32m--> 388\u001b[0m     output_img, keyframe, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minpainted_output/frame_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m06d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;66;03m# ✅✅✅ CORRECTED LINE ✅✅✅\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 320\u001b[0m, in \u001b[0;36mVideoFrameProcessor.process_frame\u001b[0;34m(self, frame, frame_idx_in_buffer, frame_buffer, keyframe)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNUM_CONTEXT_FRAMES\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    319\u001b[0m     context_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, frame_idx_in_buffer \u001b[38;5;241m-\u001b[39m ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFRAME_SKIP\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m--> 320\u001b[0m     ctx_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(\u001b[43mframe_buffer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontext_idx\u001b[49m\u001b[43m]\u001b[49m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMG_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMG_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    321\u001b[0m     ctx_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(Image\u001b[38;5;241m.\u001b[39mfromarray(cv2\u001b[38;5;241m.\u001b[39mcvtColor(ctx_frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    322\u001b[0m     context_tensors\u001b[38;5;241m.\u001b[39mappend(ctx_tensor)\n",
      "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pywt\n",
    "import collections\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"VIDEO_PATH\": \"video.mp4\",\n",
    "    \"IMG_SIZE\": 256,\n",
    "    \"MODEL_PATH\": \"models/specular_removal_model_epoch_50.pth\",\n",
    "    \"NUM_CONTEXT_FRAMES\": 4,\n",
    "    \"FRAME_SKIP\": 3,\n",
    "    \"SPECULAR_THRESHOLD\": 0.5,\n",
    "    \"WAVELET\": \"haar\",\n",
    "    \"LEVEL\": 2,\n",
    "    \"MIN_MASK_SIZE\": 10,\n",
    "    \"MAX_MASK_SIZE\": 50,\n",
    "    \"TRAIN_EPOCHS\": 50,\n",
    "    \"LEARNING_RATE\": 1e-4,\n",
    "    \"INITIAL_KEYFRAME_SECONDS\": 5,\n",
    "    \"BATCH_SIZE\": 4,\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- FFC Implementation (Corrected) ---\n",
    "class FFC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, ratio_gin, ratio_gout, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels = in_channels, out_channels\n",
    "        self.ratio_gin, self.ratio_gout = ratio_gin, ratio_gout\n",
    "        \n",
    "        self.in_local = int(self.in_channels * (1 - ratio_gin))\n",
    "        self.in_global = self.in_channels - self.in_local\n",
    "        self.out_local = int(self.out_channels * (1 - ratio_gout))\n",
    "        self.out_global = self.out_channels - self.out_local\n",
    "\n",
    "        if self.in_local > 0 and self.out_local > 0:\n",
    "            self.conv_l = nn.Conv2d(self.in_local, self.out_local, kernel_size, stride, padding, bias=False)\n",
    "        else:\n",
    "            self.conv_l = None\n",
    "        \n",
    "        if self.in_global > 0 and self.out_global > 0:\n",
    "            self.conv_g = nn.Conv2d(self.in_global * 2, self.out_global * 2, kernel_size=1, bias=False)\n",
    "        else:\n",
    "            self.conv_g = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, height, width = x.shape\n",
    "        if self.in_local > 0 and self.in_global > 0:\n",
    "            x_l, x_g = torch.split(x, [self.in_local, self.in_global], dim=1)\n",
    "        elif self.in_local > 0:\n",
    "            x_l, x_g = x, None\n",
    "        else:\n",
    "            x_l, x_g = None, x\n",
    "\n",
    "        out_l = None\n",
    "        if self.conv_l is not None:\n",
    "            if x_l is not None:\n",
    "                out_l = self.conv_l(x_l)\n",
    "            elif self.out_local > 0:\n",
    "                out_l = torch.zeros(batch_size, self.out_local, height, width, device=x.device)\n",
    "\n",
    "        out_g = None\n",
    "        if x_g is not None and self.conv_g is not None:\n",
    "            fft_g = torch.fft.rfft2(x_g, norm='ortho')\n",
    "            fft_g_real_imag = torch.cat([fft_g.real, fft_g.imag], dim=1)\n",
    "            ffc_out_real_imag = self.conv_g(fft_g_real_imag)\n",
    "            ffc_out_real, ffc_out_imag = torch.split(ffc_out_real_imag, self.out_global, dim=1)\n",
    "            fft_g = torch.complex(ffc_out_real, ffc_out_imag)\n",
    "            out_g = torch.fft.irfft2(fft_g, s=(height, width), norm='ortho')\n",
    "        elif self.out_global > 0:\n",
    "            out_g = torch.zeros(batch_size, self.out_global, height, width, device=x.device)\n",
    "\n",
    "        if out_l is not None and out_g is not None:\n",
    "            return torch.cat([out_l, out_g], dim=1)\n",
    "        elif out_l is not None:\n",
    "            return out_l\n",
    "        elif out_g is not None:\n",
    "            return out_g\n",
    "        else:\n",
    "            return torch.zeros(batch_size, self.out_channels, height, width, device=x.device)\n",
    "\n",
    "class FFC_BN_ACT(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 ratio_gin=0.75, ratio_gout=0.75, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.ffc = FFC(in_channels, out_channels, kernel_size, ratio_gin, ratio_gout, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ffc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "# --- Model Definition (Corrected Architecture) ---\n",
    "\n",
    "class LaMaUNet(nn.Module):\n",
    "    def __init__(self, n_channels=18, n_classes=3, bilinear=True):\n",
    "        super().__init__()\n",
    "        ratio = 0.75\n",
    "\n",
    "        # --- Encoder ---\n",
    "        self.inc = nn.Sequential(\n",
    "            FFC_BN_ACT(n_channels, 32, kernel_size=3, padding=1, ratio_gin=0, ratio_gout=ratio),\n",
    "            FFC_BN_ACT(32, 32, kernel_size=3, padding=1, ratio_gin=ratio, ratio_gout=ratio)\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            FFC_BN_ACT(32, 64, kernel_size=3, padding=1, ratio_gin=ratio, ratio_gout=ratio)\n",
    "        )\n",
    "        \n",
    "        # --- Bottleneck ---\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            FFC_BN_ACT(64, 128, kernel_size=3, padding=1, ratio_gin=ratio, ratio_gout=ratio),\n",
    "            FFC_BN_ACT(128, 64, kernel_size=3, padding=1, ratio_gin=ratio, ratio_gout=ratio)\n",
    "        )\n",
    "        \n",
    "        # --- Decoder ---\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='bilinear' if bilinear else 'nearest', align_corners=True if bilinear else None)\n",
    "        # Input channels: 64 from upsample (from bottleneck) + 32 from skip connection (x1) = 96\n",
    "        self.conv_up1 = nn.Sequential(\n",
    "            FFC_BN_ACT(96, 32, kernel_size=3, padding=1, ratio_gin=ratio, ratio_gout=ratio),\n",
    "            FFC_BN_ACT(32, 32, kernel_size=3, padding=1, ratio_gin=ratio, ratio_gout=ratio)\n",
    "        )\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        self.outc = nn.Conv2d(32, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Encoder Path ---\n",
    "        x1 = self.inc(x)           # Shape: [B, 32, 256, 256]\n",
    "        x2 = self.down1(x1)         # Shape: [B, 64, 128, 128]\n",
    "        \n",
    "        # --- Bottleneck ---\n",
    "        x_bottle = self.bottleneck(x2) # Shape: [B, 64, 128, 128]\n",
    "\n",
    "        # --- Decoder Path ---\n",
    "        u1 = self.up1(x_bottle)     # Shape: [B, 64, 256, 256]\n",
    "        \n",
    "        # Skip Connection: Concatenate with x1 from the same level\n",
    "        c1 = torch.cat([u1, x1], dim=1) # Shape: [B, 64+32=96, 256, 256]\n",
    "        \n",
    "        # Up-convolution\n",
    "        d1 = self.conv_up1(c1)      # Shape: [B, 32, 256, 256]\n",
    "\n",
    "        # Final output\n",
    "        logits = self.outc(d1)      # Shape: [B, 3, 256, 256]\n",
    "\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "# ... The rest of the code (VideoFrameProcessor, main, etc.) remains unchanged ...\n",
    "def calculate_specular_score(frame):\n",
    "    \"\"\"Calculates a specular score based on the area and intensity of specular regions.\"\"\"\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    v = hsv[:, :, 2]\n",
    "    specular_mask = (v > 200) & (hsv[:, :, 1] < 40)\n",
    "    if np.sum(specular_mask) == 0:\n",
    "        return 0.0\n",
    "    specular_area = np.sum(specular_mask) / (frame.shape[0] * frame.shape[1])\n",
    "    specular_intensity = np.mean(v[specular_mask]) / 255.0\n",
    "    return specular_area * specular_intensity\n",
    "\n",
    "def get_sharpness(image):\n",
    "    \"\"\"Calculates image sharpness using the variance of the Laplacian.\"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "class VideoFrameProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.model = LaMaUNet(n_channels=18, n_classes=3).to(DEVICE)\n",
    "        if os.path.exists(config[\"MODEL_PATH\"]):\n",
    "            self.model.load_state_dict(torch.load(config[\"MODEL_PATH\"], map_location=DEVICE))\n",
    "            print(f\"Loaded model from {config['MODEL_PATH']}\")\n",
    "        else:\n",
    "            print(f\"Model file {config['MODEL_PATH']} not found. Training a new model...\")\n",
    "            self._train_model()\n",
    "        self.model.eval()\n",
    "\n",
    "    def _train_model(self):\n",
    "        from torch.utils.data import Dataset, DataLoader\n",
    "        class TempDataset(Dataset):\n",
    "            def __init__(self, frames, keyframe, config):\n",
    "                self.frames = frames\n",
    "                self.keyframe = keyframe\n",
    "                self.config = config\n",
    "                self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "            def __len__(self): return len(self.frames)\n",
    "            def __getitem__(self, idx):\n",
    "                frame = cv2.resize(self.frames[idx], (self.config[\"IMG_SIZE\"], self.config[\"IMG_SIZE\"]))\n",
    "                target_tensor = self.transform(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
    "                \n",
    "                keyframe_tensor = self.transform(Image.fromarray(cv2.cvtColor(cv2.resize(self.keyframe, (self.config[\"IMG_SIZE\"], self.config[\"IMG_SIZE\"])), cv2.COLOR_BGR2RGB))) * 0.5\n",
    "                \n",
    "                context_tensors = []\n",
    "                for i in range(self.config[\"NUM_CONTEXT_FRAMES\"]):\n",
    "                    context_idx = max(0, idx - ((i + 1) * self.config[\"FRAME_SKIP\"]))\n",
    "                    ctx_frame = cv2.resize(self.frames[context_idx], (self.config[\"IMG_SIZE\"], self.config[\"IMG_SIZE\"]))\n",
    "                    ctx_tensor = self.transform(Image.fromarray(cv2.cvtColor(ctx_frame, cv2.COLOR_BGR2RGB)))\n",
    "                    context_tensors.append(ctx_tensor)\n",
    "                \n",
    "                masked_target_tensor = target_tensor.clone()\n",
    "                if np.random.random() > 0.5:\n",
    "                    mask_h, mask_w = np.random.randint(10, 30), np.random.randint(10, 30)\n",
    "                    start_h, start_w = np.random.randint(0, self.config[\"IMG_SIZE\"] - mask_h), np.random.randint(0, self.config[\"IMG_SIZE\"] - mask_w)\n",
    "                    masked_target_tensor[:, start_h:start_h+mask_h, start_w:start_w+mask_w] = 0\n",
    "                \n",
    "                model_input = torch.cat(context_tensors[::-1] + [masked_target_tensor, keyframe_tensor], dim=0)\n",
    "                ground_truth = target_tensor\n",
    "                return model_input, ground_truth\n",
    "\n",
    "        cap = cv2.VideoCapture(self.config[\"VIDEO_PATH\"])\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        best_score = float('inf')\n",
    "        best_keyframe = frames[0]\n",
    "        for frame in frames[:int(self.config[\"INITIAL_KEYFRAME_SECONDS\"] * 30)]:\n",
    "            score = calculate_specular_score(frame)\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_keyframe = frame\n",
    "        \n",
    "        dataset = TempDataset(frames, best_keyframe, self.config)\n",
    "        loader = DataLoader(dataset, batch_size=self.config[\"BATCH_SIZE\"], shuffle=True)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.config[\"LEARNING_RATE\"])\n",
    "        \n",
    "        self.model.train()\n",
    "        print(\"Starting fallback training...\")\n",
    "        for epoch in range(self.config[\"TRAIN_EPOCHS\"]):\n",
    "            epoch_loss = 0.0\n",
    "            for model_input, ground_truth in loader:\n",
    "                model_input, ground_truth = model_input.to(DEVICE), ground_truth.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(model_input)\n",
    "                loss = criterion(outputs, ground_truth)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Training epoch {epoch+1}/{self.config['TRAIN_EPOCHS']} completed. Average Loss: {epoch_loss / len(loader):.6f}\")\n",
    "            \n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(self.model.state_dict(), self.config[\"MODEL_PATH\"])\n",
    "        print(f\"Trained and saved model to {self.config['MODEL_PATH']}\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def _detect_specular_regions(self, frame):\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        coeffs = pywt.wavedec2(gray, self.config[\"WAVELET\"], level=self.config[\"LEVEL\"])\n",
    "        details = []\n",
    "        for detail_coeffs in coeffs[1:]:\n",
    "            details.extend([np.abs(detail_coeffs[i]) for i in range(3)])\n",
    "        \n",
    "        detail_features = np.stack([cv2.resize(d, (gray.shape[1], gray.shape[0])) for d in details])\n",
    "        detail_features = np.max(detail_features, axis=0)\n",
    "        \n",
    "        normalized_features = (detail_features - np.min(detail_features)) / (np.max(detail_features) - np.min(detail_features) + 1e-6)\n",
    "        wavelet_mask = normalized_features > self.config[\"SPECULAR_THRESHOLD\"]\n",
    "        \n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        v = hsv[:, :, 2]\n",
    "        high_v = v > 200\n",
    "        low_s = hsv[:,:,1] < 40\n",
    "        combined_mask = np.logical_or(np.logical_and(high_v, low_s), wavelet_mask)\n",
    "        \n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.morphologyEx(combined_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        return mask\n",
    "\n",
    "    def _mask_with_black_boxes(self, frame, mask):\n",
    "        h, w = frame.shape[:2]\n",
    "        black_box_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) > 10:\n",
    "                x, y, w_box, h_box = cv2.boundingRect(contour)\n",
    "                box_size = np.random.randint(self.config[\"MIN_MASK_SIZE\"], self.config[\"MAX_MASK_SIZE\"])\n",
    "                x_start = max(0, x + w_box // 2 - box_size // 2)\n",
    "                y_start = max(0, y + h_box // 2 - box_size // 2)\n",
    "                x_end = min(w, x_start + box_size)\n",
    "                y_end = min(h, y_start + box_size)\n",
    "                cv2.rectangle(black_box_mask, (x_start, y_start), (x_end, y_end), 1, -1)\n",
    "        return black_box_mask\n",
    "\n",
    "    def process_frame(self, frame, frame_idx_in_buffer, frame_buffer, keyframe):\n",
    "        orig_h, orig_w = frame.shape[:2]\n",
    "        frame_resized = cv2.resize(frame, (self.config[\"IMG_SIZE\"], self.config[\"IMG_SIZE\"]))\n",
    "        keyframe_resized = cv2.resize(keyframe, (self.config[\"IMG_SIZE\"], self.config[\"IMG_SIZE\"]))\n",
    "\n",
    "        frame_tensor = self.transform(Image.fromarray(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(DEVICE)\n",
    "        keyframe_tensor = self.transform(Image.fromarray(cv2.cvtColor(keyframe_resized, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(DEVICE) * 0.5\n",
    "\n",
    "        specular_mask = self._detect_specular_regions(frame_resized)\n",
    "        black_box_mask = self._mask_with_black_boxes(frame_resized, specular_mask)\n",
    "        mask_tensor = torch.from_numpy(black_box_mask).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        masked_target_tensor = frame_tensor * (1 - mask_tensor)\n",
    "\n",
    "        context_tensors = []\n",
    "        for i in range(self.config[\"NUM_CONTEXT_FRAMES\"]):\n",
    "            context_idx = max(0, frame_idx_in_buffer - ((i + 1) * self.config[\"FRAME_SKIP\"]))\n",
    "            ctx_frame = cv2.resize(frame_buffer[context_idx], (self.config[\"IMG_SIZE\"], self.config[\"IMG_SIZE\"]))\n",
    "            ctx_tensor = self.transform(Image.fromarray(cv2.cvtColor(ctx_frame, cv2.COLOR_BGR2RGB))).unsqueeze(0).to(DEVICE)\n",
    "            context_tensors.append(ctx_tensor)\n",
    "        \n",
    "        input_tensor = torch.cat(context_tensors[::-1] + [masked_target_tensor, keyframe_tensor], dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tensor = self.model(input_tensor)\n",
    "\n",
    "        output_img_resized = (output_tensor[0].cpu().permute(1, 2, 0).numpy() * 255).clip(0, 255).astype(np.uint8)\n",
    "        output_img_resized = cv2.cvtColor(output_img_resized, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        output_score = calculate_specular_score(output_img_resized)\n",
    "        keyframe_score = calculate_specular_score(keyframe_resized)\n",
    "        \n",
    "        if output_score < keyframe_score * 0.95:\n",
    "            output_sharpness = get_sharpness(output_img_resized)\n",
    "            keyframe_sharpness = get_sharpness(keyframe_resized)\n",
    "            \n",
    "            if output_sharpness >= keyframe_sharpness * 0.8:\n",
    "                keyframe = cv2.resize(output_img_resized, (orig_w, orig_h))\n",
    "                print(f\"  -> Keyframe updated! New score: {output_score:.4f}, Sharpness: {output_sharpness:.2f}\")\n",
    "            else:\n",
    "                print(f\"  -> Rejected keyframe update due to low sharpness ({output_sharpness:.2f} vs {keyframe_sharpness:.2f})\")\n",
    "        \n",
    "        final_output_img = cv2.resize(output_img_resized, (orig_w, orig_h))\n",
    "        return final_output_img, keyframe, black_box_mask\n",
    "\n",
    "def main():\n",
    "    processor = VideoFrameProcessor(CONFIG)\n",
    "\n",
    "    cap = cv2.VideoCapture(CONFIG[\"VIDEO_PATH\"])\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file at {CONFIG['VIDEO_PATH']}\")\n",
    "        return\n",
    "        \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "\n",
    "    print(\"Scanning initial frames for best keyframe...\")\n",
    "    initial_frames = []\n",
    "    num_initial_frames = int(CONFIG[\"INITIAL_KEYFRAME_SECONDS\"] * fps)\n",
    "    for _ in range(num_initial_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        initial_frames.append(frame)\n",
    "\n",
    "    if not initial_frames:\n",
    "        print(\"Error: Could not read any frames from video.\")\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    best_keyframe = initial_frames[0].copy()\n",
    "    best_score = float('inf')\n",
    "    for frame in initial_frames:\n",
    "        score = calculate_specular_score(frame)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_keyframe = frame.copy()\n",
    "    print(f\"Initial keyframe found with specular score: {best_score:.4f}\")\n",
    "    \n",
    "    max_buffer_size = CONFIG[\"NUM_CONTEXT_FRAMES\"] * CONFIG[\"FRAME_SKIP\"] + 5\n",
    "    frame_buffer = collections.deque(initial_frames, maxlen=max_buffer_size)\n",
    "    \n",
    "    os.makedirs(\"inpainted_output\", exist_ok=True)\n",
    "    keyframe = best_keyframe\n",
    "    \n",
    "    # Process the frames already in the buffer first\n",
    "    for i, frame in enumerate(initial_frames):\n",
    "        output_img, keyframe, _ = processor.process_frame(frame, i, frame_buffer, keyframe)\n",
    "        output_path = f\"inpainted_output/frame_{i:06d}.png\"\n",
    "        \n",
    "        # ✅✅✅ CORRECTED LINE ✅✅✅\n",
    "        cv2.imwrite(output_path, output_img)\n",
    "        \n",
    "        print(f\"Processed initial frame {i+1}/{len(initial_frames)}\")\n",
    "        \n",
    "    # Process the rest of the video\n",
    "    frame_idx = len(initial_frames)\n",
    "    while True:\n",
    "        ret, current_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_buffer.append(current_frame)\n",
    "        \n",
    "        output_img, keyframe, _ = processor.process_frame(current_frame, len(frame_buffer) - 1, frame_buffer, keyframe)\n",
    "        \n",
    "        output_path = f\"inpainted_output/frame_{frame_idx:06d}.png\"\n",
    "        cv2.imwrite(output_path, output_img)\n",
    "        print(f\"Processed frame {frame_idx}...\")\n",
    "        frame_idx += 1\n",
    "        \n",
    "    cap.release()\n",
    "    print(\"Processing complete. Output saved to 'inpainted_output' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13d652-fa93-4b0c-8d8c-8ef5a915892e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
