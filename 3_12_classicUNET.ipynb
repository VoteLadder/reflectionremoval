{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86187401-844b-47a3-b567-f57cec462bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (0.18.3)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d01924b-7b0d-4e2a-9851-b2dca7b19fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total frames: 1548, FPS: 30, Resolution: 1280x720\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0921, D_loss=168.3369] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, G Loss: 0.1388, D Loss: 4169.8215, Content: 0.01, Edge: 0.02, Time: 24.4s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.1388\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.60it/s, G_loss=0.0890, D_loss=54.4915] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, G Loss: 0.0858, D Loss: 104.0735, Content: 0.01, Edge: 0.01, Time: 51.6s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0858\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0686, D_loss=31.7626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, G Loss: 0.0743, D Loss: 53.0399, Content: 0.00, Edge: 0.01, Time: 76.2s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0743\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0621, D_loss=17.2646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, G Loss: 0.0711, D Loss: 25.1283, Content: 0.00, Edge: 0.01, Time: 100.9s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0711\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0499, D_loss=11.4806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, G Loss: 0.0632, D Loss: 20.3862, Content: 0.00, Edge: 0.01, Time: 125.5s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0632\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0852, D_loss=10.6172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, G Loss: 0.0602, D Loss: 11.9976, Content: 0.00, Edge: 0.01, Time: 150.2s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0602\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0476, D_loss=7.3449] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, G Loss: 0.0548, D Loss: 9.0077, Content: 0.00, Edge: 0.01, Time: 177.2s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0548\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0507, D_loss=15.8434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, G Loss: 0.0538, D Loss: 7.5489, Content: 0.00, Edge: 0.01, Time: 201.8s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0538\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0462, D_loss=2.9099] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, G Loss: 0.0536, D Loss: 5.5864, Content: 0.00, Edge: 0.01, Time: 226.5s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0536\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0565, D_loss=3.2022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, G Loss: 0.0510, D Loss: 3.4383, Content: 0.00, Edge: 0.01, Time: 251.1s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0510\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0407, D_loss=1.8372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, G Loss: 0.0485, D Loss: 3.2062, Content: 0.00, Edge: 0.01, Time: 275.8s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0485\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0439, D_loss=2.2068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, G Loss: 0.0456, D Loss: 2.7421, Content: 0.00, Edge: 0.01, Time: 303.0s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0456\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0493, D_loss=1.0841] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, G Loss: 0.0441, D Loss: 2.7729, Content: 0.00, Edge: 0.01, Time: 327.7s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0441\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0484, D_loss=1.8938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, G Loss: 0.0442, D Loss: 1.5767, Content: 0.00, Edge: 0.01, Time: 352.4s, G LR: 0.000200\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0438, D_loss=0.5121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, G Loss: 0.0418, D Loss: 1.6100, Content: 0.00, Edge: 0.01, Time: 377.0s, G LR: 0.000200\n",
      "New best model saved with G loss: 0.0418\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0310, D_loss=1.1397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, G Loss: 0.0413, D Loss: 1.0951, Content: 0.00, Edge: 0.01, Time: 401.7s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0413\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0289, D_loss=4.3400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, G Loss: 0.0338, D Loss: 1.1915, Content: 0.00, Edge: 0.00, Time: 428.7s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0338\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0297, D_loss=0.7581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, G Loss: 0.0339, D Loss: 1.1357, Content: 0.00, Edge: 0.00, Time: 453.5s, G LR: 0.000100\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0283, D_loss=0.7078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, G Loss: 0.0326, D Loss: 1.6555, Content: 0.00, Edge: 0.00, Time: 478.1s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0326\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0281, D_loss=0.8496]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, G Loss: 0.0322, D Loss: 1.0421, Content: 0.00, Edge: 0.00, Time: 502.7s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0322\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0273, D_loss=1.9346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, G Loss: 0.0321, D Loss: 1.0715, Content: 0.00, Edge: 0.00, Time: 527.4s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0321\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0361, D_loss=1.1109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, G Loss: 0.0322, D Loss: 0.7878, Content: 0.00, Edge: 0.00, Time: 554.3s, G LR: 0.000100\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0341, D_loss=1.2855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, G Loss: 0.0308, D Loss: 0.6626, Content: 0.00, Edge: 0.00, Time: 578.9s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0308\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0392, D_loss=0.9637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, G Loss: 0.0314, D Loss: 0.7287, Content: 0.00, Edge: 0.00, Time: 603.6s, G LR: 0.000100\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0317, D_loss=0.3658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, G Loss: 0.0308, D Loss: 0.6979, Content: 0.00, Edge: 0.00, Time: 628.2s, G LR: 0.000100\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0344, D_loss=0.2510]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, G Loss: 0.0308, D Loss: 0.6683, Content: 0.00, Edge: 0.00, Time: 652.8s, G LR: 0.000100\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0240, D_loss=0.5590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, G Loss: 0.0304, D Loss: 0.7042, Content: 0.00, Edge: 0.00, Time: 680.0s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0304\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0290, D_loss=0.2423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, G Loss: 0.0295, D Loss: 0.7419, Content: 0.00, Edge: 0.00, Time: 704.7s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0295\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0287, D_loss=0.7521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, G Loss: 0.0295, D Loss: 0.4671, Content: 0.00, Edge: 0.00, Time: 729.4s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0295\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0246, D_loss=0.1332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, G Loss: 0.0297, D Loss: 0.5823, Content: 0.00, Edge: 0.00, Time: 754.1s, G LR: 0.000100\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0305, D_loss=0.6259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, G Loss: 0.0285, D Loss: 0.5764, Content: 0.00, Edge: 0.00, Time: 778.7s, G LR: 0.000100\n",
      "New best model saved with G loss: 0.0285\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0328, D_loss=0.3107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, G Loss: 0.0293, D Loss: 0.3934, Content: 0.00, Edge: 0.00, Time: 805.6s, G LR: 0.000100\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0283, D_loss=0.6301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, G Loss: 0.0283, D Loss: 0.4127, Content: 0.00, Edge: 0.00, Time: 830.3s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0283\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0227, D_loss=0.2494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, G Loss: 0.0247, D Loss: 0.5290, Content: 0.00, Edge: 0.00, Time: 855.0s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0247\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0293, D_loss=1.1408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, G Loss: 0.0247, D Loss: 0.4209, Content: 0.00, Edge: 0.00, Time: 879.6s, G LR: 0.000050\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0244, D_loss=0.1274]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, G Loss: 0.0243, D Loss: 0.5410, Content: 0.00, Edge: 0.00, Time: 904.2s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0243\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0212, D_loss=0.2537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, G Loss: 0.0244, D Loss: 0.5019, Content: 0.00, Edge: 0.00, Time: 931.2s, G LR: 0.000050\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0210, D_loss=0.2293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, G Loss: 0.0243, D Loss: 0.6779, Content: 0.00, Edge: 0.00, Time: 955.8s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0243\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0253, D_loss=0.0820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, G Loss: 0.0240, D Loss: 0.2839, Content: 0.00, Edge: 0.00, Time: 980.6s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0240\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0269, D_loss=0.3416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, G Loss: 0.0237, D Loss: 0.4023, Content: 0.00, Edge: 0.00, Time: 1005.2s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0237\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0211, D_loss=0.3158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, G Loss: 0.0243, D Loss: 0.4670, Content: 0.00, Edge: 0.00, Time: 1029.9s, G LR: 0.000050\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0302, D_loss=0.2231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, G Loss: 0.0241, D Loss: 0.5528, Content: 0.00, Edge: 0.00, Time: 1056.8s, G LR: 0.000050\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0227, D_loss=0.2046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, G Loss: 0.0231, D Loss: 0.6472, Content: 0.00, Edge: 0.00, Time: 1081.4s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0231\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0225, D_loss=0.7012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, G Loss: 0.0235, D Loss: 0.3705, Content: 0.00, Edge: 0.00, Time: 1106.1s, G LR: 0.000050\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0249, D_loss=1.2450]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, G Loss: 0.0234, D Loss: 0.3706, Content: 0.00, Edge: 0.00, Time: 1130.7s, G LR: 0.000050\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.57it/s, G_loss=0.0206, D_loss=0.2568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, G Loss: 0.0234, D Loss: 0.3397, Content: 0.00, Edge: 0.00, Time: 1155.3s, G LR: 0.000050\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.58it/s, G_loss=0.0245, D_loss=0.1085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, G Loss: 0.0238, D Loss: 0.5149, Content: 0.00, Edge: 0.00, Time: 1182.5s, G LR: 0.000050\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0209, D_loss=0.1162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, G Loss: 0.0228, D Loss: 0.3938, Content: 0.00, Edge: 0.00, Time: 1207.2s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0228\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0234, D_loss=0.3042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, G Loss: 0.0228, D Loss: 0.4262, Content: 0.00, Edge: 0.00, Time: 1231.9s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0228\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 63/63 [00:24<00:00,  2.56it/s, G_loss=0.0184, D_loss=0.2074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, G Loss: 0.0227, D Loss: 0.4266, Content: 0.00, Edge: 0.00, Time: 1256.6s, G LR: 0.000050\n",
      "New best model saved with G loss: 0.0227\n",
      "Processing video frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1548/1548 [03:12<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video from processed frames...\n",
      "Processed video saved as processed_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from 'processed_frames/frame_%04d.png':\n",
      "  Duration: 00:00:51.60, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgb24(pc), 1280x720, 30 fps, 30 tbr, 30 tbn, 30 tbc\n",
      "File 'processed_video.mp4' already exists. Overwrite? [y/N] Not overwriting - exiting\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import os\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Enhanced frame extraction with original resolution and color\n",
    "def extract_frames(video_path, output_dir=\"frames\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Total frames: {frame_count}, FPS: {fps}, Resolution: {width}x{height}\")\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_path = f\"{output_dir}/frame_{i:04d}.png\"\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frames.append((frame_path, (height, width)))\n",
    "    cap.release()\n",
    "    return frames, fps\n",
    "\n",
    "# Specialized dataset for reflection removal\n",
    "class ReflectionDataset(Dataset):\n",
    "    def __init__(self, frame_data, patch_size=256, augment=True):\n",
    "        self.frame_data = frame_data\n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frame_data)\n",
    "    \n",
    "    def detect_reflections(self, frame):\n",
    "        \"\"\"\n",
    "        More inclusive reflection detection for endoscopy images - \n",
    "        designed to catch a broader range of specular highlights\n",
    "        \"\"\"\n",
    "        # Convert to multiple color spaces\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_RGB2HSV)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        lab = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "        # Extract channels\n",
    "        s_channel = hsv[:,:,1]    # Saturation\n",
    "        v_channel = hsv[:,:,2]    # Value/brightness\n",
    "        l_channel = lab[:,:,0]    # Lightness\n",
    "\n",
    "        # 1. Primary method: High brightness\n",
    "        # More relaxed brightness threshold (top 5% instead of top 1.5%)\n",
    "        v_thresh = np.percentile(v_channel, 95)  \n",
    "        bright_areas = (v_channel > v_thresh).astype(np.uint8)\n",
    "\n",
    "        # 2. Secondary method: High brightness with low saturation (classic specular)\n",
    "        v_thresh_high = np.percentile(v_channel, 97)\n",
    "        s_thresh_low = np.percentile(s_channel, 30)  # More inclusive saturation threshold\n",
    "        specular_highlights = ((v_channel > v_thresh_high) & (s_channel < s_thresh_low)).astype(np.uint8)\n",
    "\n",
    "        # 3. Third method: Local brightness peaks\n",
    "        kernel_size = 9  # Smaller kernel to detect more localized brightness peaks\n",
    "        max_filtered = cv2.dilate(gray, np.ones((kernel_size, kernel_size), np.uint8))\n",
    "        local_maxima = ((gray == max_filtered) & (gray > np.percentile(gray, 92))).astype(np.uint8)\n",
    "\n",
    "        # 4. LAB space extreme lightness\n",
    "        l_thresh = np.percentile(l_channel, 95)\n",
    "        lab_bright = (l_channel > l_thresh).astype(np.uint8)\n",
    "\n",
    "        # Combine all methods\n",
    "        combined_mask = cv2.bitwise_or(bright_areas, specular_highlights)\n",
    "        combined_mask = cv2.bitwise_or(combined_mask, local_maxima)\n",
    "        combined_mask = cv2.bitwise_or(combined_mask, lab_bright)\n",
    "\n",
    "        # Morphological operations to connect nearby reflections\n",
    "        kernel_close = np.ones((5, 5), np.uint8)\n",
    "        kernel_open = np.ones((2, 2), np.uint8)  # Smaller kernel to preserve more details\n",
    "\n",
    "        cleaned_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel_close)\n",
    "        cleaned_mask = cv2.morphologyEx(cleaned_mask, cv2.MORPH_OPEN, kernel_open)\n",
    "\n",
    "        # Connected component filtering with smaller minimum size\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(cleaned_mask, 4)\n",
    "        min_size = 2  # Smaller threshold to keep more reflection points\n",
    "        refined_mask = np.zeros_like(cleaned_mask)\n",
    "\n",
    "        for i in range(1, num_labels):\n",
    "            if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
    "                refined_mask[labels == i] = 1\n",
    "\n",
    "        # Dilate to ensure full coverage of reflections\n",
    "        kernel_dilate = np.ones((3, 3), np.uint8)\n",
    "        final_mask = cv2.dilate(refined_mask, kernel_dilate, iterations=1)\n",
    "\n",
    "        return final_mask * 255  # Scale to 0-255 range\n",
    "    \n",
    "\n",
    "    \n",
    "    def create_patch(self, image, mask):\n",
    "        \"\"\"Create a patch containing reflections for training\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Find reflection areas\n",
    "        y_indices, x_indices = np.where(mask > 0)\n",
    "        \n",
    "        if len(y_indices) > 0:\n",
    "            # Choose a random reflection point\n",
    "            idx = random.randrange(len(y_indices))\n",
    "            center_y, center_x = y_indices[idx], x_indices[idx]\n",
    "            \n",
    "            # Calculate crop boundaries\n",
    "            half_size = self.patch_size // 2\n",
    "            x_start = max(0, min(center_x - half_size, w - self.patch_size))\n",
    "            y_start = max(0, min(center_y - half_size, h - self.patch_size))\n",
    "            \n",
    "            # Extract patches\n",
    "            image_patch = image[y_start:y_start+self.patch_size, x_start:x_start+self.patch_size].copy()\n",
    "            mask_patch = mask[y_start:y_start+self.patch_size, x_start:x_start+self.patch_size].copy()\n",
    "        else:\n",
    "            # If no reflections, take a random patch\n",
    "            x_start = random.randint(0, max(0, w - self.patch_size))\n",
    "            y_start = random.randint(0, max(0, h - self.patch_size))\n",
    "            image_patch = image[y_start:y_start+self.patch_size, x_start:x_start+self.patch_size].copy()\n",
    "            mask_patch = mask[y_start:y_start+self.patch_size, x_start:x_start+self.patch_size].copy()\n",
    "            \n",
    "        return image_patch, mask_patch\n",
    "    \n",
    "    def augment_sample(self, image, mask):\n",
    "        \"\"\"Apply data augmentation\"\"\"\n",
    "        # Random flip\n",
    "        if random.random() > 0.5:\n",
    "            image = np.fliplr(image).copy()\n",
    "            mask = np.fliplr(mask).copy()\n",
    "        \n",
    "        # Random rotation\n",
    "        k = random.randint(0, 3)\n",
    "        if k > 0:\n",
    "            image = np.rot90(image, k).copy()\n",
    "            mask = np.rot90(mask, k).copy()\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame_path, _ = self.frame_data[idx]\n",
    "        frame = cv2.imread(frame_path)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize for consistent processing\n",
    "        frame = cv2.resize(frame, (512, 512))\n",
    "        frame_float = frame.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Detect reflections\n",
    "        mask = self.detect_reflections(frame)\n",
    "        mask_float = mask.astype(np.float32) / 255.0\n",
    "        \n",
    "        if self.augment:\n",
    "            # Create patch for training\n",
    "            image_patch, mask_patch = self.create_patch(frame_float, mask_float)\n",
    "            image_patch, mask_patch = self.augment_sample(image_patch, mask_patch)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            image_tensor = torch.from_numpy(image_patch.transpose(2, 0, 1)).float()\n",
    "            mask_tensor = torch.from_numpy(mask_patch).unsqueeze(0).float()\n",
    "        else:\n",
    "            # For inference, use the whole image\n",
    "            image_tensor = torch.from_numpy(frame_float.transpose(2, 0, 1)).float()\n",
    "            mask_tensor = torch.from_numpy(mask_float).unsqueeze(0).float()\n",
    "        \n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "# Residual Block for Generator\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.InstanceNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.InstanceNorm2d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Simplified Generator\n",
    "class InpaintingGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InpaintingGenerator, self).__init__()\n",
    "        \n",
    "        # Initial convolution - take image and mask as input\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Downsampling\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.resblocks = nn.Sequential(\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256)\n",
    "        )\n",
    "        \n",
    "        # Upsampling\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_conv = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Concatenate input image and mask\n",
    "        x_input = torch.cat([x, mask], dim=1)\n",
    "        \n",
    "        # Encoder\n",
    "        e0 = self.init_conv(x_input)\n",
    "        e1 = self.down1(e0)\n",
    "        e2 = self.down2(e1)\n",
    "        \n",
    "        # Residual blocks\n",
    "        r = self.resblocks(e2)\n",
    "        \n",
    "        # Decoder\n",
    "        d1 = self.up1(r)\n",
    "        d2 = self.up2(d1)\n",
    "        \n",
    "        # Output\n",
    "        output = self.output_conv(d2)\n",
    "        \n",
    "        # Create a smoothed mask for blending\n",
    "        smoothed_mask = F.avg_pool2d(F.avg_pool2d(mask, 3, stride=1, padding=1), 3, stride=1, padding=1)\n",
    "        \n",
    "        # Combine original image and generated image using the mask\n",
    "        # Original content outside mask, generated content inside mask\n",
    "        result = x * (1 - smoothed_mask) + output * smoothed_mask\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Input: 3 channel image\n",
    "        self.layers = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Combined loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, outputs, targets, masks, d_outputs=None):\n",
    "        # Content loss (areas outside mask should remain unchanged)\n",
    "        content_loss = self.l1_loss(outputs * (1 - masks), targets * (1 - masks)) * 10.0\n",
    "        \n",
    "        # Reflection area loss\n",
    "        reflection_loss = self.l1_loss(outputs * masks, targets * masks) * 5.0\n",
    "        \n",
    "        # Edge consistency loss for smooth transitions\n",
    "        # Create a boundary region around reflections\n",
    "        dilated_mask = F.max_pool2d(masks, kernel_size=5, stride=1, padding=2)\n",
    "        boundary_mask = dilated_mask - masks\n",
    "        \n",
    "        # Edge loss focuses on the boundary area\n",
    "        edge_loss = self.l1_loss(outputs * boundary_mask, targets * boundary_mask) * 20.0\n",
    "        \n",
    "        # Adversarial loss (optional, used if discriminator is available)\n",
    "        adv_loss = torch.tensor(0.0, device=outputs.device)\n",
    "        if d_outputs is not None:\n",
    "            adv_loss = -torch.mean(d_outputs) * 0.1  # WGAN-style adversarial loss\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = content_loss + reflection_loss + edge_loss + adv_loss\n",
    "        \n",
    "        # Return loss components for logging\n",
    "        loss_components = {\n",
    "            'content': content_loss.item(),\n",
    "            'reflection': reflection_loss.item(),\n",
    "            'edge': edge_loss.item(),\n",
    "            'adversarial': adv_loss.item() if isinstance(adv_loss, torch.Tensor) else adv_loss\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_components\n",
    "\n",
    "# Training function\n",
    "def train_model(frame_data, epochs=50, batch_size=8, save_dir=\"results\"):\n",
    "    \"\"\"Train the reflection removal model using GAN approach\"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Create test frames for visualization\n",
    "    test_frames = random.sample(frame_data, min(5, len(frame_data)))\n",
    "    \n",
    "    # Create training dataset\n",
    "    train_frames = random.sample(frame_data, min(500, len(frame_data)))\n",
    "    dataset = ReflectionDataset(train_frames, patch_size=256, augment=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_dataset = ReflectionDataset(test_frames, augment=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = InpaintingGenerator().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    # Initialize optimizers\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    g_scheduler = torch.optim.lr_scheduler.MultiStepLR(g_optimizer, milestones=[epochs//3, epochs*2//3], gamma=0.5)\n",
    "    d_scheduler = torch.optim.lr_scheduler.MultiStepLR(d_optimizer, milestones=[epochs//3, epochs*2//3], gamma=0.5)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = CombinedLoss()\n",
    "    \n",
    "    # Create directory for progress visualization\n",
    "    progress_dir = os.path.join(save_dir, \"progress\")\n",
    "    if not os.path.exists(progress_dir):\n",
    "        os.makedirs(progress_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        \n",
    "        # Track losses\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        loss_component_sums = {'content': 0, 'reflection': 0, 'edge': 0, 'adversarial': 0}\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "        \n",
    "        for i, (images, masks) in enumerate(progress_bar):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            # ---------------------\n",
    "            # Train Discriminator\n",
    "            # ---------------------\n",
    "            if i % 3 == 0:  # Train D less frequently\n",
    "                d_optimizer.zero_grad()\n",
    "                \n",
    "                # Real images\n",
    "                real_outputs = discriminator(images)\n",
    "                \n",
    "                # Generate fake images\n",
    "                with torch.no_grad():\n",
    "                    fake_images = generator(images, masks)\n",
    "                fake_outputs = discriminator(fake_images.detach())\n",
    "                \n",
    "                # Compute loss (WGAN)\n",
    "                d_loss = torch.mean(fake_outputs) - torch.mean(real_outputs)\n",
    "                \n",
    "                # Add gradient penalty\n",
    "                alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "                interpolates = alpha * images + (1 - alpha) * fake_images.detach()\n",
    "                interpolates.requires_grad_(True)\n",
    "                \n",
    "                d_interpolates = discriminator(interpolates)\n",
    "                \n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=d_interpolates,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=torch.ones_like(d_interpolates),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True\n",
    "                )[0]\n",
    "                \n",
    "                gradients = gradients.view(batch_size, -1)\n",
    "                gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10\n",
    "                \n",
    "                d_loss = d_loss + gradient_penalty\n",
    "                \n",
    "                d_loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            # ---------------------\n",
    "            # Train Generator\n",
    "            # ---------------------\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Generate fake images\n",
    "            fake_images = generator(images, masks)\n",
    "            \n",
    "            # Adversarial loss (only if discriminator is being used)\n",
    "            fake_outputs = None\n",
    "            if i % 3 == 0:\n",
    "                fake_outputs = discriminator(fake_images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            g_loss, loss_components = criterion(fake_images, images, masks, fake_outputs)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            g_losses.append(g_loss.item())\n",
    "            for k, v in loss_components.items():\n",
    "                loss_component_sums[k] += v\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'G_loss': f\"{g_loss.item():.4f}\",\n",
    "                'D_loss': f\"{d_losses[-1] if d_losses else 0:.4f}\"\n",
    "            })\n",
    "        \n",
    "        # Update learning rates\n",
    "        g_scheduler.step()\n",
    "        d_scheduler.step()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_g_loss = sum(g_losses) / len(g_losses) if g_losses else 0\n",
    "        avg_d_loss = sum(d_losses) / len(d_losses) if d_losses else 0\n",
    "        avg_components = {k: v / len(g_losses) for k, v in loss_component_sums.items()} if g_losses else {}\n",
    "        \n",
    "        # Print stats\n",
    "        elapsed = time.time() - start_time\n",
    "        curr_g_lr = g_optimizer.param_groups[0]['lr']\n",
    "        curr_d_lr = d_optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, G Loss: {avg_g_loss:.4f}, D Loss: {avg_d_loss:.4f}, \"\n",
    "              f\"Content: {avg_components.get('content', 0):.2f}, \"\n",
    "              f\"Edge: {avg_components.get('edge', 0):.2f}, \"\n",
    "              f\"Time: {elapsed:.1f}s, G LR: {curr_g_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_g_loss < best_loss:\n",
    "            best_loss = avg_g_loss\n",
    "            torch.save({\n",
    "                'generator': generator.state_dict(),\n",
    "                'discriminator': discriminator.state_dict(),\n",
    "                'epoch': epoch\n",
    "            }, os.path.join(save_dir, \"best_model.pth\"))\n",
    "            print(f\"New best model saved with G loss: {best_loss:.4f}\")\n",
    "        \n",
    "        # Save latest model\n",
    "        torch.save({\n",
    "            'generator': generator.state_dict(),\n",
    "            'discriminator': discriminator.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, os.path.join(save_dir, \"latest_model.pth\"))\n",
    "        \n",
    "        # Visualization\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, (test_image, test_mask) in enumerate(test_loader):\n",
    "                    test_image = test_image.to(device)\n",
    "                    test_mask = test_mask.to(device)\n",
    "                    \n",
    "                    # Generate output\n",
    "                    test_output = generator(test_image, test_mask)\n",
    "                    \n",
    "                    # Convert to images for display\n",
    "                    input_img = test_image[0].cpu().numpy().transpose(1, 2, 0) * 255\n",
    "                    mask_img = test_mask[0, 0].cpu().numpy() * 255\n",
    "                    output_img = test_output[0].cpu().numpy().transpose(1, 2, 0) * 255\n",
    "                    \n",
    "                    # Clip values\n",
    "                    input_img = np.clip(input_img, 0, 255).astype(np.uint8)\n",
    "                    mask_img = np.clip(mask_img, 0, 255).astype(np.uint8)\n",
    "                    output_img = np.clip(output_img, 0, 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Create visualization\n",
    "                    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                    \n",
    "                    axes[0].imshow(input_img)\n",
    "                    axes[0].set_title(\"Input Image\")\n",
    "                    axes[0].axis(\"off\")\n",
    "                    \n",
    "                    axes[1].imshow(mask_img, cmap='gray')\n",
    "                    axes[1].set_title(\"Reflection Mask\")\n",
    "                    axes[1].axis(\"off\")\n",
    "                    \n",
    "                    axes[2].imshow(output_img)\n",
    "                    axes[2].set_title(\"Reflection Removed\")\n",
    "                    axes[2].axis(\"off\")\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(progress_dir, f\"sample_{i}_epoch_{epoch+1}.png\"))\n",
    "                    plt.close()\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Process video using trained model\n",
    "def process_video(generator, frame_data, output_dir=\"processed_frames\", fps=30):\n",
    "    \"\"\"Process the entire video with the trained model\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create dataset for full video\n",
    "    full_dataset = ReflectionDataset(frame_data, augment=False)\n",
    "    \n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        print(\"Processing video frames...\")\n",
    "        progress_bar = tqdm(enumerate(frame_data), total=len(frame_data))\n",
    "        for i, (frame_path, original_size) in progress_bar:\n",
    "            # Load original frame\n",
    "            orig_frame = cv2.imread(frame_path)\n",
    "            orig_height, orig_width = orig_frame.shape[:2]\n",
    "            \n",
    "            # Get processed frame from dataset\n",
    "            frame_tensor, mask_tensor = full_dataset[i]\n",
    "            \n",
    "            # Skip processing if no reflections detected\n",
    "            if torch.sum(mask_tensor) == 0:\n",
    "                cv2.imwrite(f\"{output_dir}/frame_{i:04d}.png\", orig_frame)\n",
    "                continue\n",
    "            \n",
    "            # Process the frame\n",
    "            frame_tensor = frame_tensor.unsqueeze(0).to(device)\n",
    "            mask_tensor = mask_tensor.unsqueeze(0).to(device)\n",
    "            output = generator(frame_tensor, mask_tensor)\n",
    "            \n",
    "            # Convert output tensor to image\n",
    "            output_img = output[0].cpu().numpy().transpose(1, 2, 0) * 255\n",
    "            output_img = np.clip(output_img, 0, 255).astype(np.uint8)\n",
    "            output_img = cv2.cvtColor(output_img, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # Resize back to original dimensions\n",
    "            output_img = cv2.resize(output_img, (orig_width, orig_height))\n",
    "            \n",
    "            # Save processed frame\n",
    "            cv2.imwrite(f\"{output_dir}/frame_{i:04d}.png\", output_img)\n",
    "    \n",
    "    # Combine frames back into video\n",
    "    print(\"Creating video from processed frames...\")\n",
    "    os.system(f\"ffmpeg -framerate {fps} -i {output_dir}/frame_%04d.png -c:v libx264 -pix_fmt yuv420p -crf 18 processed_video.mp4\")\n",
    "    print(\"Processed video saved as processed_video.mp4\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Extract frames\n",
    "    video_path = \"video.mp4\"\n",
    "    frame_data, video_fps = extract_frames(video_path)\n",
    "    \n",
    "    # Train model\n",
    "    generator = train_model(frame_data, epochs=50, batch_size=8, save_dir=\"results\")\n",
    "    \n",
    "    # Process full video\n",
    "    process_video(generator, frame_data, fps=video_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0886b5b-6dd0-40a0-8ac6-5b10bc928107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd0481-1f22-4941-98d6-f32662ac0de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
