{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c6b2b9-f324-4898-a2a1-1ef97903f75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources cleaned up successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "import threading\n",
    "from queue import Queue\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display # Import IPython.display\n",
    "\n",
    "# --- RealtimeSpecularProcessor class (UNCHANGED from previous correct version) ---\n",
    "# (Paste the full RealtimeSpecularProcessor class here. For brevity, I'm omitting it,\n",
    "#  but ensure it's the one that includes the _rgb_to_hsv_torch method)\n",
    "class RealtimeSpecularProcessor:\n",
    "    def __init__(self, \n",
    "                 wavelet='db4',\n",
    "                 threshold=0.06,\n",
    "                 level=2,  # Reduced from 3 for speed to optimize for real-time\n",
    "                 use_gpu=True,\n",
    "                 detection_scale=0.5,  # Process detection at half resolution for speed\n",
    "                 repository_update_rate=5,  # Update repository every N frames\n",
    "                 repository_alpha=0.05): # Alpha for exponential moving average, lower for more stability\n",
    "        \n",
    "        self.wavelet = wavelet\n",
    "        self.threshold = threshold\n",
    "        self.level = level\n",
    "        self.detection_scale = detection_scale\n",
    "        self.repository_update_rate = repository_update_rate\n",
    "        self.repository_alpha = repository_alpha\n",
    "        \n",
    "        # GPU setup\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda:0\" if self.use_gpu else \"cpu\")\n",
    "        \n",
    "        # Repository management: Stores the averaged background image and its confidence\n",
    "        self.current_repository = None # Stores the averaged background image (float32, HWC)\n",
    "        self.repository_confidence_map = None # Stores confidence/update count for each pixel (float32, HW, range 0-1)\n",
    "        \n",
    "        self.frame_count = 0\n",
    "        \n",
    "        # Performance monitoring\n",
    "        self.processing_times = deque(maxlen=30)\n",
    "        self.fps_history = deque(maxlen=10)\n",
    "        \n",
    "        # Threading for background repository updates (non-blocking)\n",
    "        self.repository_queue = Queue(maxsize=10) \n",
    "        self.repository_thread = threading.Thread(target=self._repository_updater, daemon=True)\n",
    "        self.repository_thread.start()\n",
    "        \n",
    "        print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "        print(f\"Using GPU: {self.use_gpu}\")\n",
    "        print(f\"Detection scale: {self.detection_scale}\")\n",
    "\n",
    "    def _rgb_to_hsv_torch(self, image_rgb_chw):\n",
    "        \"\"\"\n",
    "        Converts a batch of RGB images to HSV.\n",
    "        Input: torch.Tensor (C, H, W) normalized to [0.0, 1.0].\n",
    "               C=3 for R, G, B channels.\n",
    "        Output: torch.Tensor (C, H, W) with H, S, V channels normalized to [0.0, 1.0].\n",
    "        This is a vectorized implementation suitable for GPU.\n",
    "        \"\"\"\n",
    "        r, g, b = image_rgb_chw[0], image_rgb_chw[1], image_rgb_chw[2]\n",
    "\n",
    "        C_max = torch.max(r, torch.max(g, b))\n",
    "        C_min = torch.min(r, torch.min(g, b))\n",
    "        delta = C_max - C_min\n",
    "\n",
    "        h = torch.zeros_like(r, device=self.device)\n",
    "        s = torch.zeros_like(r, device=self.device)\n",
    "        v = C_max\n",
    "\n",
    "        # Saturation\n",
    "        # Only compute saturation where C_max is not zero to avoid division by zero\n",
    "        s[C_max != 0] = delta[C_max != 0] / C_max[C_max != 0]\n",
    "\n",
    "        # Hue\n",
    "        # Handle cases where delta is zero (grayscale) - h remains 0 as initialized\n",
    "        # Case 1: C_max == R\n",
    "        mask_r = (C_max == r) & (delta != 0)\n",
    "        h[mask_r] = (60 * (((g[mask_r] - b[mask_r]) / delta[mask_r]) % 6))\n",
    "\n",
    "        # Case 2: C_max == G\n",
    "        mask_g = (C_max == g) & (delta != 0)\n",
    "        h[mask_g] = (60 * (((b[mask_g] - r[mask_g]) / delta[mask_g]) + 2))\n",
    "\n",
    "        # Case 3: C_max == B\n",
    "        mask_b = (C_max == b) & (delta != 0)\n",
    "        h[mask_b] = (60 * (((r[mask_b] - g[mask_b]) / delta[mask_b]) + 4))\n",
    "\n",
    "        # Normalize hue to [0, 1] (0-360 degrees divided by 360)\n",
    "        h = h / 360.0\n",
    "\n",
    "        return torch.stack([h, s, v], dim=0)\n",
    "    \n",
    "    def _fast_wavelet_features(self, frame_np):\n",
    "        \"\"\"\n",
    "        Optimized wavelet feature extraction on CPU.\n",
    "        (pywt is CPU-bound, so this part remains on CPU).\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        if len(frame_np.shape) == 3:\n",
    "            gray_np = cv2.cvtColor(frame_np, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray_np = frame_np.copy()\n",
    "        \n",
    "        # Downsample for speed if detection_scale < 1.0\n",
    "        if self.detection_scale < 1.0:\n",
    "            new_size = (int(gray_np.shape[1] * self.detection_scale), \n",
    "                       int(gray_np.shape[0] * self.detection_scale))\n",
    "            gray_small_np = cv2.resize(gray_np, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "        else:\n",
    "            gray_small_np = gray_np\n",
    "        \n",
    "        try:\n",
    "            # Apply wavelet transform with reduced levels for speed\n",
    "            coeffs = pywt.wavedec2(gray_small_np, self.wavelet, level=self.level)\n",
    "            \n",
    "            # Initialize detail features with zeros, size of the small grayscale frame\n",
    "            target_shape = gray_small_np.shape\n",
    "            detail_features_np = np.zeros(target_shape, dtype=np.float32)\n",
    "            \n",
    "            # Extract and combine detail coefficients safely.\n",
    "            # coeffs[0] is the approximation coefficient (cA_n).\n",
    "            # coeffs[1:] are tuples (cH_i, cV_i, cD_i) for each level (from coarsest to finest).\n",
    "            for i in range(1, len(coeffs)): \n",
    "                cH, cV, cD = coeffs[i] \n",
    "                \n",
    "                # Absolute values of detail coefficients represent high-frequency energy\n",
    "                h_detail = np.abs(cH)  \n",
    "                v_detail = np.abs(cV)  \n",
    "                \n",
    "                # Resize each detail map to the target shape (downsampled gray frame size)\n",
    "                # This ensures all detail maps are compatible for combination.\n",
    "                if h_detail.shape != target_shape:\n",
    "                    h_detail = cv2.resize(h_detail, (target_shape[1], target_shape[0]), \n",
    "                                        interpolation=cv2.INTER_LINEAR)\n",
    "                if v_detail.shape != target_shape:\n",
    "                    v_detail = cv2.resize(v_detail, (target_shape[1], target_shape[0]), \n",
    "                                        interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "                # Combine by taking maximum across all levels to highlight strongest features\n",
    "                detail_features_np = np.maximum(detail_features_np, h_detail)\n",
    "                detail_features_np = np.maximum(detail_features_np, v_detail)\n",
    "            \n",
    "            # Resize the combined features back to the original full frame size if downsampled\n",
    "            if self.detection_scale < 1.0:\n",
    "                detail_features_np = cv2.resize(detail_features_np, \n",
    "                                           (gray_np.shape[1], gray_np.shape[0]), \n",
    "                                           interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Wavelet processing error: {e}\")\n",
    "            detail_features_np = np.zeros_like(gray_np, dtype=np.float32)\n",
    "        \n",
    "        return detail_features_np\n",
    "    \n",
    "    def _fast_specular_detection(self, frame_torch):\n",
    "        \"\"\"\n",
    "        Optimized specular detection combining wavelets and HSV.\n",
    "        Most operations are performed on GPU when `self.use_gpu` is True.\n",
    "        \"\"\"\n",
    "        detection_start_time = time.time() \n",
    "        \n",
    "        # Wavelet features (CPU-bound due to pywt)\n",
    "        frame_np_cpu = frame_torch.cpu().numpy() # Move to CPU for pywt\n",
    "        wavelet_features_np = self._fast_wavelet_features(frame_np_cpu)\n",
    "        \n",
    "        # Convert wavelet features to torch tensor and move to device (GPU)\n",
    "        wavelet_features_torch = torch.from_numpy(wavelet_features_np).to(self.device)\n",
    "        \n",
    "        # HSV analysis on GPU\n",
    "        frame_normalized_torch = frame_torch.float() / 255.0 \n",
    "        \n",
    "        # Permute from (H, W, C) to (C, H, W) for _rgb_to_hsv_torch\n",
    "        # OpenCV frames are BGR, _rgb_to_hsv_torch expects RGB, so swap channels too.\n",
    "        frame_chw_bgr = frame_normalized_torch.permute(2, 0, 1) \n",
    "        frame_chw_rgb = frame_chw_bgr[[2, 1, 0], :, :] # Swap B and R channels for RGB\n",
    "        \n",
    "        hsv_torch = self._rgb_to_hsv_torch(frame_chw_rgb) # Use the custom HSV conversion\n",
    "        \n",
    "        s_channel_torch = hsv_torch[1, :, :] * 255.0 # Scale S back to 0-255 for thresholding consistency\n",
    "        v_channel_torch = hsv_torch[2, :, :] * 255.0 # Scale V back to 0-255\n",
    "        \n",
    "        # Vectorized operations for speed on GPU\n",
    "        high_brightness_torch = v_channel_torch > 200\n",
    "        low_saturation_torch = s_channel_torch < 40\n",
    "        \n",
    "        # Normalize wavelet features efficiently on GPU\n",
    "        w_min, w_max = wavelet_features_torch.min(), wavelet_features_torch.max()\n",
    "        if w_max > w_min:\n",
    "            normalized_wavelets_torch = (wavelet_features_torch - w_min) / (w_max - w_min)\n",
    "        else:\n",
    "            normalized_wavelets_torch = torch.zeros_like(wavelet_features_torch)\n",
    "        \n",
    "        # Combine detection methods on GPU\n",
    "        wavelet_mask_torch = normalized_wavelets_torch > self.threshold\n",
    "        hsv_mask_torch = high_brightness_torch & low_saturation_torch\n",
    "        \n",
    "        combined_mask_torch = wavelet_mask_torch | hsv_mask_torch\n",
    "        \n",
    "        # Morphological operations (performed on CPU using OpenCV for optimized performance)\n",
    "        combined_mask_np = combined_mask_torch.cpu().numpy().astype(np.uint8)\n",
    "        \n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "        # Use MORPH_OPEN to remove small noise and break tiny connections\n",
    "        refined_mask_np = cv2.morphologyEx(combined_mask_np, cv2.MORPH_OPEN, kernel) \n",
    "        # Then MORPH_CLOSE to fill small holes and connect nearby regions\n",
    "        refined_mask_np = cv2.morphologyEx(refined_mask_np, cv2.MORPH_CLOSE, kernel) \n",
    "        \n",
    "        # Convert refined mask back to torch tensor (boolean type) and move to device for inpainting\n",
    "        refined_mask_torch = torch.from_numpy(refined_mask_np.astype(bool)).to(self.device)\n",
    "        \n",
    "        detection_time_taken = time.time() - detection_start_time\n",
    "        return refined_mask_torch, detection_time_taken\n",
    "    \n",
    "    def _repository_updater(self):\n",
    "        \"\"\"\n",
    "        Background thread for updating the repository.\n",
    "        Processes frames/masks from the queue and updates the EMA repository on CPU.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                frame_data = self.repository_queue.get(timeout=1.0)\n",
    "                if frame_data is None:  # Shutdown signal\n",
    "                    break\n",
    "                \n",
    "                frame_np, mask_np = frame_data\n",
    "                self._update_repository_background(frame_np, mask_np)\n",
    "                self.repository_queue.task_done()\n",
    "                \n",
    "            except Exception: # Catch any exception (e.g., Queue empty timeout) and continue\n",
    "                continue\n",
    "    \n",
    "    def _update_repository_background(self, frame_np, mask_np):\n",
    "        \"\"\"\n",
    "        Update repository (EMA) and confidence map on CPU.\n",
    "        This function runs in a separate thread.\n",
    "        \"\"\"\n",
    "        mask_bool = mask_np.astype(bool)\n",
    "        \n",
    "        if self.current_repository is None:\n",
    "            # Initialize repository with the current frame's non-reflective parts.\n",
    "            self.current_repository = frame_np.astype(np.float32)\n",
    "            # For reflective areas in the first frame, set to 0.0 or a neutral value.\n",
    "            # These areas will only be filled once valid non-specular data is seen.\n",
    "            self.current_repository[mask_bool] = 0.0 \n",
    "            \n",
    "            # Initialize confidence map: 1.0 for non-reflective areas, 0.0 for reflective areas.\n",
    "            self.repository_confidence_map = (~mask_bool).astype(np.float32)\n",
    "        else:\n",
    "            frame_float = frame_np.astype(np.float32)\n",
    "            non_reflective_mask = ~mask_bool\n",
    "            \n",
    "            # Perform Exponential Moving Average (EMA) only on non-reflective pixels.\n",
    "            if np.any(non_reflective_mask):\n",
    "                self.current_repository[non_reflective_mask] = (\n",
    "                    self.repository_alpha * frame_float[non_reflective_mask] + \n",
    "                    (1 - self.repository_alpha) * self.current_repository[non_reflective_mask]\n",
    "                )\n",
    "                # Increase confidence for updated pixels, capping at 1.0.\n",
    "                self.repository_confidence_map[non_reflective_mask] = np.minimum(\n",
    "                    self.repository_confidence_map[non_reflective_mask] + self.repository_alpha, 1.0\n",
    "                )\n",
    "                \n",
    "    def _fast_inpaint(self, frame_torch, mask_torch):\n",
    "        \"\"\"\n",
    "        Fast inpainting using the background repository.\n",
    "        Performed on GPU when `self.use_gpu` is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If repository is not yet initialized, return the original frame.\n",
    "        if self.current_repository is None or self.repository_confidence_map is None:\n",
    "            return frame_torch \n",
    "        \n",
    "        # Convert repository and confidence map to torch tensors and move to device (GPU).\n",
    "        # These arrays are managed on CPU by the background thread, so they need to be moved to GPU here.\n",
    "        repo_torch = torch.from_numpy(self.current_repository).to(self.device)\n",
    "        confidence_torch = torch.from_numpy(self.repository_confidence_map).to(self.device)\n",
    "        \n",
    "        processed_frame_torch = frame_torch.clone()\n",
    "        \n",
    "        # Select reflective areas where the repository has accumulated sufficient confidence.\n",
    "        # A confidence threshold (e.g., > 0.5) ensures we only use reliable background data.\n",
    "        usable_areas_torch = mask_torch & (confidence_torch > 0.5) \n",
    "        \n",
    "        if usable_areas_torch.any():\n",
    "            # Expand the 2D boolean mask to 3 channels to apply to the 3-channel image.\n",
    "            usable_areas_expanded = usable_areas_torch.unsqueeze(-1).expand_as(processed_frame_torch)\n",
    "            \n",
    "            # Apply repository data to the selected reflective areas on the GPU.\n",
    "            processed_frame_torch[usable_areas_expanded] = repo_torch[usable_areas_expanded].to(processed_frame_torch.dtype)\n",
    "        \n",
    "        return processed_frame_torch\n",
    "    \n",
    "    def process_frame_realtime(self, frame_np):\n",
    "        \"\"\"\n",
    "        Main real-time processing function for a single frame.\n",
    "        Handles detection, inpainting, and performance tracking.\n",
    "        \"\"\"\n",
    "        frame_start = time.time()\n",
    "        \n",
    "        # Convert numpy frame (H, W, C) to torch tensor and move to GPU.\n",
    "        frame_torch = torch.from_numpy(frame_np).to(self.device)\n",
    "        \n",
    "        # Detect specular reflections. mask_torch will be on device (GPU).\n",
    "        mask_torch, detection_time = self._fast_specular_detection(frame_torch)\n",
    "        \n",
    "        # Process the frame (inpainting) using the detected mask.\n",
    "        processed_frame_torch = self._fast_inpaint(frame_torch, mask_torch)\n",
    "        \n",
    "        # Convert processed frame and mask back to numpy for OpenCV display/saving.\n",
    "        processed_frame_np = processed_frame_torch.cpu().numpy()\n",
    "        mask_np = mask_torch.cpu().numpy()\n",
    "        \n",
    "        # Queue repository update (non-blocking).\n",
    "        # Send copies of frame and mask to the background thread to prevent data races.\n",
    "        if self.frame_count % self.repository_update_rate == 0:\n",
    "            try:\n",
    "                self.repository_queue.put_nowait((frame_np.copy(), mask_np.copy())) \n",
    "            except Exception:\n",
    "                # Silently skip if queue is full to maintain real-time performance.\n",
    "                pass \n",
    "        \n",
    "        # Performance tracking.\n",
    "        total_time = time.time() - frame_start\n",
    "        self.processing_times.append(total_time)\n",
    "        current_fps = 1.0 / total_time if total_time > 0 else 0\n",
    "        self.fps_history.append(current_fps)\n",
    "        \n",
    "        self.frame_count += 1\n",
    "        \n",
    "        return processed_frame_np, mask_np, {\n",
    "            'fps': current_fps,\n",
    "            'avg_fps': np.mean(self.fps_history) if self.fps_history else 0,\n",
    "            'detection_time': detection_time, # Time specifically for detection stage\n",
    "            'total_time': total_time,        # Total time for the entire frame processing\n",
    "            'frame_count': self.frame_count\n",
    "        }\n",
    "    \n",
    "    def get_performance_stats(self):\n",
    "        \"\"\"Get current performance statistics.\"\"\"\n",
    "        if not self.processing_times:\n",
    "            return {}\n",
    "        \n",
    "        recent_times = list(self.processing_times)\n",
    "        \n",
    "        repository_size_mb = 0\n",
    "        if self.current_repository is not None and self.repository_confidence_map is not None:\n",
    "            # Calculate memory usage of repository data\n",
    "            repository_size_mb = (self.current_repository.nbytes + self.repository_confidence_map.nbytes) / (1024 * 1024)\n",
    "            \n",
    "        return {\n",
    "            'avg_processing_time': np.mean(recent_times),\n",
    "            'max_processing_time': np.max(recent_times),\n",
    "            'min_processing_time': np.min(recent_times),\n",
    "            'avg_fps': np.mean(self.fps_history) if self.fps_history else 0,\n",
    "            'memory_usage': psutil.Process().memory_info().rss / 1024 / 1024,  # Total process memory in MB\n",
    "            'repository_size_mb': repository_size_mb # Memory for repository data itself\n",
    "        }\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources by stopping the background repository thread.\"\"\"\n",
    "        try:\n",
    "            self.repository_queue.put_nowait(None) # Signal shutdown to the thread\n",
    "        except Exception:\n",
    "            pass # Queue might be already closed or full\n",
    "        \n",
    "        if self.repository_thread.is_alive():\n",
    "            self.repository_thread.join(timeout=2.0) # Give the thread some time to terminate\n",
    "            if self.repository_thread.is_alive():\n",
    "                print(\"Warning: Repository thread did not terminate gracefully.\")\n",
    "# --- End RealtimeSpecularProcessor class ---\n",
    "\n",
    "\n",
    "# --- Modified Demo Functions for Jupyter Notebook using IPython.display ---\n",
    "\n",
    "def realtime_camera_demo_jupyter(camera_id=0):\n",
    "    \"\"\"Demo function for real-time camera processing in Jupyter Notebook using IPython.display.\"\"\"\n",
    "    \n",
    "    processor = RealtimeSpecularProcessor(\n",
    "        threshold=0.06, detection_scale=0.7, repository_update_rate=3, use_gpu=True\n",
    "    )\n",
    "    \n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open camera {camera_id}\")\n",
    "        return\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    # Matplotlib Setup (still create figure once)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].set_title('Original')\n",
    "    axes[1].set_title('Processed')\n",
    "    axes[2].set_title('Reflection Mask')\n",
    "    for ax in axes: ax.axis('off')\n",
    "    \n",
    "    print(\"Real-time processing started. Interrupt kernel (Ctrl+C or Stop button) to quit.\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error reading from camera. Exiting.\")\n",
    "                break\n",
    "            \n",
    "            processed_frame, mask, frame_stats = processor.process_frame_realtime(frame)\n",
    "            \n",
    "            # --- Update Matplotlib Plots using IPython.display ---\n",
    "            axes[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            axes[1].imshow(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB))\n",
    "            axes[2].imshow(mask, cmap='gray', vmin=0, vmax=1)\n",
    "            \n",
    "            axes[0].set_title(f'Original\\nFPS: {frame_stats[\"fps\"]:.1f} | Avg: {frame_stats[\"avg_fps\"]:.1f}')\n",
    "            axes[1].set_title(f'Processed\\nDetection: {frame_stats[\"detection_time\"]*1000:.1f}ms | Total: {frame_stats[\"total_time\"]*1000:.1f}ms')\n",
    "            \n",
    "            display.clear_output(wait=True) # Clear previous output\n",
    "            display.display(fig)            # Display updated figure\n",
    "            # --- End Matplotlib Update ---\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcessing interrupted by user.\")\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        processor.cleanup()\n",
    "        plt.close(fig) # Close the matplotlib figure\n",
    "        display.clear_output() # Clear the final display\n",
    "        print(\"Resources cleaned up successfully.\")\n",
    "\n",
    "def realtime_video_demo_jupyter(video_path):\n",
    "    \"\"\"Demo function for real-time video file processing in Jupyter Notebook using IPython.display.\"\"\"\n",
    "    \n",
    "    processor = RealtimeSpecularProcessor(\n",
    "        threshold=0.06, detection_scale=0.8, repository_update_rate=2, use_gpu=True\n",
    "    )\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Matplotlib Setup\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].set_title('Original')\n",
    "    axes[1].set_title('Processed')\n",
    "    axes[2].set_title('Reflection Mask')\n",
    "    for ax in axes: ax.axis('off')\n",
    "        \n",
    "    print(f\"Processing {total_frames} frames from video at {fps} FPS.\")\n",
    "    print(\"Interrupt kernel (Ctrl+C or Stop button) to quit.\")\n",
    "\n",
    "    try:\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            processed_frame, mask, stats = processor.process_frame_realtime(frame)\n",
    "            \n",
    "            # --- Update Matplotlib Plots using IPython.display ---\n",
    "            axes[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            axes[1].imshow(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB))\n",
    "            axes[2].imshow(mask, cmap='gray', vmin=0, vmax=1)\n",
    "            \n",
    "            axes[0].set_title(f'Original\\nFPS: {stats[\"fps\"]:.1f} | Avg: {stats[\"avg_fps\"]:.1f}')\n",
    "            axes[1].set_title(f'Processed\\nDetection: {stats[\"detection_time\"]*1000:.1f}ms | Total: {stats[\"total_time\"]*1000:.1f}ms')\n",
    "            \n",
    "            display.clear_output(wait=True) # Clear previous output\n",
    "            display.display(fig)            # Display updated figure\n",
    "            # --- End Matplotlib Update ---\n",
    "\n",
    "            # Display progress in console (less frequent to avoid flooding)\n",
    "            if frame_count % 30 == 0: \n",
    "                # Console print will be cleared by display.clear_output unless printed after display.display\n",
    "                # For this reason, console prints for progress are less effective with this method\n",
    "                pass \n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "        final_stats = processor.get_performance_stats()\n",
    "        print(\"\\n=== Final Performance Stats ===\") # This will appear after the loop finishes or is interrupted\n",
    "        for k, v in final_stats.items():\n",
    "            print(f\"{k}: {v:.2f}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcessing interrupted by user.\")\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        processor.cleanup()\n",
    "        plt.close(fig)\n",
    "        display.clear_output()\n",
    "        print(\"Resources cleaned up successfully.\")\n",
    "\n",
    "\n",
    "# ============= CONFIGURATION (CHANGE THIS) =============\n",
    "videoname = \"video.mp4\"  # Set this to your video filename\n",
    "\n",
    "# ============= PROCESSING PARAMETERS =============\n",
    "threshold = 0.06\n",
    "max_frames = 1200\n",
    "sample_rate = 10\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block is typically not run directly in a Jupyter cell in the same way.\n",
    "    # You would usually call realtime_camera_demo_jupyter() or realtime_video_demo_jupyter(videoname)\n",
    "    # directly in a Jupyter cell.\n",
    "    \n",
    "#     print(\"To run in Jupyter, call realtime_camera_demo_jupyter() or realtime_video_demo_jupyter('your_video.mp4') directly in a cell.\")\n",
    "    \n",
    "#     Example for direct calling in a Jupyter cell:\n",
    "    if os.path.exists(videoname):\n",
    "        realtime_video_demo_jupyter(videoname)\n",
    "    else:\n",
    "        print(f\"Video '{videoname}' not found. Trying camera demo.\")\n",
    "        realtime_camera_demo_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab81aa-69bb-4c55-9fa8-5de4c5f271f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
