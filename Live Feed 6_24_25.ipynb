{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e3a8a-24a9-4e12-88ad-899831b183f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This program will use the weights from the trainer to 'live' process the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4603051f-0eb6-47ad-89db-2f38234bb020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded from specular_removal_unet_perceptual_emphasis.pth and set to evaluation mode on cuda.\n",
      "Processing video: 2023-08-01_152123_VID003.mp4\n",
      "Source: 512x512 @ 29.97 FPS (Metadata total frames: 19454)\n",
      "Outputting combined video to: output_demo_video3.mp4 (768x384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "End of video or error reading frame.: : 19454it [12:43, 25.49it/s, Frames: 19454, CurFPS: 32.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input video capture released.\n",
      "Output video writer released. Video saved to: output_demo_video3.mp4\n",
      "Video processing finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm # Keep tqdm for progress\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# --- Configuration (Keep as is, or adjust as needed) ---\n",
    "VIDEO_OUTPUT_CONFIG = {\n",
    "    \"INPUT_VIDEO_PATH\": \"2023-08-01_152123_VID003.mp4\",\n",
    "    \"OUTPUT_VIDEO_PATH\": \"output_demo_video3.mp4\", \n",
    "    \"MODEL_PATH\": \"specular_removal_unet_perceptual_emphasis.pth\",\n",
    "    \"TARGET_IMG_SIZE\": (512, 512), \n",
    "    \"OUTPUT_RESOLUTION_SCALE\": 0.75, \n",
    "    \"FPS_AVERAGE_WINDOW\": 30,\n",
    "    \"USE_NORMALIZATION\": False,\n",
    "    \"NORM_MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"NORM_STD\": [0.229, 0.224, 0.225],\n",
    "    \"CONCAT_AXIS\": 1, \n",
    "    \"TEXT_COLOR\": (0, 255, 0), \n",
    "    \"FONT_SCALE\": 0.7,\n",
    "    \"FONT_THICKNESS\": 2,\n",
    "}\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- U-Net Model Definition (Paste your UNet, DoubleConv, Down, Up, OutConv classes here) ---\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels: mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))\n",
    "    def forward(self, x): return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY, diffX = x2.size()[2] - x1.size()[2], x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels_in, n_channels_out, bilinear=True, base_features=64):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = DoubleConv(n_channels_in, base_features)\n",
    "        self.down1 = Down(base_features, base_features * 2)\n",
    "        self.down2 = Down(base_features * 2, base_features * 4)\n",
    "        self.down3 = Down(base_features * 4, base_features * 8)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(base_features * 8, base_features * 16 // factor)\n",
    "        self.up1 = Up(base_features * 16, base_features * 8 // factor, bilinear)\n",
    "        self.up2 = Up(base_features * 8, base_features * 4 // factor, bilinear)\n",
    "        self.up3 = Up(base_features * 4, base_features * 2 // factor, bilinear)\n",
    "        self.up4 = Up(base_features * 2, base_features, bilinear)\n",
    "        self.outc = OutConv(base_features, n_channels_out)\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x); x2 = self.down1(x1); x3 = self.down2(x2); x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4); x = self.up2(x, x3); x = self.up3(x, x2); x = self.up4(x, x1)\n",
    "        return torch.sigmoid(self.outc(x))\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def load_model(model_path, n_channels_in=3, n_channels_out=3, base_features=64):\n",
    "    model = UNet(n_channels_in=n_channels_in, n_channels_out=n_channels_out, base_features=base_features)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "        print(f\"Model loaded from {model_path} and set to evaluation mode on {DEVICE}.\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def preprocess_frame(frame_bgr, target_size, use_normalization=False, norm_mean=None, norm_std=None):\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(frame_rgb)\n",
    "    transform_list = [transforms.Resize(target_size), transforms.ToTensor()]\n",
    "    if use_normalization:\n",
    "        if norm_mean is None or norm_std is None:\n",
    "            raise ValueError(\"Normalization mean and std must be provided if use_normalization is True.\")\n",
    "        transform_list.append(transforms.Normalize(mean=norm_mean, std=norm_std))\n",
    "    preprocess = transforms.Compose(transform_list)\n",
    "    return preprocess(pil_img).unsqueeze(0)\n",
    "\n",
    "def postprocess_output(tensor_output, use_normalization=False, norm_mean=None, norm_std=None):\n",
    "    img_tensor = tensor_output.squeeze(0).cpu().permute(1, 2, 0)\n",
    "    if use_normalization:\n",
    "        if norm_mean is None or norm_std is None:\n",
    "            raise ValueError(\"Normalization mean and std must be provided for postprocessing.\")\n",
    "        mean = torch.tensor(norm_mean).view(1, 1, -1); std = torch.tensor(norm_std).view(1, 1, -1)\n",
    "        img_tensor = img_tensor * std + mean\n",
    "    img_np = (img_tensor.numpy() * 255).clip(0, 255).astype(np.uint8)\n",
    "    return cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# --- Video Processing Function (Revised Loop and Cleanup) ---\n",
    "def process_video_to_file(config):\n",
    "    model = load_model(config[\"MODEL_PATH\"])\n",
    "    if model is None: return # Exit if model loading failed\n",
    "\n",
    "    cap = cv2.VideoCapture(config[\"INPUT_VIDEO_PATH\"])\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {config['INPUT_VIDEO_PATH']}\")\n",
    "        return\n",
    "\n",
    "    out_video = None # Initialize to None\n",
    "\n",
    "    try:\n",
    "        source_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        source_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        source_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames_metadata = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Metadata count\n",
    "\n",
    "        display_w = int(source_w * config[\"OUTPUT_RESOLUTION_SCALE\"])\n",
    "        display_h = int(source_h * config[\"OUTPUT_RESOLUTION_SCALE\"])\n",
    "\n",
    "        if config[\"CONCAT_AXIS\"] == 1: out_w, out_h = display_w * 2, display_h\n",
    "        else: out_w, out_h = display_w, display_h * 2\n",
    "        \n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out_video = cv2.VideoWriter(config[\"OUTPUT_VIDEO_PATH\"], fourcc, source_fps, (out_w, out_h))\n",
    "        if not out_video.isOpened():\n",
    "            print(f\"Error: Could not open VideoWriter for {config['OUTPUT_VIDEO_PATH']}\")\n",
    "            return # No cap.release() here as it's in finally\n",
    "\n",
    "        print(f\"Processing video: {config['INPUT_VIDEO_PATH']}\")\n",
    "        print(f\"Source: {source_w}x{source_h} @ {source_fps:.2f} FPS (Metadata total frames: {total_frames_metadata})\")\n",
    "        print(f\"Outputting combined video to: {config['OUTPUT_VIDEO_PATH']} ({out_w}x{out_h})\")\n",
    "\n",
    "        fps_deque = deque(maxlen=config[\"FPS_AVERAGE_WINDOW\"])\n",
    "        \n",
    "        # Use tqdm without specifying total initially, update manually\n",
    "        pbar = tqdm(desc=\"Processing Video\")\n",
    "        frame_count_processed = 0\n",
    "\n",
    "        while True: # More robust loop\n",
    "            ret, original_frame_full_res = cap.read()\n",
    "            if not ret:\n",
    "                pbar.set_description(\"End of video or error reading frame.\")\n",
    "                break # Exit loop if no frame or error\n",
    "\n",
    "            process_start_time = time.time()\n",
    "            input_tensor = preprocess_frame(\n",
    "                original_frame_full_res, config[\"TARGET_IMG_SIZE\"],\n",
    "                config[\"USE_NORMALIZATION\"], config[\"NORM_MEAN\"], config[\"NORM_STD\"]\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_tensor = model(input_tensor)\n",
    "            \n",
    "            processed_frame_model_size = postprocess_output(\n",
    "                output_tensor, config[\"USE_NORMALIZATION\"],\n",
    "                config[\"NORM_MEAN\"], config[\"NORM_STD\"]\n",
    "            )\n",
    "            process_time = time.time() - process_start_time\n",
    "            \n",
    "            frame_display_orig = cv2.resize(original_frame_full_res, (display_w, display_h))\n",
    "            frame_display_processed = cv2.resize(processed_frame_model_size, (display_w, display_h))\n",
    "\n",
    "            if config[\"CONCAT_AXIS\"] == 1:\n",
    "                combined_frame = np.concatenate((frame_display_orig, frame_display_processed), axis=1)\n",
    "            else:\n",
    "                combined_frame = np.concatenate((frame_display_orig, frame_display_processed), axis=0)\n",
    "\n",
    "            current_fps = 1.0 / process_time if process_time > 0 else float('inf')\n",
    "            fps_deque.append(current_fps)\n",
    "            avg_fps = np.mean(fps_deque) if fps_deque else 0\n",
    "\n",
    "            fps_text = f\"FPS: {current_fps:.1f} (Avg: {avg_fps:.1f})\"\n",
    "            infer_text = f\"Infer: {process_time*1000:.1f}ms\"\n",
    "            cv2.putText(combined_frame, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        config[\"FONT_SCALE\"], config[\"TEXT_COLOR\"], config[\"FONT_THICKNESS\"], cv2.LINE_AA)\n",
    "            cv2.putText(combined_frame, infer_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        config[\"FONT_SCALE\"], config[\"TEXT_COLOR\"], config[\"FONT_THICKNESS\"], cv2.LINE_AA)\n",
    "            \n",
    "            out_video.write(combined_frame)\n",
    "            \n",
    "            frame_count_processed += 1\n",
    "            pbar.update(1) # Manually update tqdm progress\n",
    "            pbar.set_postfix_str(f\"Frames: {frame_count_processed}, CurFPS: {current_fps:.1f}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\")\n",
    "    finally:\n",
    "        pbar.close() # Ensure tqdm progress bar is closed\n",
    "        if cap.isOpened():\n",
    "            cap.release()\n",
    "            print(\"Input video capture released.\")\n",
    "        if out_video is not None and out_video.isOpened(): # Check if out_video was successfully opened\n",
    "            out_video.release()\n",
    "            print(f\"Output video writer released. Video saved to: {config['OUTPUT_VIDEO_PATH']}\")\n",
    "        cv2.destroyAllWindows() # Good practice\n",
    "        print(\"Video processing finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(VIDEO_OUTPUT_CONFIG[\"MODEL_PATH\"]):\n",
    "        print(f\"Error: Model file not found at {VIDEO_OUTPUT_CONFIG['MODEL_PATH']}\")\n",
    "    elif not os.path.exists(VIDEO_OUTPUT_CONFIG[\"INPUT_VIDEO_PATH\"]):\n",
    "        print(f\"Error: Input video file not found at {VIDEO_OUTPUT_CONFIG['INPUT_VIDEO_PATH']}\")\n",
    "    else:\n",
    "        process_video_to_file(VIDEO_OUTPUT_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6642c-1cb3-48b9-a357-d2cb6a0362a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
