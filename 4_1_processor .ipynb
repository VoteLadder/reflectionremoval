{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c879a86c-b045-4ac7-ab48-06d5cbc5ca07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: video.mp4\n",
      "Output will be saved to the 'reflection_processing' directory\n",
      "Using fill radius of 10 pixels for neighborhood averaging\n",
      "GPU Available: True\n",
      "Using GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  72%|███████▏  | 1114/1548 [02:31<00:58,  7.38it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 362\u001b[0m\n\u001b[1;32m    354\u001b[0m processor \u001b[38;5;241m=\u001b[39m SpecularReflectionProcessor(\n\u001b[1;32m    355\u001b[0m     threshold\u001b[38;5;241m=\u001b[39mthreshold,\n\u001b[1;32m    356\u001b[0m     highlight_color\u001b[38;5;241m=\u001b[39mhighlight_color,\n\u001b[1;32m    357\u001b[0m     use_gpu\u001b[38;5;241m=\u001b[39muse_gpu,\n\u001b[1;32m    358\u001b[0m     debug\u001b[38;5;241m=\u001b[39muse_debug\n\u001b[1;32m    359\u001b[0m )\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# Process the video\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideoname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_base_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_radius\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_radius\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Display sample results\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDisplaying sample results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 242\u001b[0m, in \u001b[0;36mSpecularReflectionProcessor.process_video\u001b[0;34m(self, input_path, output_prefix, sample_rate, max_frames, fill_radius)\u001b[0m\n\u001b[1;32m    239\u001b[0m specular_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_specular_regions(frame)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Update the baseline repository with non-reflective areas\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_baseline_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecular_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Process the frame if it's time to save it\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m sample_rate \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Create highlighted version\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 155\u001b[0m, in \u001b[0;36mSpecularReflectionProcessor._update_baseline_repository\u001b[0;34m(self, frame, mask)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaseline_repository[non_reflective_areas] \u001b[38;5;241m=\u001b[39m frame[non_reflective_areas]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Keep track of the most recent valid frame\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_valid_frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Specular Reflection Processor for Endoscopic Video\n",
    "\n",
    "Just change the videoname variable at the top of this file and run the entire script.\n",
    "The program will process the video and save samples to their respective folders.\n",
    "\"\"\"\n",
    "\n",
    "# ============= CONFIGURATION (CHANGE THIS) =============\n",
    "videoname = \"video.mp4\"  # Set this to your video filename\n",
    "\n",
    "# ============= PROCESSING PARAMETERS =============\n",
    "# You can adjust these parameters as needed\n",
    "threshold = 0.08        # Threshold for specular reflection detection (0.05-0.20)\n",
    "sample_rate = 10        # Save a frame every N frames\n",
    "max_frames = None       # Set to a number to limit frames processed, or None for all\n",
    "highlight_color = (0, 255, 0)  # Green color for highlighting reflections\n",
    "use_debug = True        # Whether to save debug visualizations\n",
    "use_gpu = True          # Whether to use GPU for processing if available\n",
    "fill_radius = 10        # Radius (in pixels) to average for filling when no data is available\n",
    "\n",
    "# ============= IMPORTS =============\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm  # Using tqdm instead of tqdm.notebook for broader compatibility\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# ============= MAIN CLASS =============\n",
    "class SpecularReflectionProcessor:\n",
    "    def __init__(self, \n",
    "                 wavelet='db4',         \n",
    "                 threshold=0.15,        \n",
    "                 level=3,               \n",
    "                 use_gpu=True,          \n",
    "                 highlight_color=(0, 255, 0),  \n",
    "                 debug=True):           \n",
    "        \n",
    "        self.wavelet = wavelet\n",
    "        self.threshold = threshold\n",
    "        self.level = level\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.highlight_color = highlight_color\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = \"reflection_processing\"\n",
    "        self.original_dir = os.path.join(self.output_dir, \"original\")\n",
    "        self.highlighted_dir = os.path.join(self.output_dir, \"highlighted\")\n",
    "        self.processed_dir = os.path.join(self.output_dir, \"processed\")  \n",
    "        self.mask_dir = os.path.join(self.output_dir, \"masks\")\n",
    "        self.debug_dir = os.path.join(self.output_dir, \"debug\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        for directory in [self.output_dir, self.original_dir, self.highlighted_dir, \n",
    "                          self.processed_dir, self.mask_dir, self.debug_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "        print(f\"Using GPU: {self.use_gpu}\")\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        # Initialize repository of non-reflective pixels\n",
    "        self.baseline_repository = None\n",
    "        self.last_valid_frame = None\n",
    "    \n",
    "    def _calculate_wavelet_features(self, frame):\n",
    "        \"\"\"Calculate wavelet decomposition features from the frame.\"\"\"\n",
    "        # Convert to grayscale for wavelet analysis\n",
    "        if len(frame.shape) == 3:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = frame.copy()\n",
    "            \n",
    "        # Apply wavelet transform\n",
    "        coeffs = pywt.wavedec2(gray, self.wavelet, level=self.level)\n",
    "        \n",
    "        # Extract high-frequency components (details)\n",
    "        details = []\n",
    "        for detail_coeffs in coeffs[1:]:\n",
    "            details.extend([np.abs(detail_coeffs[i]) for i in range(3)])\n",
    "        \n",
    "        # Normalize and stack details\n",
    "        detail_features = np.stack([cv2.resize(d, (gray.shape[1], gray.shape[0])) for d in details])\n",
    "        detail_features = np.max(detail_features, axis=0)\n",
    "        \n",
    "        return detail_features\n",
    "    \n",
    "    def _detect_specular_regions(self, frame):\n",
    "        \"\"\"Detect specular reflection regions using wavelets and HSV analysis.\"\"\"\n",
    "        # Calculate wavelet features\n",
    "        features = self._calculate_wavelet_features(frame)\n",
    "        \n",
    "        # Convert to HSV for better highlight detection\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        \n",
    "        # High Value and low Saturation indicates specular reflection\n",
    "        high_v = v > 200\n",
    "        low_s = s < 40\n",
    "        \n",
    "        # Combine with wavelet features\n",
    "        normalized_features = (features - np.min(features)) / (np.max(features) - np.min(features) + 1e-6)\n",
    "        wavelet_mask = normalized_features > self.threshold\n",
    "        \n",
    "        # Combine masks\n",
    "        combined_mask = np.logical_or(np.logical_and(high_v, low_s), wavelet_mask)\n",
    "        \n",
    "        # Clean up the mask\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.morphologyEx(combined_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Further refine the mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        refined_mask = np.zeros_like(mask)\n",
    "        \n",
    "        # Only keep contours of meaningful size\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 10:  # Minimum area threshold\n",
    "                cv2.drawContours(refined_mask, [contour], -1, 1, -1)\n",
    "        \n",
    "        return refined_mask\n",
    "    \n",
    "    def _create_highlighted_image(self, frame, mask):\n",
    "        \"\"\"Create version of frame with reflections highlighted in green.\"\"\"\n",
    "        highlighted = frame.copy()\n",
    "        mask_bool = mask.astype(bool)\n",
    "        highlighted[mask_bool] = self.highlight_color\n",
    "        return highlighted\n",
    "    \n",
    "    def _update_baseline_repository(self, frame, mask):\n",
    "        \"\"\"Update repository of non-reflective pixels.\"\"\"\n",
    "        if self.baseline_repository is None:\n",
    "            # Initialize the repository with the current frame\n",
    "            self.baseline_repository = frame.copy()\n",
    "            # Set reflective areas to zeros (to be filled later)\n",
    "            self.baseline_repository[mask.astype(bool)] = 0\n",
    "            self.last_valid_frame = frame.copy()\n",
    "        else:\n",
    "            # Update only the non-reflective areas\n",
    "            non_reflective_areas = ~mask.astype(bool)\n",
    "            self.baseline_repository[non_reflective_areas] = frame[non_reflective_areas]\n",
    "            \n",
    "            # Keep track of the most recent valid frame\n",
    "            self.last_valid_frame = frame.copy()\n",
    "    \n",
    "    def _inpaint_reflections(self, frame, mask, radius=10):\n",
    "        \"\"\"\n",
    "        Replace reflective areas using baseline repository only. \n",
    "        If no repository data exists, keep original pixels.\n",
    "        \n",
    "        Args:\n",
    "            frame: The input frame\n",
    "            mask: Binary mask of reflective areas\n",
    "            radius: Unused parameter (kept for compatibility)\n",
    "            \n",
    "        Returns:\n",
    "            Processed frame with reflections removed where repository data exists\n",
    "        \"\"\"\n",
    "        if self.baseline_repository is None:\n",
    "            # If no repository exists yet, just return the original frame\n",
    "            return frame.copy()\n",
    "        \n",
    "        # Create a processed frame\n",
    "        processed = frame.copy()\n",
    "        \n",
    "        # Get reflective areas\n",
    "        reflective_areas = mask.astype(bool)\n",
    "        \n",
    "        # Use the baseline repository where valid data exists\n",
    "        valid_repository = (self.baseline_repository != 0).all(axis=2) if len(self.baseline_repository.shape) == 3 else (self.baseline_repository != 0)\n",
    "        usable_repository = valid_repository & reflective_areas\n",
    "        \n",
    "        if np.any(usable_repository):\n",
    "            # Use repository data where available\n",
    "            processed[usable_repository] = self.baseline_repository[usable_repository]\n",
    "        \n",
    "        # For areas not in repository, keep original pixels\n",
    "        # (This happens automatically since we're using a copy of the original frame)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def process_video(self, input_path, output_prefix, sample_rate=10, max_frames=None, fill_radius=10):\n",
    "        \"\"\"\n",
    "        Process video to detect, highlight and remove specular reflections.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input video file\n",
    "            output_prefix: Prefix for output filenames\n",
    "            sample_rate: Save a frame every N frames\n",
    "            max_frames: Maximum number of frames to process\n",
    "            fill_radius: Radius for neighborhood averaging when no data is available\n",
    "        \"\"\"\n",
    "        # Open the video file\n",
    "        video = cv2.VideoCapture(input_path)\n",
    "        if not video.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {input_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        # For creating output video\n",
    "        output_video_path = os.path.join(self.output_dir, f\"{output_prefix}_processed.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps // sample_rate, (width, height))\n",
    "        \n",
    "        # Limit frames if specified\n",
    "        if max_frames is not None:\n",
    "            frame_count = min(frame_count, max_frames)\n",
    "        \n",
    "        # Process each frame\n",
    "        processed_frames = 0\n",
    "        saved_frames = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.baseline_repository = None  # Reset repository\n",
    "        \n",
    "        for i in tqdm(range(frame_count), desc=\"Processing video\"):\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process every frame for repository updates\n",
    "            # Detect specular reflections\n",
    "            specular_mask = self._detect_specular_regions(frame)\n",
    "            \n",
    "            # Update the baseline repository with non-reflective areas\n",
    "            self._update_baseline_repository(frame, specular_mask)\n",
    "            \n",
    "            # Process the frame if it's time to save it\n",
    "            if i % sample_rate == 0:\n",
    "                # Create highlighted version\n",
    "                highlighted_frame = self._create_highlighted_image(frame, specular_mask)\n",
    "                \n",
    "                # Create processed version (with reflections removed)\n",
    "                processed_frame = self._inpaint_reflections(frame, specular_mask, radius=fill_radius)\n",
    "                \n",
    "                # Save the frames\n",
    "                frame_filename = f\"{output_prefix}_{saved_frames:04d}\"\n",
    "                \n",
    "                cv2.imwrite(os.path.join(self.original_dir, f\"{frame_filename}.jpg\"), frame)\n",
    "                cv2.imwrite(os.path.join(self.highlighted_dir, f\"{frame_filename}.jpg\"), highlighted_frame)\n",
    "                cv2.imwrite(os.path.join(self.processed_dir, f\"{frame_filename}.jpg\"), processed_frame)\n",
    "                cv2.imwrite(os.path.join(self.mask_dir, f\"{frame_filename}.png\"), specular_mask * 255)\n",
    "                \n",
    "                # Write to video\n",
    "                out.write(processed_frame)\n",
    "                \n",
    "                if self.debug and saved_frames % 10 == 0:\n",
    "                    # Create visualization of the detection\n",
    "                    wavelet_features = self._calculate_wavelet_features(frame)\n",
    "                    normalized_features = (wavelet_features - np.min(wavelet_features)) / (np.max(wavelet_features) - np.min(wavelet_features) + 1e-6)\n",
    "                    wavelet_vis = cv2.applyColorMap((normalized_features * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "                    \n",
    "                    # Create side-by-side comparison for debugging\n",
    "                    top_row = np.hstack((frame, highlighted_frame))\n",
    "                    bottom_row = np.hstack((processed_frame, wavelet_vis))\n",
    "                    comparison = np.vstack((top_row, bottom_row))\n",
    "                    \n",
    "                    cv2.imwrite(os.path.join(self.debug_dir, f\"{frame_filename}_comparison.jpg\"), comparison)\n",
    "                \n",
    "                saved_frames += 1\n",
    "            \n",
    "            processed_frames += 1\n",
    "        \n",
    "        # Clean up\n",
    "        video.release()\n",
    "        out.release()\n",
    "        \n",
    "        # Calculate processing stats\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Processed {processed_frames} frames and saved {saved_frames} frame sets in {processing_time:.2f} seconds\")\n",
    "        print(f\"Output video saved to: {output_video_path}\")\n",
    "        \n",
    "        return saved_frames\n",
    "    \n",
    "    def display_sample_results(self, num_samples=3):\n",
    "        \"\"\"Display sample results using matplotlib.\"\"\"\n",
    "        original_files = sorted(glob.glob(os.path.join(self.original_dir, \"*.jpg\")))\n",
    "        if not original_files:\n",
    "            print(\"No processed files found.\")\n",
    "            return\n",
    "            \n",
    "        # Select a few samples (every 10th saved frame)\n",
    "        samples = []\n",
    "        for i, path in enumerate(original_files):\n",
    "            if i % 10 == 0 and len(samples) < num_samples:\n",
    "                samples.append(path)\n",
    "        \n",
    "        if not samples:\n",
    "            samples = original_files[:num_samples]\n",
    "            \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(len(samples), 3, figsize=(15, 5 * len(samples)))\n",
    "        if len(samples) == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for i, orig_path in enumerate(samples):\n",
    "            base_name = os.path.basename(orig_path)\n",
    "            \n",
    "            # Load images\n",
    "            original = cv2.imread(orig_path)\n",
    "            highlighted = cv2.imread(os.path.join(self.highlighted_dir, base_name))\n",
    "            processed = cv2.imread(os.path.join(self.processed_dir, base_name))\n",
    "            \n",
    "            # Convert to RGB for display\n",
    "            original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "            highlighted = cv2.cvtColor(highlighted, cv2.COLOR_BGR2RGB)\n",
    "            processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Display\n",
    "            axes[i][0].imshow(original)\n",
    "            axes[i][0].set_title('Original')\n",
    "            axes[i][0].axis('off')\n",
    "            \n",
    "            axes[i][1].imshow(highlighted)\n",
    "            axes[i][1].set_title('Reflections Highlighted')\n",
    "            axes[i][1].axis('off')\n",
    "            \n",
    "            axes[i][2].imshow(processed)\n",
    "            axes[i][2].set_title('Processed (Reflections Removed)')\n",
    "            axes[i][2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"sample_results.png\"))\n",
    "        plt.show()\n",
    "\n",
    "# ============= MAIN FUNCTION =============\n",
    "if __name__ == \"__main__\":\n",
    "    # Extract video name without extension\n",
    "    video_base_name = os.path.splitext(os.path.basename(videoname))[0]\n",
    "    \n",
    "    print(f\"Processing video: {videoname}\")\n",
    "    print(f\"Output will be saved to the 'reflection_processing' directory\")\n",
    "    print(f\"Using fill radius of {fill_radius} pixels for neighborhood averaging\")\n",
    "    \n",
    "    # Create processor with specified parameters\n",
    "    processor = SpecularReflectionProcessor(\n",
    "        threshold=threshold,\n",
    "        highlight_color=highlight_color,\n",
    "        use_gpu=use_gpu,\n",
    "        debug=use_debug\n",
    "    )\n",
    "    \n",
    "    # Process the video\n",
    "    processor.process_video(\n",
    "        videoname,\n",
    "        video_base_name,\n",
    "        sample_rate=sample_rate,\n",
    "        max_frames=max_frames,\n",
    "        fill_radius=fill_radius\n",
    "    )\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nDisplaying sample results...\")\n",
    "    processor.display_sample_results(num_samples=3)\n",
    "    \n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(\"Results saved in the following directories:\")\n",
    "    print(f\"  - Original frames: {processor.original_dir}\")\n",
    "    print(f\"  - Highlighted frames: {processor.highlighted_dir}\")\n",
    "    print(f\"  - Processed frames: {processor.processed_dir}\")\n",
    "    print(f\"  - Masks: {processor.mask_dir}\")\n",
    "    if use_debug:\n",
    "        print(f\"  - Debug visualizations: {processor.debug_dir}\")\n",
    "    print(f\"  - Processed video: {os.path.join(processor.output_dir, f'{video_base_name}_processed.mp4')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ba94e-e1f1-4a33-ad3a-5f52edc7b1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
