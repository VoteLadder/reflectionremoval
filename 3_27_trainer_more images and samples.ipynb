{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea893385-97a9-4fe0-b6a7-e8229eb28ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_wavelets\n",
      "  Downloading pytorch_wavelets-1.3.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch_wavelets) (1.26.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pytorch_wavelets) (1.16.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch_wavelets) (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch_wavelets) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->pytorch_wavelets) (1.3.0)\n",
      "Downloading pytorch_wavelets-1.3.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch_wavelets\n",
      "Successfully installed pytorch_wavelets-1.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0477b6-b2c5-4a57-a958-4d559fa01584",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear image directory: /notebooks/output_3_18/smokeV2/clear\n",
      "Blurry image directory: /notebooks/output_3_18/smokeV2/blurry\n",
      "Output directory: /notebooks/output_3_18/smoke/output_V2\n",
      "Using device: cuda\n",
      "Limiting training dataset from 10212 to 5000 samples.\n",
      "Using 4 dataloader workers.\n",
      "Selecting 5 fixed random indices for sampling from range [5000, 10211].\n",
      "Using fixed sample indices: [8009, 8703, 6311, 9161, 6144]\n",
      "Using 10212 samples for this dataset instance.\n",
      "Saved initial prediction sample.\n",
      "Training dataset size: 5000\n",
      "Total available pairs: 10212\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 625/625 [13:28<00:00,  1.29s/it, avg_loss=0.015049, loss=0.008244, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 finished. Average Training Loss: 0.015049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 625/625 [13:47<00:00,  1.32s/it, avg_loss=0.007532, loss=0.006601, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 finished. Average Training Loss: 0.007532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 625/625 [13:35<00:00,  1.30s/it, avg_loss=0.006383, loss=0.005555, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 finished. Average Training Loss: 0.006383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 625/625 [13:54<00:00,  1.34s/it, avg_loss=0.006428, loss=0.005662, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 finished. Average Training Loss: 0.006428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 625/625 [13:44<00:00,  1.32s/it, avg_loss=0.005642, loss=0.004905, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 finished. Average Training Loss: 0.005642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 625/625 [13:51<00:00,  1.33s/it, avg_loss=0.005253, loss=0.004955, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 finished. Average Training Loss: 0.005253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 625/625 [13:49<00:00,  1.33s/it, avg_loss=0.004848, loss=0.004565, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 finished. Average Training Loss: 0.004848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 625/625 [13:46<00:00,  1.32s/it, avg_loss=0.004682, loss=0.004280, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 finished. Average Training Loss: 0.004682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 625/625 [14:06<00:00,  1.36s/it, avg_loss=0.004364, loss=0.004044, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 finished. Average Training Loss: 0.004364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50:  34%|███▍      | 211/625 [04:41<09:12,  1.33s/it, avg_loss=0.004236, loss=0.003780, lr=5.0e-04]\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fc761428e10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 545\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;66;03m# Using 5000 samples and batch size 8 as per last user code\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclear_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblurry_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# As used in the previous code\u001b[39;49;00m\n\u001b[1;32m    549\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwavelet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdb1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_samples_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# As used in the previous code\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[1;32m    556\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining process completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 426\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(clear_dir, blurry_dir, output_dir, epochs, batch_size, lr, wavelet, alpha, weight_decay, max_samples_train)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: pred \u001b[38;5;241m=\u001b[39m model(blurry_img, wavelet_input)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mForward pass error batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m); traceback\u001b[38;5;241m.\u001b[39mprint_exc(); \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoss calculation error batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m); traceback\u001b[38;5;241m.\u001b[39mprint_exc(); \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss): \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN loss detected at batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, skipping backward/step.\u001b[39m\u001b[38;5;124m\"\u001b[39m); optimizer\u001b[38;5;241m.\u001b[39mzero_grad(); \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 205\u001b[0m, in \u001b[0;36mCombinedLossDWT.forward\u001b[0;34m(self, pred, target)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         pred_wavelet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_dwt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m         target_wavelet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dwt(target)\n\u001b[1;32m    207\u001b[0m         freq_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_loss(pred_wavelet, target_wavelet)\n",
      "Cell \u001b[0;32mIn[1], line 184\u001b[0m, in \u001b[0;36mCombinedLossDWT._compute_dwt\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_dwt\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Input x shape: [B, C, H, W]\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m# Ensure input is on the correct device\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     LL, H_coeffs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdwt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# LL shape: [B, C, H/2, W/2]\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# H_coeffs is a list containing one tensor for J=1\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# H_coeffs[0] shape: [B, C, 3, H/2, W/2] where 3 is for LH, HL, HH\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     details \u001b[38;5;241m=\u001b[39m H_coeffs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_wavelets/dwt/transform2d.py:70\u001b[0m, in \u001b[0;36mDWTForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Do a multilevel transform\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Do 1 level of the transform\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     ll, high \u001b[38;5;241m=\u001b[39m \u001b[43mlowlevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAFB2D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh0_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh1_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh0_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh1_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     yh\u001b[38;5;241m.\u001b[39mappend(high)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ll, yh\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_wavelets/dwt/lowlevel.py:341\u001b[0m, in \u001b[0;36mAFB2D.forward\u001b[0;34m(ctx, x, h0_row, h1_row, h0_col, h1_col, mode)\u001b[0m\n\u001b[1;32m    339\u001b[0m mode \u001b[38;5;241m=\u001b[39m int_to_mode(mode)\n\u001b[1;32m    340\u001b[0m ctx\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 341\u001b[0m lohi \u001b[38;5;241m=\u001b[39m \u001b[43mafb1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh1_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m y \u001b[38;5;241m=\u001b[39m afb1d(lohi, h0_col, h1_col, mode\u001b[38;5;241m=\u001b[39mmode, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    343\u001b[0m s \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_wavelets/dwt/lowlevel.py:168\u001b[0m, in \u001b[0;36mafb1d\u001b[0;34m(x, h0, h1, mode, dim)\u001b[0m\n\u001b[1;32m    166\u001b[0m     pad \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, p\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, (p\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (p\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, (p\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    167\u001b[0m     x \u001b[38;5;241m=\u001b[39m mypad(x, pad\u001b[38;5;241m=\u001b[39mpad, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m--> 168\u001b[0m     lohi \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnkown pad type: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(mode))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pywt\n",
    "from tqdm import tqdm\n",
    "from pytorch_wavelets import DWTForward\n",
    "import traceback\n",
    "import random # Import random for sampling\n",
    "\n",
    "# Custom Dataset for Paired Smoky/Clear Images\n",
    "class ColonoscopyDataset(Dataset):\n",
    "    def __init__(self, clear_dir, blurry_dir, wavelet='db1', max_samples=None):\n",
    "        try:\n",
    "            # Load all files first to determine total count before limiting\n",
    "            all_clear_files = sorted([f for f in os.listdir(clear_dir) if os.path.isfile(os.path.join(clear_dir, f))])\n",
    "            all_blurry_files = sorted([f for f in os.listdir(blurry_dir) if os.path.isfile(os.path.join(blurry_dir, f))])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error accessing directories: {e}\")\n",
    "            raise e\n",
    "\n",
    "        if not all_clear_files or not all_blurry_files:\n",
    "            raise ValueError(f\"No files found in directories: Clear={len(all_clear_files)}, Blurry={len(all_blurry_files)}\")\n",
    "\n",
    "        min_len = min(len(all_clear_files), len(all_blurry_files))\n",
    "        if len(all_clear_files) != len(all_blurry_files):\n",
    "            print(f\"Warning: Mismatched file counts. Using {min_len} pairs.\")\n",
    "            self.clear_files = all_clear_files[:min_len]\n",
    "            self.blurry_files = all_blurry_files[:min_len]\n",
    "        else:\n",
    "            self.clear_files = all_clear_files\n",
    "            self.blurry_files = all_blurry_files\n",
    "\n",
    "        self.total_available_samples = len(self.clear_files) # Store total count before limiting\n",
    "\n",
    "        # Apply max_samples limit for the training dataset instance\n",
    "        if max_samples is not None and max_samples > 0 and self.total_available_samples > max_samples:\n",
    "            print(f\"Limiting training dataset from {self.total_available_samples} to {max_samples} samples.\")\n",
    "            self.clear_files = self.clear_files[:max_samples]\n",
    "            self.blurry_files = self.blurry_files[:max_samples]\n",
    "        else:\n",
    "             # If max_samples is None or not limiting, use all available matched pairs\n",
    "             print(f\"Using {len(self.clear_files)} samples for this dataset instance.\") # Use len(self.clear_files) after potential limit\n",
    "\n",
    "\n",
    "        self.clear_dir = clear_dir\n",
    "        self.blurry_dir = blurry_dir\n",
    "        self.wavelet = wavelet\n",
    "\n",
    "    def __len__(self):\n",
    "        # This length reflects the samples used by THIS dataset instance (potentially limited)\n",
    "        return len(self.clear_files)\n",
    "\n",
    "    def get_total_available_samples(self):\n",
    "        # Helper to get the count BEFORE applying max_samples limit\n",
    "        return self.total_available_samples\n",
    "\n",
    "    def get_filenames_by_index(self, idx):\n",
    "        # Helper to get filenames for specific indices (relative to the potentially limited list of this instance)\n",
    "         if idx >= len(self.clear_files):\n",
    "                raise IndexError(f\"Index {idx} out of bounds for current dataset size {len(self.clear_files)}\")\n",
    "         return self.clear_files[idx], self.blurry_files[idx]\n",
    "\n",
    "    def _load_and_preprocess_image(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            base, ext = os.path.splitext(img_path)\n",
    "            if not ext:\n",
    "                for try_ext in [\".png\", \".jpg\", \".jpeg\"]: # Added jpeg\n",
    "                    img = cv2.imread(img_path + try_ext)\n",
    "                    if img is not None:\n",
    "                        break\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        elif img.shape[2] == 4:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        elif img.shape[2] != 3:\n",
    "            raise ValueError(f\"Image {img_path} has unexpected shape {img.shape}\")\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # This uses the potentially limited file lists of the current instance\n",
    "            clear_file, blurry_file = self.get_filenames_by_index(idx)\n",
    "            clear_img_path = os.path.join(self.clear_dir, clear_file)\n",
    "            blurry_img_path = os.path.join(self.blurry_dir, blurry_file)\n",
    "\n",
    "            clear_img_np = self._load_and_preprocess_image(clear_img_path)\n",
    "            blurry_img_np = self._load_and_preprocess_image(blurry_img_path)\n",
    "\n",
    "            # Resize blurry to match clear if needed\n",
    "            if clear_img_np.shape[:2] != blurry_img_np.shape[:2]:\n",
    "                target_h, target_w = clear_img_np.shape[:2]\n",
    "                blurry_img_np = cv2.resize(blurry_img_np, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            clear_img_tensor = torch.from_numpy(clear_img_np).permute(2, 0, 1).float()\n",
    "            blurry_img_tensor = torch.from_numpy(blurry_img_np).permute(2, 0, 1).float()\n",
    "\n",
    "            return blurry_img_tensor, clear_img_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item at index {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            # Fallback for errors during training iteration\n",
    "            if idx > 0:\n",
    "                try:\n",
    "                    return self.__getitem__(0) # Try loading the first item\n",
    "                except: # If even item 0 fails, return dummy\n",
    "                     dummy_tensor = torch.zeros((3, 256, 256), dtype=torch.float32)\n",
    "                     return dummy_tensor, dummy_tensor\n",
    "            else: # Error on item 0 itself\n",
    "                dummy_tensor = torch.zeros((3, 256, 256), dtype=torch.float32)\n",
    "                return dummy_tensor, dummy_tensor\n",
    "\n",
    "# Wavelet-U-Net Model with BatchNorm\n",
    "class WaveletUNet_BN(nn.Module):\n",
    "    def __init__(self, in_channels=3, wavelet_channels=12):\n",
    "        super().__init__()\n",
    "        def conv_block(in_ch, out_ch):\n",
    "            # Kaiming init added here for convenience\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            for m in block.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            return block\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "        self.wavelet_enc1 = conv_block(wavelet_channels, 64)\n",
    "        self.wavelet_enc2 = conv_block(64, 128)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec3 = conv_block(512 + 256 + 128, 256)\n",
    "        self.dec2 = conv_block(256 + 128 + 64, 128)\n",
    "        self.dec1 = conv_block(128 + 64, 64)\n",
    "        self.final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "        # Init final layer weights\n",
    "        nn.init.kaiming_normal_(self.final.weight, mode='fan_out', nonlinearity='linear')\n",
    "        if self.final.bias is not None: nn.init.constant_(self.final.bias, 0)\n",
    "\n",
    "    def forward(self, x, wavelet):\n",
    "        e1 = self.enc1(x); p1 = self.pool(e1)\n",
    "        e2 = self.enc2(p1); p2 = self.pool(e2)\n",
    "        e3 = self.enc3(p2); p3 = self.pool(e3)\n",
    "        e4 = self.enc4(p3)\n",
    "        w_feat2 = self.wavelet_enc1(wavelet); pw_feat2 = self.pool(w_feat2)\n",
    "        w_feat3 = self.wavelet_enc2(pw_feat2)\n",
    "        up3 = self.up(e4); cat3 = torch.cat([up3, e3, w_feat3], dim=1); d3 = self.dec3(cat3)\n",
    "        up2 = self.up(d3); cat2 = torch.cat([up2, e2, w_feat2], dim=1); d2 = self.dec2(cat2)\n",
    "        up1 = self.up(d2); cat1 = torch.cat([up1, e1], dim=1); d1 = self.dec1(cat1)\n",
    "        out = self.final(d1)\n",
    "        return torch.sigmoid(out) # Output in [0, 1] range\n",
    "\n",
    "\n",
    "# Combined Loss with Differentiable DWT\n",
    "class CombinedLossDWT(nn.Module):\n",
    "    def __init__(self, alpha=0.85, wavelet='db1', device='cpu'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        # Ensure DWTForward is created only once and on the correct device\n",
    "        self.dwt = DWTForward(J=1, wave=wavelet, mode='symmetric').to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _compute_dwt(self, x):\n",
    "        # Input x shape: [B, C, H, W]\n",
    "        x = x.to(self.device) # Ensure input is on the correct device\n",
    "        LL, H_coeffs = self.dwt(x)\n",
    "        # LL shape: [B, C, H/2, W/2]\n",
    "        # H_coeffs is a list containing one tensor for J=1\n",
    "        # H_coeffs[0] shape: [B, C, 3, H/2, W/2] where 3 is for LH, HL, HH\n",
    "        details = H_coeffs[0]\n",
    "        B, C, _, H_d, W_d = details.shape\n",
    "\n",
    "        # Reshape details: [B, C, 3, H/2, W/2] -> [B, C*3, H/2, W/2]\n",
    "        details_reshaped = details.reshape(B, C * 3, H_d, W_d)\n",
    "\n",
    "        # Concatenate along channel dimension: [B, C + C*3, H/2, W/2] = [B, 4*C, H/2, W/2]\n",
    "        coeffs_combined = torch.cat([LL, details_reshaped], dim=1)\n",
    "        return coeffs_combined\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred, target = pred.float(), target.float()\n",
    "        spatial_loss = self.l1_loss(pred, target)\n",
    "\n",
    "        # Compute DWT only if alpha < 1 (frequency loss is used)\n",
    "        if self.alpha < 1.0:\n",
    "            try:\n",
    "                pred_wavelet = self._compute_dwt(pred)\n",
    "                target_wavelet = self._compute_dwt(target)\n",
    "                freq_loss = self.l1_loss(pred_wavelet, target_wavelet)\n",
    "            except Exception as e:\n",
    "                 print(f\"Error computing DWT loss: {e}. Setting freq_loss to 0.\")\n",
    "                 freq_loss = torch.tensor(0.0, device=self.device) # Avoid crashing if DWT fails\n",
    "        else:\n",
    "            freq_loss = torch.tensor(0.0, device=self.device) # No frequency loss if alpha is 1\n",
    "\n",
    "        total_loss = self.alpha * spatial_loss + (1 - self.alpha) * freq_loss\n",
    "\n",
    "        if torch.isnan(total_loss):\n",
    "            print(f\"NaN loss! Spatial: {spatial_loss.item()}, Freq: {freq_loss.item()}\")\n",
    "            # Return a large finite loss to allow scheduler/logging but indicate error\n",
    "            return torch.tensor(1000.0, device=self.device, requires_grad=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "# Helper function for non-differentiable wavelet INPUT generation\n",
    "def get_wavelet_input(img_tensor, wavelet='db1', device='cpu'):\n",
    "    # Input: img_tensor [B, C, H, W]\n",
    "    B, C, H, W = img_tensor.shape\n",
    "    img_np = img_tensor.detach().cpu().numpy().transpose(0, 2, 3, 1) # -> [B, H, W, C]\n",
    "    batch_wavelets = []\n",
    "    target_h_half, target_w_half = H // 2, W // 2\n",
    "    target_ch_out = C * 4\n",
    "\n",
    "    for i in range(img_np.shape[0]):\n",
    "        try:\n",
    "            coeffs = pywt.dwt2(img_np[i], wavelet, mode='symmetric', axes=(-3, -2))\n",
    "            cA, (cH, cV, cD) = coeffs\n",
    "            wavelet_np = np.concatenate([cA, cH, cV, cD], axis=2).astype(np.float32) # [H/2, W/2, 4*C]\n",
    "\n",
    "            # Handle potential size mismatches due to odd dimensions in DWT\n",
    "            h_np, w_np = wavelet_np.shape[:2]\n",
    "            if h_np != target_h_half or w_np != target_w_half:\n",
    "                 wavelet_np = cv2.resize(wavelet_np, (target_w_half, target_h_half), interpolation=cv2.INTER_LINEAR)\n",
    "                 if wavelet_np.ndim == 2: wavelet_np = wavelet_np[:, :, np.newaxis] # Add channel dim back if lost\n",
    "                 if wavelet_np.shape[2] != target_ch_out: # Fix channel count if necessary (unlikely but defensive)\n",
    "                     print(f\"Warning: Wavelet channel mismatch after resize ({wavelet_np.shape[2]} vs {target_ch_out}).\")\n",
    "                     if wavelet_np.shape[2] < target_ch_out:\n",
    "                         padding = np.zeros((target_h_half, target_w_half, target_ch_out - wavelet_np.shape[2]), dtype=np.float32)\n",
    "                         wavelet_np = np.concatenate([wavelet_np, padding], axis=2)\n",
    "                     else:\n",
    "                         wavelet_np = wavelet_np[:, :, :target_ch_out]\n",
    "\n",
    "            # Normalize wavelet coefficients (per channel)\n",
    "            for ch in range(wavelet_np.shape[2]):\n",
    "                channel_data = wavelet_np[:, :, ch]\n",
    "                std = channel_data.std()\n",
    "                wavelet_np[:, :, ch] = (channel_data - channel_data.mean()) / (std + 1e-8)\n",
    "\n",
    "            batch_wavelets.append(wavelet_np)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating pywt input for item {i}: {e}. Using zeros.\")\n",
    "            zero_wavelet = np.zeros((target_h_half, target_w_half, target_ch_out), dtype=np.float32)\n",
    "            batch_wavelets.append(zero_wavelet)\n",
    "\n",
    "    wavelet_batch_np = np.stack(batch_wavelets) # [B, H/2, W/2, 4*C]\n",
    "    wavelet_tensor = torch.from_numpy(wavelet_batch_np).permute(0, 3, 1, 2).float() # [B, 4*C, H/2, W/2]\n",
    "    return wavelet_tensor.to(device)\n",
    "\n",
    "# Function to Load Specific Image Pairs by Indices\n",
    "def load_image_pairs_by_indices(indices, clear_dir, blurry_dir, device='cpu'):\n",
    "    \"\"\"Loads specific clear/blurry image pairs given their indices from the full dataset.\"\"\"\n",
    "    # Create a dataset instance that loads ALL files to access any index based on original file lists\n",
    "    full_dataset = ColonoscopyDataset(clear_dir, blurry_dir, max_samples=None) # Ensure no limit for loading\n",
    "\n",
    "    loaded_blurry_tensors = []\n",
    "    loaded_clear_tensors = []\n",
    "    loaded_indices = []\n",
    "\n",
    "    for idx in indices:\n",
    "        # Check index against the total available samples found by full_dataset\n",
    "        if idx >= full_dataset.get_total_available_samples():\n",
    "            print(f\"Warning: Requested sample index {idx} is out of bounds ({full_dataset.get_total_available_samples()}). Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            # Use the __getitem__ of the full dataset instance which accesses files based on the index\n",
    "            blurry_tensor, clear_tensor = full_dataset[idx]\n",
    "            loaded_blurry_tensors.append(blurry_tensor)\n",
    "            loaded_clear_tensors.append(clear_tensor)\n",
    "            loaded_indices.append(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample pair at index {idx}: {e}. Skipping.\")\n",
    "\n",
    "    if not loaded_blurry_tensors:\n",
    "        return None, None, None, []\n",
    "\n",
    "    # Stack tensors into batches and move to device\n",
    "    blurry_batch = torch.stack(loaded_blurry_tensors).to(device)\n",
    "    clear_batch = torch.stack(loaded_clear_tensors).to(device)\n",
    "\n",
    "    # Generate wavelet input for the blurry batch\n",
    "    wavelet_input_batch = get_wavelet_input(blurry_batch, device=device)\n",
    "\n",
    "    return blurry_batch, wavelet_input_batch, clear_batch, loaded_indices\n",
    "\n",
    "# Utility function to convert tensor batch to visualizable numpy images\n",
    "def tensors_to_cv2_images(tensor_batch):\n",
    "    \"\"\" Converts a batch of [B, C, H, W] tensors (range [0,1]) to list of OpenCV images (BGR, uint8). \"\"\"\n",
    "    images = []\n",
    "    if tensor_batch is None: return images # Handle None input\n",
    "    tensor_batch = tensor_batch.detach().cpu()\n",
    "    for i in range(tensor_batch.shape[0]):\n",
    "        img_np = tensor_batch[i].numpy().transpose(1, 2, 0) # H, W, C\n",
    "        img_np = np.clip(img_np, 0, 1) # Ensure range [0, 1]\n",
    "        img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "        img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2BGR) # Convert to BGR for OpenCV saving\n",
    "        images.append(img_bgr)\n",
    "    return images\n",
    "\n",
    "# --- Training Function (MODIFIED Sampling Section) ---\n",
    "def train_model(clear_dir, blurry_dir, output_dir, epochs=50, batch_size=4, lr=0.0005,\n",
    "                wavelet='db1', alpha=0.85, weight_decay=1e-5, max_samples_train=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = WaveletUNet_BN().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = CombinedLossDWT(alpha=alpha, wavelet=wavelet, device=device)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    # Initialize dataset for training (potentially limited)\n",
    "    try:\n",
    "        train_dataset = ColonoscopyDataset(clear_dir, blurry_dir, wavelet=wavelet, max_samples=max_samples_train)\n",
    "        if len(train_dataset) == 0: print(\"ERROR: Training dataset is empty!\"); return None\n",
    "        total_available_samples = train_dataset.get_total_available_samples() # Get total count from before limit\n",
    "    except Exception as e: print(f\"Error initializing dataset: {e}\"); return None\n",
    "\n",
    "    num_w = min(4, os.cpu_count() // 2 if os.cpu_count() > 1 else 1)\n",
    "    print(f\"Using {num_w} dataloader workers.\")\n",
    "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_w,\n",
    "                            pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
    "\n",
    "    samples_dir = os.path.join(output_dir, \"samples\")\n",
    "    checkpoints_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    # --- Determine indices for sampling ---\n",
    "    num_samples_to_show = 5\n",
    "    # Use max_samples_train to define the end of the training set\n",
    "    train_set_end_idx = (max_samples_train - 1) if (max_samples_train is not None and max_samples_train > 0) else (total_available_samples - 1)\n",
    "    # Sampling range starts *after* the training set\n",
    "    sampling_start_idx = train_set_end_idx + 1\n",
    "    sampling_end_idx = total_available_samples - 1 # Last available index overall\n",
    "    num_available_for_sampling = max(0, sampling_end_idx - sampling_start_idx + 1)\n",
    "\n",
    "    fixed_sample_indices = []\n",
    "    if num_available_for_sampling >= num_samples_to_show:\n",
    "        print(f\"Selecting {num_samples_to_show} fixed random indices for sampling from range [{sampling_start_idx}, {sampling_end_idx}].\")\n",
    "        fixed_sample_indices = random.sample(range(sampling_start_idx, sampling_end_idx + 1), num_samples_to_show)\n",
    "    elif num_available_for_sampling > 0:\n",
    "        print(f\"Warning: Only {num_available_for_sampling} samples available outside training set. Using all.\")\n",
    "        fixed_sample_indices = list(range(sampling_start_idx, sampling_end_idx + 1))\n",
    "    else:\n",
    "        print(\"Warning: No samples available outside the training set for sampling. Using last available samples instead.\")\n",
    "        # Fallback: use last samples from the *entire* dataset if none outside training\n",
    "        fallback_start = max(0, total_available_samples - num_samples_to_show)\n",
    "        fixed_sample_indices = list(range(fallback_start, total_available_samples))\n",
    "\n",
    "    print(f\"Using fixed sample indices: {fixed_sample_indices}\")\n",
    "\n",
    "    # Pre-load the fixed sample pairs ONCE before training loop\n",
    "    sample_blurry_batch, sample_wavelet_batch, sample_clear_batch, loaded_indices = None, None, None, []\n",
    "    if fixed_sample_indices: # Only load if indices were determined\n",
    "        try:\n",
    "            sample_blurry_batch, sample_wavelet_batch, sample_clear_batch, loaded_indices = \\\n",
    "                load_image_pairs_by_indices(fixed_sample_indices, clear_dir, blurry_dir, device=device)\n",
    "            if sample_blurry_batch is None:\n",
    "                 print(\"ERROR: Failed to load any sample images. Disabling sampling.\")\n",
    "                 fixed_sample_indices = [] # Disable sampling\n",
    "            elif len(loaded_indices) != len(fixed_sample_indices):\n",
    "                 print(f\"Warning: Loaded {len(loaded_indices)} sample images, but requested {len(fixed_sample_indices)}. Using loaded ones.\")\n",
    "                 fixed_sample_indices = loaded_indices # Update indices to reflect reality\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR loading initial sample images: {e}. Disabling sampling.\")\n",
    "            fixed_sample_indices = []\n",
    "    # --- End sampling setup ---\n",
    "\n",
    "\n",
    "    # Save initial prediction using one of the sample images if available\n",
    "    if fixed_sample_indices and sample_blurry_batch is not None:\n",
    "        try:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                initial_pred = model(sample_blurry_batch[0:1], sample_wavelet_batch[0:1]) # Predict first sample\n",
    "            initial_pred_img = tensors_to_cv2_images(initial_pred)[0] # Convert first prediction\n",
    "            cv2.imwrite(os.path.join(samples_dir, \"initial_pred_test.png\"), initial_pred_img)\n",
    "            print(\"Saved initial prediction sample.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating initial prediction: {e}\")\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Total available pairs: {total_available_samples}\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            # --- Training Step ---\n",
    "            if batch is None: continue\n",
    "            try: blurry_img, clear_img = batch\n",
    "            except Exception as e: print(f\"Error unpacking batch {i}: {e}\"); continue\n",
    "\n",
    "            blurry_img = blurry_img.to(device); clear_img = clear_img.to(device)\n",
    "            if not isinstance(blurry_img, torch.Tensor) or not isinstance(clear_img, torch.Tensor): continue\n",
    "            if blurry_img.ndim != 4 or clear_img.ndim != 4: continue\n",
    "\n",
    "            try: wavelet_input = get_wavelet_input(blurry_img, wavelet=wavelet, device=device)\n",
    "            except Exception as e: print(f\"Error generating wavelet input batch {i}: {e}\"); continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            try: pred = model(blurry_img, wavelet_input)\n",
    "            except Exception as e: print(f\"\\nForward pass error batch {i}: {e}\"); traceback.print_exc(); continue\n",
    "\n",
    "            try: loss = criterion(pred, clear_img)\n",
    "            except Exception as e: print(f\"\\nLoss calculation error batch {i}: {e}\"); traceback.print_exc(); continue\n",
    "\n",
    "            if torch.isnan(loss): print(f\"NaN loss detected at batch {i}, skipping backward/step.\"); optimizer.zero_grad(); continue\n",
    "\n",
    "            try: loss.backward()\n",
    "            except Exception as e: print(f\"\\nBackward pass error batch {i}: {e}\"); traceback.print_exc(); continue\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Optional\n",
    "\n",
    "            try: optimizer.step()\n",
    "            except Exception as e: print(f\"\\nOptimizer step error batch {i}: {e}\"); traceback.print_exc(); continue\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            running_loss += current_loss\n",
    "            progress_bar.set_postfix(loss=f\"{current_loss:.6f}\", avg_loss=f\"{running_loss/(i+1):.6f}\", lr=f\"{optimizer.param_groups[0]['lr']:.1e}\")\n",
    "        # --- End Batch Loop ---\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} finished. Average Training Loss: {avg_loss:.6f}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "\n",
    "        # --- Generate and Save Sample Montage EVERY Epoch ---\n",
    "        # Check if sampling is enabled and data was successfully loaded\n",
    "        if fixed_sample_indices and sample_blurry_batch is not None and sample_clear_batch is not None:\n",
    "            try:\n",
    "                model.eval() # Set model to evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    pred_sample_batch = model(sample_blurry_batch, sample_wavelet_batch)\n",
    "\n",
    "                # Convert tensors to OpenCV images (BGR, uint8)\n",
    "                clear_cv2 = tensors_to_cv2_images(sample_clear_batch) # Row 1: Ground Truth Clear\n",
    "                pred_cv2 = tensors_to_cv2_images(pred_sample_batch)   # Row 2: Model Output\n",
    "                blurry_cv2 = tensors_to_cv2_images(sample_blurry_batch) # Row 3: Original Blurry Input\n",
    "\n",
    "                # Ensure all images for the montage have the same size\n",
    "                # Use size of first predicted image as target (or clear image, should be same if dataset is consistent)\n",
    "                if pred_cv2: # Check if prediction list is not empty\n",
    "                     target_h, target_w = pred_cv2[0].shape[:2]\n",
    "                elif clear_cv2:\n",
    "                     target_h, target_w = clear_cv2[0].shape[:2]\n",
    "                else: # Cannot determine target size\n",
    "                     print(f\"Warning: Cannot determine target size for montage epoch {epoch+1}. Skipping.\")\n",
    "                     continue # Skip montage generation for this epoch\n",
    "\n",
    "                # Resize function with check\n",
    "                def resize_img_list(img_list, target_w, target_h):\n",
    "                     return [cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_LINEAR) if img.shape[:2] != (target_h, target_w) else img for img in img_list]\n",
    "\n",
    "                resized_clear = resize_img_list(clear_cv2, target_w, target_h)\n",
    "                resized_pred = resize_img_list(pred_cv2, target_w, target_h)\n",
    "                resized_blurry = resize_img_list(blurry_cv2, target_w, target_h)\n",
    "\n",
    "                # Create the montage if all lists have content\n",
    "                if resized_clear and resized_pred and resized_blurry:\n",
    "                    row1 = cv2.hconcat(resized_clear)  # Top: Ground Truth Clear\n",
    "                    row2 = cv2.hconcat(resized_pred)   # Middle: Model Output\n",
    "                    row3 = cv2.hconcat(resized_blurry) # Bottom: Original Blurry Input\n",
    "\n",
    "                    # Add padding between rows\n",
    "                    padding_height = 10\n",
    "                    padding = np.zeros((padding_height, row1.shape[1], 3), dtype=np.uint8) # Black padding\n",
    "\n",
    "                    # Stack rows vertically: Clear, Padding, Prediction, Padding, Blurry\n",
    "                    montage = cv2.vconcat([row1, padding.copy(), row2, padding.copy(), row3])\n",
    "\n",
    "                    sample_path = os.path.join(samples_dir, f\"sample_montage_epoch_{epoch+1:03d}.png\")\n",
    "                    cv2.imwrite(sample_path, montage)\n",
    "                    # print(f\"Saved sample montage to {sample_path}\") # Reduce frequency if desired\n",
    "                else:\n",
    "                     print(f\"Warning: Could not generate montage for epoch {epoch+1}, one or more image lists were empty after processing.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating sample montage for epoch {epoch+1}: {e}\")\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                 model.train() # Ensure model is back in training mode\n",
    "        # --- End Sample Montage Generation ---\n",
    "\n",
    "        # Save model checkpoint periodically\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            checkpoint_path = os.path.join(checkpoints_dir, f\"wavelet_unet_bn_epoch_{epoch+1:03d}.pth\")\n",
    "            try:\n",
    "                 torch.save({\n",
    "                    'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'loss': avg_loss,\n",
    "                 }, checkpoint_path)\n",
    "                 print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "            except Exception as e: print(f\"Error saving checkpoint: {e}\")\n",
    "    # --- End Epoch Loop ---\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    final_model_path = os.path.join(output_dir, \"wavelet_unet_bn_final.pth\")\n",
    "    try:\n",
    "         torch.save(model.state_dict(), final_model_path)\n",
    "         print(f\"Saved final model to {final_model_path}\")\n",
    "    except Exception as e: print(f\"Error saving final model: {e}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clear_dir = \"output_3_18/smokeV2/clear\"\n",
    "    blurry_dir = \"output_3_18/smokeV2/blurry\"\n",
    "    # Update output directory name for the 3-row montage version\n",
    "    output_dir = \"output_3_18/smoke/output_V2\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Clear image directory: {os.path.abspath(clear_dir)}\")\n",
    "    print(f\"Blurry image directory: {os.path.abspath(blurry_dir)}\")\n",
    "    print(f\"Output directory: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "    if not os.path.isdir(clear_dir) or not os.path.isdir(blurry_dir):\n",
    "        print(f\"Error: Directory not found - Check paths!\")\n",
    "    else:\n",
    "        try:\n",
    "            # Using 5000 samples and batch size 8 as per last user code\n",
    "            model = train_model(\n",
    "                clear_dir, blurry_dir, output_dir,\n",
    "                epochs=50,\n",
    "                batch_size=8,       # As used in the previous code\n",
    "                lr=0.0005,\n",
    "                wavelet='db1',\n",
    "                alpha=0.85,\n",
    "                weight_decay=1e-5,\n",
    "                max_samples_train=5000 # As used in the previous code\n",
    "            )\n",
    "            if model:\n",
    "                print(\"Training process completed.\")\n",
    "            else:\n",
    "                print(\"Training process failed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during script execution: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4d33b-43d7-4682-b8df-6460de1ac048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
