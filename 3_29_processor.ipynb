{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8579822-773a-44e6-a649-552d2c398384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.16.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.8.0.76)\n",
      "Requirement already satisfied: pywavelets in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
      "Collecting pytorch_wavelets\n",
      "  Downloading pytorch_wavelets-1.3.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pytorch_wavelets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading pytorch_wavelets-1.3.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch_wavelets\n",
      "Successfully installed pytorch_wavelets-1.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision numpy opencv-python pywavelets pytorch_wavelets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b244e6e5-903b-4d1e-903c-0dd5bc20649f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Direct loading failed: Error(s) in loading state_dict for WaveletUNet_BN:\n",
      "\tMissing key(s) in state_dict: \"enc1.0.weight\", \"enc1.1.weight\", \"enc1.1.bias\", \"enc1.1.running_mean\", \"enc1.1.running_var\", \"enc1.3.weight\", \"enc1.4.weight\", \"enc1.4.bias\", \"enc1.4.running_mean\", \"enc1.4.running_var\", \"enc2.0.weight\", \"enc2.1.weight\", \"enc2.1.bias\", \"enc2.1.running_mean\", \"enc2.1.running_var\", \"enc2.3.weight\", \"enc2.4.weight\", \"enc2.4.bias\", \"enc2.4.running_mean\", \"enc2.4.running_var\", \"enc3.0.weight\", \"enc3.1.weight\", \"enc3.1.bias\", \"enc3.1.running_mean\", \"enc3.1.running_var\", \"enc3.3.weight\", \"enc3.4.weight\", \"enc3.4.bias\", \"enc3.4.running_mean\", \"enc3.4.running_var\", \"enc4.0.weight\", \"enc4.1.weight\", \"enc4.1.bias\", \"enc4.1.running_mean\", \"enc4.1.running_var\", \"enc4.3.weight\", \"enc4.4.weight\", \"enc4.4.bias\", \"enc4.4.running_mean\", \"enc4.4.running_var\", \"wavelet_enc1.0.weight\", \"wavelet_enc1.1.weight\", \"wavelet_enc1.1.bias\", \"wavelet_enc1.1.running_mean\", \"wavelet_enc1.1.running_var\", \"wavelet_enc1.3.weight\", \"wavelet_enc1.4.weight\", \"wavelet_enc1.4.bias\", \"wavelet_enc1.4.running_mean\", \"wavelet_enc1.4.running_var\", \"wavelet_enc2.0.weight\", \"wavelet_enc2.1.weight\", \"wavelet_enc2.1.bias\", \"wavelet_enc2.1.running_mean\", \"wavelet_enc2.1.running_var\", \"wavelet_enc2.3.weight\", \"wavelet_enc2.4.weight\", \"wavelet_enc2.4.bias\", \"wavelet_enc2.4.running_mean\", \"wavelet_enc2.4.running_var\", \"dec3.0.weight\", \"dec3.1.weight\", \"dec3.1.bias\", \"dec3.1.running_mean\", \"dec3.1.running_var\", \"dec3.3.weight\", \"dec3.4.weight\", \"dec3.4.bias\", \"dec3.4.running_mean\", \"dec3.4.running_var\", \"dec2.0.weight\", \"dec2.1.weight\", \"dec2.1.bias\", \"dec2.1.running_mean\", \"dec2.1.running_var\", \"dec2.3.weight\", \"dec2.4.weight\", \"dec2.4.bias\", \"dec2.4.running_mean\", \"dec2.4.running_var\", \"dec1.0.weight\", \"dec1.1.weight\", \"dec1.1.bias\", \"dec1.1.running_mean\", \"dec1.1.running_var\", \"dec1.3.weight\", \"dec1.4.weight\", \"dec1.4.bias\", \"dec1.4.running_mean\", \"dec1.4.running_var\", \"final.weight\", \"final.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"epoch\", \"model_state_dict\", \"optimizer_state_dict\", \"scheduler_state_dict\", \"train_loss\", \"val_loss\", \"best_val_loss\". . Trying checkpoint format...\n",
      "Loaded checkpoint from wavelet_unet_bn_best.pth\n",
      "Model loaded successfully\n",
      "Found 15397 image files in output_3_18/classified_clear\n",
      "Selected 154 images to process (every 100th image)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 154/154 [00:14<00:00, 10.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved montage to deblurred_output/montage.png\n",
      "Processing complete!\n",
      "Processed 154 images\n",
      "Results saved to deblurred_output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pywt\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "from pytorch_wavelets import DWTForward\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Change these parameters according to your needs\n",
    "INPUT_DIR = \"output_3_18/classified_clear\"  # Directory containing blurry images\n",
    "OUTPUT_DIR = \"deblurred_output\"  # Directory to save processed images\n",
    "MODEL_PATH = \"wavelet_unet_bn_best.pth\"  # Path to model weights\n",
    "PROCESS_EVERY_N = 100  # Process every Nth image\n",
    "MONTAGE_COLS = 1  # Number of image pairs per row in montage\n",
    "TARGET_HEIGHT = None  # Target height for processing (optional)\n",
    "TARGET_WIDTH = None  # Target width for processing (optional)\n",
    "BATCH_SIZE = 1  # Batch size for processing\n",
    "WAVELET = 'db1'  # Wavelet type for preprocessing\n",
    "# ==================== END CONFIGURATION ====================\n",
    "\n",
    "# Wavelet-U-Net Model with BatchNorm\n",
    "class WaveletUNet_BN(nn.Module):\n",
    "    def __init__(self, in_channels=3, wavelet_channels=12):\n",
    "        super().__init__()\n",
    "        def conv_block(in_ch, out_ch):\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            for m in block.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            return block\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "        self.wavelet_enc1 = conv_block(wavelet_channels, 64)\n",
    "        self.wavelet_enc2 = conv_block(64, 128)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec3 = conv_block(512 + 256 + 128, 256)\n",
    "        self.dec2 = conv_block(256 + 128 + 64, 128)\n",
    "        self.dec1 = conv_block(128 + 64, 64)\n",
    "        self.final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.final.weight, mode='fan_out', nonlinearity='linear')\n",
    "        if self.final.bias is not None: \n",
    "            nn.init.constant_(self.final.bias, 0)\n",
    "\n",
    "    def forward(self, x, wavelet):\n",
    "        e1 = self.enc1(x); p1 = self.pool(e1)\n",
    "        e2 = self.enc2(p1); p2 = self.pool(e2)\n",
    "        e3 = self.enc3(p2); p3 = self.pool(e3)\n",
    "        e4 = self.enc4(p3)\n",
    "        w_feat2 = self.wavelet_enc1(wavelet); pw_feat2 = self.pool(w_feat2)\n",
    "        w_feat3 = self.wavelet_enc2(pw_feat2)\n",
    "        up3 = self.up(e4); cat3 = torch.cat([up3, e3, w_feat3], dim=1); d3 = self.dec3(cat3)\n",
    "        up2 = self.up(d3); cat2 = torch.cat([up2, e2, w_feat2], dim=1); d2 = self.dec2(cat2)\n",
    "        up1 = self.up(d2); cat1 = torch.cat([up1, e1], dim=1); d1 = self.dec1(cat1)\n",
    "        out = self.final(d1)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Helper function for wavelet input generation\n",
    "def get_wavelet_input(img_tensor, wavelet='db1', device='cpu'):\n",
    "    # Input: img_tensor [B, C, H, W]\n",
    "    B, C, H, W = img_tensor.shape\n",
    "    img_np = img_tensor.detach().cpu().numpy().transpose(0, 2, 3, 1)  # -> [B, H, W, C]\n",
    "    batch_wavelets = []\n",
    "    target_h_half, target_w_half = H // 2, W // 2\n",
    "    target_ch_out = C * 4\n",
    "\n",
    "    for i in range(img_np.shape[0]):\n",
    "        try:\n",
    "            coeffs = pywt.dwt2(img_np[i], wavelet, mode='symmetric', axes=(-3, -2))\n",
    "            cA, (cH, cV, cD) = coeffs\n",
    "            wavelet_np = np.concatenate([cA, cH, cV, cD], axis=2).astype(np.float32)  # [H/2, W/2, 4*C]\n",
    "\n",
    "            # Handle potential size mismatches due to odd dimensions in DWT\n",
    "            h_np, w_np = wavelet_np.shape[:2]\n",
    "            if h_np != target_h_half or w_np != target_w_half:\n",
    "                wavelet_np = cv2.resize(wavelet_np, (target_w_half, target_h_half), interpolation=cv2.INTER_LINEAR)\n",
    "                if wavelet_np.ndim == 2: \n",
    "                    wavelet_np = wavelet_np[:, :, np.newaxis]  # Add channel dim back if lost\n",
    "                if wavelet_np.shape[2] != target_ch_out:  # Fix channel count if necessary\n",
    "                    print(f\"Warning: Wavelet channel mismatch after resize ({wavelet_np.shape[2]} vs {target_ch_out}).\")\n",
    "                    if wavelet_np.shape[2] < target_ch_out:\n",
    "                        padding = np.zeros((target_h_half, target_w_half, target_ch_out - wavelet_np.shape[2]), dtype=np.float32)\n",
    "                        wavelet_np = np.concatenate([wavelet_np, padding], axis=2)\n",
    "                    else:\n",
    "                        wavelet_np = wavelet_np[:, :, :target_ch_out]\n",
    "\n",
    "            # Normalize wavelet coefficients (per channel)\n",
    "            for ch in range(wavelet_np.shape[2]):\n",
    "                channel_data = wavelet_np[:, :, ch]\n",
    "                std = channel_data.std()\n",
    "                wavelet_np[:, :, ch] = (channel_data - channel_data.mean()) / (std + 1e-8)\n",
    "\n",
    "            batch_wavelets.append(wavelet_np)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating pywt input for item {i}: {e}. Using zeros.\")\n",
    "            zero_wavelet = np.zeros((target_h_half, target_w_half, target_ch_out), dtype=np.float32)\n",
    "            batch_wavelets.append(zero_wavelet)\n",
    "\n",
    "    wavelet_batch_np = np.stack(batch_wavelets)  # [B, H/2, W/2, 4*C]\n",
    "    wavelet_tensor = torch.from_numpy(wavelet_batch_np).permute(0, 3, 1, 2).float()  # [B, 4*C, H/2, W/2]\n",
    "    return wavelet_tensor.to(device)\n",
    "\n",
    "# Utility function to load and preprocess an image for the model\n",
    "def load_and_preprocess_image(img_path, target_size=None):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        base, ext = os.path.splitext(img_path)\n",
    "        if not ext:\n",
    "            for try_ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "                img = cv2.imread(img_path + try_ext)\n",
    "                if img is not None:\n",
    "                    break\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    elif img.shape[2] == 4:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    elif img.shape[2] != 3:\n",
    "        raise ValueError(f\"Image {img_path} has unexpected shape {img.shape}\")\n",
    "\n",
    "    # Store original size for later\n",
    "    original_h, original_w = img.shape[:2]\n",
    "    \n",
    "    # Resize if requested\n",
    "    if target_size and (original_h != target_size[0] or original_w != target_size[1]):\n",
    "        img = cv2.resize(img, (target_size[1], target_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Convert to RGB and normalize to [0,1]\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert to tensor [C,H,W]\n",
    "    img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float()\n",
    "    \n",
    "    return img_tensor, original_h, original_w\n",
    "\n",
    "# Function to process a single image\n",
    "def process_single_image(model, image_path, device, target_size=None, wavelet=WAVELET):\n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        img_tensor, orig_h, orig_w = load_and_preprocess_image(image_path, target_size)\n",
    "        \n",
    "        # Add batch dimension and send to device\n",
    "        img_batch = img_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate wavelet input\n",
    "        wavelet_batch = get_wavelet_input(img_batch, wavelet=wavelet, device=device)\n",
    "        \n",
    "        # Process with model\n",
    "        with torch.no_grad():\n",
    "            output_batch = model(img_batch, wavelet_batch)\n",
    "        \n",
    "        # Convert output to numpy image\n",
    "        output_tensor = output_batch[0].cpu()\n",
    "        output_np = output_tensor.permute(1, 2, 0).numpy()\n",
    "        output_np = np.clip(output_np, 0, 1)\n",
    "        \n",
    "        # Convert to BGR for OpenCV and rescale to uint8\n",
    "        output_bgr = cv2.cvtColor((output_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Resize back to original dimensions if needed\n",
    "        if target_size and (orig_h != target_size[0] or orig_w != target_size[1]):\n",
    "            output_bgr = cv2.resize(output_bgr, (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        return output_bgr\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Function to create comparison image\n",
    "def create_comparison(input_path, output_image):\n",
    "    # Read the original input image\n",
    "    input_image = cv2.imread(input_path)\n",
    "    if input_image is None:\n",
    "        print(f\"Error reading input image: {input_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure both images have the same dimensions\n",
    "    if input_image.shape != output_image.shape:\n",
    "        output_image = cv2.resize(output_image, (input_image.shape[1], input_image.shape[0]))\n",
    "    \n",
    "    # Create a side-by-side comparison image with labels\n",
    "    h, w = input_image.shape[:2]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.8\n",
    "    font_thickness = 2\n",
    "    \n",
    "    # Add labels to the images\n",
    "    cv2.putText(input_image, \"Before\", (10, 30), font, font_scale, (0, 0, 255), font_thickness)\n",
    "    cv2.putText(output_image, \"After\", (10, 30), font, font_scale, (0, 255, 0), font_thickness)\n",
    "    \n",
    "    # Create the comparison image\n",
    "    comparison = np.hstack((input_image, output_image))\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Function to create a montage of multiple comparisons\n",
    "def create_montage(comparisons, cols=MONTAGE_COLS):\n",
    "    if not comparisons:\n",
    "        return None\n",
    "    \n",
    "    # Get the dimensions of the first comparison\n",
    "    h, w = comparisons[0].shape[:2]\n",
    "    \n",
    "    # Calculate the number of rows needed\n",
    "    rows = (len(comparisons) + cols - 1) // cols\n",
    "    \n",
    "    # Create an empty montage\n",
    "    montage = np.zeros((rows * h, cols * w, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Fill the montage with comparisons\n",
    "    for i, comp in enumerate(comparisons):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        # Handle the case where the last row isn't full\n",
    "        if i >= len(comparisons):\n",
    "            break\n",
    "            \n",
    "        y_start = row * h\n",
    "        y_end = y_start + h\n",
    "        x_start = col * w\n",
    "        x_end = x_start + w\n",
    "        \n",
    "        # Ensure the dimensions match (resize if needed)\n",
    "        if comp.shape[:2] != (h, w):\n",
    "            comp = cv2.resize(comp, (w, h))\n",
    "            \n",
    "        # Place the comparison in the montage\n",
    "        montage[y_start:y_end, x_start:x_end] = comp\n",
    "    \n",
    "    return montage\n",
    "\n",
    "# Main processing function\n",
    "def process_images():\n",
    "    # Calculate target size\n",
    "    target_size = None\n",
    "    if TARGET_HEIGHT and TARGET_WIDTH:\n",
    "        target_size = (TARGET_HEIGHT, TARGET_WIDTH)\n",
    "        print(f\"Using target size: {target_size}\")\n",
    "    \n",
    "    # Check if input directory exists\n",
    "    if not os.path.isdir(INPUT_DIR):\n",
    "        print(f\"Error: Input directory {INPUT_DIR} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory structure if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"individual\"), exist_ok=True)\n",
    "    \n",
    "    # Set up device - use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model and load weights\n",
    "    try:\n",
    "        model = WaveletUNet_BN().to(device)\n",
    "        \n",
    "        # Check if model path has a specific format suffix\n",
    "        if MODEL_PATH.endswith('.pth'):\n",
    "            try:\n",
    "                # Try loading as a state dict directly\n",
    "                model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "                print(f\"Loaded state dict from {MODEL_PATH}\")\n",
    "            except Exception as e:\n",
    "                # If that fails, try loading as a checkpoint dictionary\n",
    "                print(f\"Direct loading failed: {e}. Trying checkpoint format...\")\n",
    "                checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    print(f\"Loaded checkpoint from {MODEL_PATH}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not find model state dict in checkpoint\")\n",
    "        else:\n",
    "            raise ValueError(f\"Model path should end with .pth\")\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        print(\"Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    # Get all image files from input directory\n",
    "    image_files = []\n",
    "    for ext in ['.png', '.jpg', '.jpeg', '.bmp']:\n",
    "        image_files.extend(sorted([os.path.join(INPUT_DIR, f) for f in os.listdir(INPUT_DIR) \n",
    "                            if f.lower().endswith(ext)]))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No image files found in {INPUT_DIR}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files in {INPUT_DIR}\")\n",
    "    \n",
    "    # Select every Nth image\n",
    "    selected_images = image_files[::PROCESS_EVERY_N]\n",
    "    print(f\"Selected {len(selected_images)} images to process (every {PROCESS_EVERY_N}th image)\")\n",
    "    \n",
    "    # Process images\n",
    "    comparisons = []\n",
    "    for i, img_path in enumerate(tqdm(selected_images, desc=\"Processing images\")):\n",
    "        try:\n",
    "            # Process the image\n",
    "            output_image = process_single_image(model, img_path, device, target_size, WAVELET)\n",
    "            \n",
    "            if output_image is None:\n",
    "                print(f\"Failed to process {img_path}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Save individual processed image\n",
    "            img_name = os.path.basename(img_path)\n",
    "            output_path = os.path.join(OUTPUT_DIR, \"individual\", f\"processed_{img_name}\")\n",
    "            cv2.imwrite(output_path, output_image)\n",
    "            \n",
    "            # Create and save comparison\n",
    "            comparison = create_comparison(img_path, output_image)\n",
    "            if comparison is not None:\n",
    "                comparison_path = os.path.join(OUTPUT_DIR, f\"comparison_{img_name}\")\n",
    "                cv2.imwrite(comparison_path, comparison)\n",
    "                comparisons.append(comparison)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    # Create and save montage\n",
    "    if comparisons:\n",
    "        montage = create_montage(comparisons, cols=MONTAGE_COLS)\n",
    "        if montage is not None:\n",
    "            montage_path = os.path.join(OUTPUT_DIR, \"montage.png\")\n",
    "            cv2.imwrite(montage_path, montage)\n",
    "            print(f\"Saved montage to {montage_path}\")\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Processed {len(comparisons)} images\")\n",
    "    print(f\"Results saved to {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57473c09-a59f-4118-8a26-b77af5ba1269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
