{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca555e94-fb30-463a-9d4b-9d345383a753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
      "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.66.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.11.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2023.12.9)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python numpy matplotlib PyWavelets scikit-image tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690a412b-c66b-4385-975d-edc47ed8ea0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Using GPU: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61602243c5fe4b6e84a0bb527a6f514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 549\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (img1_orig, img1_highlighted, img1_mask), (img2_orig, img2_highlighted, img2_mask)\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# 1. Process sample images\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# process_sample_images('image1.jpg', 'image2.jpg', threshold=0.15)\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# Generate a complete dataset from your video\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_reflection_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvideo.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.08\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Adjust sensitivity\u001b[39;49;00m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhighlight_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Green highlighting\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Process every 5th frame\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Limit number of frames (optional)\u001b[39;49;00m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# Save debug visualizations\u001b[39;49;00m\n\u001b[1;32m    556\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 460\u001b[0m, in \u001b[0;36mgenerate_reflection_dataset\u001b[0;34m(input_video_path, threshold, wavelet, level, highlight_color, sample_rate, max_frames, use_gpu, debug, create_split)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;66;03m# Process the video\u001b[39;00m\n\u001b[1;32m    459\u001b[0m video_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(input_video_path))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 460\u001b[0m num_frames \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_frames\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples from video\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Create train/val split if requested\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 172\u001b[0m, in \u001b[0;36mSpecularReflectionDatasetGenerator.process_video\u001b[0;34m(self, input_path, output_prefix, sample_rate, max_frames)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Save the pair of images\u001b[39;00m\n\u001b[1;32m    170\u001b[0m frame_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msaved_frames\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 172\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mframe_filename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhighlighted_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m), highlighted_frame)\n\u001b[1;32m    174\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m), specular_mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt  # This imports from the PyWavelets package\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "class SpecularReflectionDatasetGenerator:\n",
    "    def __init__(self, \n",
    "                 wavelet='db4',         # Wavelet type\n",
    "                 threshold=0.15,        # Threshold for specular reflection detection (increased for better isolation)\n",
    "                 level=3,               # Wavelet decomposition level\n",
    "                 use_gpu=True,          # Whether to use GPU for processing\n",
    "                 highlight_color=(0, 255, 0),  # Green color for highlighting\n",
    "                 debug=True):           # Whether to save debug images\n",
    "        \n",
    "        self.wavelet = wavelet\n",
    "        self.threshold = threshold\n",
    "        self.level = level\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.highlight_color = highlight_color\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Create directories for dataset\n",
    "        self.output_dir = \"reflection_dataset\"\n",
    "        self.original_dir = os.path.join(self.output_dir, \"original\")\n",
    "        self.highlighted_dir = os.path.join(self.output_dir, \"highlighted\")\n",
    "        self.mask_dir = os.path.join(self.output_dir, \"masks\")\n",
    "        self.debug_dir = os.path.join(self.output_dir, \"debug\")\n",
    "        \n",
    "        os.makedirs(self.original_dir, exist_ok=True)\n",
    "        os.makedirs(self.highlighted_dir, exist_ok=True)\n",
    "        os.makedirs(self.mask_dir, exist_ok=True)\n",
    "        os.makedirs(self.debug_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "        print(f\"Using GPU: {self.use_gpu}\")\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            # Set up GPU device\n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "    \n",
    "    def _calculate_wavelet_features(self, frame):\n",
    "        \"\"\"Calculate wavelet decomposition and extract features from the frame.\"\"\"\n",
    "        # Convert to grayscale for wavelet analysis\n",
    "        if len(frame.shape) == 3:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = frame.copy()\n",
    "            \n",
    "        # Apply wavelet transform\n",
    "        coeffs = pywt.wavedec2(gray, self.wavelet, level=self.level)\n",
    "        \n",
    "        # Extract high-frequency components (details)\n",
    "        details = []\n",
    "        for detail_coeffs in coeffs[1:]:\n",
    "            details.extend([np.abs(detail_coeffs[i]) for i in range(3)])\n",
    "        \n",
    "        # Normalize and stack details\n",
    "        detail_features = np.stack([cv2.resize(d, (gray.shape[1], gray.shape[0])) for d in details])\n",
    "        detail_features = np.max(detail_features, axis=0)\n",
    "        \n",
    "        return detail_features\n",
    "    \n",
    "    def _detect_specular_regions(self, frame):\n",
    "        \"\"\"Detect specular reflection regions in the frame using wavelet features and HSV color space.\"\"\"\n",
    "        # Calculate wavelet features\n",
    "        features = self._calculate_wavelet_features(frame)\n",
    "        \n",
    "        # Apply additional preprocessing for specular reflection detection\n",
    "        # Convert to HSV for better highlight detection\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        \n",
    "        # High Value and low Saturation indicates specular reflection\n",
    "        # Adjusted thresholds based on the images provided\n",
    "        high_v = v > 200\n",
    "        low_s = s < 40\n",
    "        \n",
    "        # Combine with wavelet features\n",
    "        normalized_features = (features - np.min(features)) / (np.max(features) - np.min(features) + 1e-6)\n",
    "        wavelet_mask = normalized_features > self.threshold\n",
    "        \n",
    "        # Combine the masks with more weight on color-based detection for these types of reflections\n",
    "        combined_mask = np.logical_or(np.logical_and(high_v, low_s), wavelet_mask)\n",
    "        \n",
    "        # Clean up the mask\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.morphologyEx(combined_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Further refine the mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        refined_mask = np.zeros_like(mask)\n",
    "        \n",
    "        # Only keep contours of meaningful size\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 10:  # Minimum area threshold\n",
    "                cv2.drawContours(refined_mask, [contour], -1, 1, -1)\n",
    "        \n",
    "        return refined_mask\n",
    "    \n",
    "    def _create_highlighted_image(self, frame, mask):\n",
    "        \"\"\"Create a version of the frame with specular reflections highlighted in green.\"\"\"\n",
    "        highlighted = frame.copy()\n",
    "        \n",
    "        # Convert mask to boolean for indexing\n",
    "        mask_bool = mask.astype(bool)\n",
    "        \n",
    "        # Apply highlight color to the detected regions\n",
    "        highlighted[mask_bool] = self.highlight_color\n",
    "        \n",
    "        return highlighted\n",
    "    \n",
    "    def process_video(self, input_path, output_prefix, sample_rate=1, max_frames=None):\n",
    "        \"\"\"\n",
    "        Process the video to generate dataset of original and highlighted frames.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to the input video file\n",
    "            output_prefix: Prefix for output filenames\n",
    "            sample_rate: Save a frame every N frames\n",
    "            max_frames: Maximum number of frames to process (None for all)\n",
    "        \n",
    "        Returns:\n",
    "            Number of frames processed\n",
    "        \"\"\"\n",
    "        # Open the video file\n",
    "        video = cv2.VideoCapture(input_path)\n",
    "        if not video.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {input_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Limit frames if specified\n",
    "        if max_frames is not None:\n",
    "            frame_count = min(frame_count, max_frames)\n",
    "        \n",
    "        # Process each frame\n",
    "        processed_frames = 0\n",
    "        saved_frames = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i in tqdm(range(frame_count)):\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Only process every Nth frame\n",
    "            if i % sample_rate != 0:\n",
    "                continue\n",
    "                \n",
    "            # Detect specular reflections\n",
    "            specular_mask = self._detect_specular_regions(frame)\n",
    "            \n",
    "            # Create highlighted version\n",
    "            highlighted_frame = self._create_highlighted_image(frame, specular_mask)\n",
    "            \n",
    "            # Save the pair of images\n",
    "            frame_filename = f\"{output_prefix}_{saved_frames:04d}\"\n",
    "            \n",
    "            cv2.imwrite(os.path.join(self.original_dir, f\"{frame_filename}.jpg\"), frame)\n",
    "            cv2.imwrite(os.path.join(self.highlighted_dir, f\"{frame_filename}.jpg\"), highlighted_frame)\n",
    "            cv2.imwrite(os.path.join(self.mask_dir, f\"{frame_filename}.png\"), specular_mask * 255)\n",
    "            \n",
    "            if self.debug and saved_frames % 10 == 0:\n",
    "                # Create visualization of the detection\n",
    "                wavelet_features = self._calculate_wavelet_features(frame)\n",
    "                normalized_features = (wavelet_features - np.min(wavelet_features)) / (np.max(wavelet_features) - np.min(wavelet_features) + 1e-6)\n",
    "                wavelet_vis = cv2.applyColorMap((normalized_features * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "                \n",
    "                # Create side-by-side comparison for debugging\n",
    "                comparison = np.hstack((frame, highlighted_frame))\n",
    "                cv2.imwrite(os.path.join(self.debug_dir, f\"{frame_filename}_comparison.jpg\"), comparison)\n",
    "                cv2.imwrite(os.path.join(self.debug_dir, f\"{frame_filename}_wavelet.jpg\"), wavelet_vis)\n",
    "            \n",
    "            processed_frames += 1\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Calculate processing stats\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Clean up\n",
    "        video.release()\n",
    "        \n",
    "        print(f\"Processed {processed_frames} frames and saved {saved_frames} frame pairs in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        return saved_frames\n",
    "    \n",
    "    def process_image_directory(self, input_dir, output_prefix, file_pattern='*.jpg'):\n",
    "        \"\"\"\n",
    "        Process all images in a directory to generate dataset of original and highlighted images.\n",
    "        \n",
    "        Args:\n",
    "            input_dir: Directory containing input images\n",
    "            output_prefix: Prefix for output filenames\n",
    "            file_pattern: Pattern to match image files\n",
    "        \n",
    "        Returns:\n",
    "            Number of images processed\n",
    "        \"\"\"\n",
    "        # Get list of image files\n",
    "        image_files = glob.glob(os.path.join(input_dir, file_pattern))\n",
    "        \n",
    "        # Process each image\n",
    "        processed_images = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, image_path in enumerate(tqdm(image_files)):\n",
    "            # Read the image\n",
    "            frame = cv2.imread(image_path)\n",
    "            if frame is None:\n",
    "                print(f\"Could not read image: {image_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Detect specular reflections\n",
    "            specular_mask = self._detect_specular_regions(frame)\n",
    "            \n",
    "            # Create highlighted version\n",
    "            highlighted_frame = self._create_highlighted_image(frame, specular_mask)\n",
    "            \n",
    "            # Save the pair of images\n",
    "            frame_filename = f\"{output_prefix}_{i:04d}\"\n",
    "            \n",
    "            cv2.imwrite(os.path.join(self.original_dir, f\"{frame_filename}.jpg\"), frame)\n",
    "            cv2.imwrite(os.path.join(self.highlighted_dir, f\"{frame_filename}.jpg\"), highlighted_frame)\n",
    "            cv2.imwrite(os.path.join(self.mask_dir, f\"{frame_filename}.png\"), specular_mask * 255)\n",
    "            \n",
    "            if self.debug and i % 10 == 0:\n",
    "                # Create visualization of the detection\n",
    "                wavelet_features = self._calculate_wavelet_features(frame)\n",
    "                normalized_features = (wavelet_features - np.min(wavelet_features)) / (np.max(wavelet_features) - np.min(wavelet_features) + 1e-6)\n",
    "                wavelet_vis = cv2.applyColorMap((normalized_features * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "                \n",
    "                # Create side-by-side comparison for debugging\n",
    "                comparison = np.hstack((frame, highlighted_frame))\n",
    "                cv2.imwrite(os.path.join(self.debug_dir, f\"{frame_filename}_comparison.jpg\"), comparison)\n",
    "                cv2.imwrite(os.path.join(self.debug_dir, f\"{frame_filename}_wavelet.jpg\"), wavelet_vis)\n",
    "            \n",
    "            processed_images += 1\n",
    "        \n",
    "        # Calculate processing stats\n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Processed {processed_images} images in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        return processed_images\n",
    "    \n",
    "    def process_single_image(self, image_path, output_prefix):\n",
    "        \"\"\"\n",
    "        Process a single image and generate dataset entries.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "            output_prefix: Prefix for output filenames\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (original, highlighted, mask) images\n",
    "        \"\"\"\n",
    "        # Read the image\n",
    "        frame = cv2.imread(image_path)\n",
    "        if frame is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "                \n",
    "        # Detect specular reflections\n",
    "        specular_mask = self._detect_specular_regions(frame)\n",
    "        \n",
    "        # Create highlighted version\n",
    "        highlighted_frame = self._create_highlighted_image(frame, specular_mask)\n",
    "        \n",
    "        # Save the pair of images\n",
    "        cv2.imwrite(os.path.join(self.original_dir, f\"{output_prefix}.jpg\"), frame)\n",
    "        cv2.imwrite(os.path.join(self.highlighted_dir, f\"{output_prefix}.jpg\"), highlighted_frame)\n",
    "        cv2.imwrite(os.path.join(self.mask_dir, f\"{output_prefix}.png\"), specular_mask * 255)\n",
    "        \n",
    "        if self.debug:\n",
    "            # Create visualization of the detection\n",
    "            wavelet_features = self._calculate_wavelet_features(frame)\n",
    "            normalized_features = (wavelet_features - np.min(wavelet_features)) / (np.max(wavelet_features) - np.min(wavelet_features) + 1e-6)\n",
    "            wavelet_vis = cv2.applyColorMap((normalized_features * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "            \n",
    "            # Create side-by-side comparison for debugging\n",
    "            comparison = np.hstack((frame, highlighted_frame))\n",
    "            cv2.imwrite(os.path.join(self.debug_dir, f\"{output_prefix}_comparison.jpg\"), comparison)\n",
    "            cv2.imwrite(os.path.join(self.debug_dir, f\"{output_prefix}_wavelet.jpg\"), wavelet_vis)\n",
    "        \n",
    "        return frame, highlighted_frame, specular_mask\n",
    "    \n",
    "    def create_train_val_split(self, val_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Create train/validation split from generated dataset\n",
    "        \n",
    "        Args:\n",
    "            val_ratio: Portion of data to use for validation\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with train/val split information\n",
    "        \"\"\"\n",
    "        # Get all image files\n",
    "        all_files = [os.path.basename(f) for f in glob.glob(os.path.join(self.original_dir, \"*.jpg\"))]\n",
    "        \n",
    "        # Shuffle the files\n",
    "        random.shuffle(all_files)\n",
    "        \n",
    "        # Split into train and validation\n",
    "        split_idx = int(len(all_files) * (1 - val_ratio))\n",
    "        train_files = all_files[:split_idx]\n",
    "        val_files = all_files[split_idx:]\n",
    "        \n",
    "        # Create train/val directories\n",
    "        train_dir = os.path.join(self.output_dir, \"train\")\n",
    "        val_dir = os.path.join(self.output_dir, \"val\")\n",
    "        \n",
    "        for directory in [train_dir, val_dir]:\n",
    "            os.makedirs(os.path.join(directory, \"original\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(directory, \"highlighted\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(directory, \"masks\"), exist_ok=True)\n",
    "        \n",
    "        # Copy files to train/val directories\n",
    "        for file in train_files:\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            os.system(f\"cp {os.path.join(self.original_dir, file)} {os.path.join(train_dir, 'original', file)}\")\n",
    "            os.system(f\"cp {os.path.join(self.highlighted_dir, file)} {os.path.join(train_dir, 'highlighted', file)}\")\n",
    "            os.system(f\"cp {os.path.join(self.mask_dir, base_name + '.png')} {os.path.join(train_dir, 'masks', base_name + '.png')}\")\n",
    "        \n",
    "        for file in val_files:\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            os.system(f\"cp {os.path.join(self.original_dir, file)} {os.path.join(val_dir, 'original', file)}\")\n",
    "            os.system(f\"cp {os.path.join(self.highlighted_dir, file)} {os.path.join(val_dir, 'highlighted', file)}\")\n",
    "            os.system(f\"cp {os.path.join(self.mask_dir, base_name + '.png')} {os.path.join(val_dir, 'masks', base_name + '.png')}\")\n",
    "        \n",
    "        split_info = {\n",
    "            \"total_files\": len(all_files),\n",
    "            \"train_files\": len(train_files),\n",
    "            \"val_files\": len(val_files),\n",
    "            \"train_ratio\": 1 - val_ratio,\n",
    "            \"val_ratio\": val_ratio\n",
    "        }\n",
    "        \n",
    "        print(f\"Created dataset split: {split_info['train_files']} training samples, {split_info['val_files']} validation samples\")\n",
    "        \n",
    "        return split_info\n",
    "    \n",
    "    def show_sample_detections(self, num_samples=5):\n",
    "        \"\"\"\n",
    "        Display sample detections from the generated dataset\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of samples to display\n",
    "        \"\"\"\n",
    "        # Get random samples\n",
    "        all_files = glob.glob(os.path.join(self.original_dir, \"*.jpg\"))\n",
    "        if len(all_files) == 0:\n",
    "            print(\"No samples available. Generate dataset first.\")\n",
    "            return\n",
    "        \n",
    "        samples = random.sample(all_files, min(num_samples, len(all_files)))\n",
    "        \n",
    "        # Display samples\n",
    "        fig, axes = plt.subplots(len(samples), 3, figsize=(15, 5 * len(samples)))\n",
    "        if len(samples) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, sample_path in enumerate(samples):\n",
    "            base_name = os.path.basename(sample_path)\n",
    "            base_name_no_ext = os.path.splitext(base_name)[0]\n",
    "            \n",
    "            # Load images\n",
    "            original = cv2.imread(sample_path)\n",
    "            highlighted = cv2.imread(os.path.join(self.highlighted_dir, base_name))\n",
    "            mask = cv2.imread(os.path.join(self.mask_dir, base_name_no_ext + \".png\"), cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Convert to RGB for display\n",
    "            original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "            highlighted = cv2.cvtColor(highlighted, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Display\n",
    "            axes[i][0].imshow(original)\n",
    "            axes[i][0].set_title('Original')\n",
    "            axes[i][0].axis('off')\n",
    "            \n",
    "            axes[i][1].imshow(highlighted)\n",
    "            axes[i][1].set_title('Highlighted')\n",
    "            axes[i][1].axis('off')\n",
    "            \n",
    "            axes[i][2].imshow(mask, cmap='gray')\n",
    "            axes[i][2].set_title('Mask')\n",
    "            axes[i][2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Function to generate dataset from video for self-supervised learning\n",
    "def generate_reflection_dataset(input_video_path,\n",
    "                               threshold=0.15,\n",
    "                               wavelet='db4',\n",
    "                               level=3,\n",
    "                               highlight_color=(0, 255, 0),\n",
    "                               sample_rate=5,\n",
    "                               max_frames=None,\n",
    "                               use_gpu=True,\n",
    "                               debug=True,\n",
    "                               create_split=True):\n",
    "    \"\"\"\n",
    "    Generate a dataset of original and highlighted images for self-supervised learning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_video_path : str\n",
    "        Path to the input video file\n",
    "    threshold : float\n",
    "        Threshold for detecting specular reflections (default: 0.15)\n",
    "    wavelet : str\n",
    "        Wavelet type to use for the transform (default: 'db4')\n",
    "    level : int\n",
    "        Level of wavelet decomposition (default: 3)\n",
    "    highlight_color : tuple\n",
    "        RGB color for highlighting reflections (default: green (0, 255, 0))\n",
    "    sample_rate : int\n",
    "        Process every Nth frame (default: 5)\n",
    "    max_frames : int or None\n",
    "        Maximum number of frames to process (default: None, process all)\n",
    "    use_gpu : bool\n",
    "        Whether to use GPU acceleration if available (default: True)\n",
    "    debug : bool\n",
    "        Whether to save debug visualizations (default: True)\n",
    "    create_split : bool\n",
    "        Whether to create train/validation split (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    generator : SpecularReflectionDatasetGenerator\n",
    "        The dataset generator object for further use\n",
    "    \"\"\"\n",
    "    # Create the generator\n",
    "    generator = SpecularReflectionDatasetGenerator(\n",
    "        wavelet=wavelet,\n",
    "        threshold=threshold,\n",
    "        level=level,\n",
    "        use_gpu=use_gpu,\n",
    "        highlight_color=highlight_color,\n",
    "        debug=debug\n",
    "    )\n",
    "    \n",
    "    # Process the video\n",
    "    video_name = os.path.splitext(os.path.basename(input_video_path))[0]\n",
    "    num_frames = generator.process_video(\n",
    "        input_video_path, \n",
    "        video_name, \n",
    "        sample_rate=sample_rate,\n",
    "        max_frames=max_frames\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {num_frames} samples from video\")\n",
    "    \n",
    "    # Create train/val split if requested\n",
    "    if create_split and num_frames > 0:\n",
    "        split_info = generator.create_train_val_split()\n",
    "        print(f\"Train/val split: {split_info}\")\n",
    "    \n",
    "    # Show samples\n",
    "    generator.show_sample_detections(num_samples=3)\n",
    "    \n",
    "    return generator\n",
    "\n",
    "# Process an image where specular reflections are already identified\n",
    "def process_sample_images(image1_path, image2_path, threshold=0.15):\n",
    "    \"\"\"\n",
    "    Process sample images to see how the reflection detection works.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image1_path : str\n",
    "        Path to the first sample image\n",
    "    image2_path : str\n",
    "        Path to the second sample image\n",
    "    threshold : float\n",
    "        Threshold for detecting specular reflections (default: 0.15)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of processed images\n",
    "    \"\"\"\n",
    "    # Create a generator with custom threshold\n",
    "    generator = SpecularReflectionDatasetGenerator(threshold=threshold)\n",
    "    \n",
    "    # Process each image\n",
    "    img1_orig, img1_highlighted, img1_mask = generator.process_single_image(image1_path, \"sample1\")\n",
    "    img2_orig, img2_highlighted, img2_mask = generator.process_single_image(image2_path, \"sample2\")\n",
    "    \n",
    "    # Display the results\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Convert to RGB for display\n",
    "    img1_orig_rgb = cv2.cvtColor(img1_orig, cv2.COLOR_BGR2RGB)\n",
    "    img1_highlighted_rgb = cv2.cvtColor(img1_highlighted, cv2.COLOR_BGR2RGB)\n",
    "    img2_orig_rgb = cv2.cvtColor(img2_orig, cv2.COLOR_BGR2RGB)\n",
    "    img2_highlighted_rgb = cv2.cvtColor(img2_highlighted, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # First image\n",
    "    axes[0, 0].imshow(img1_orig_rgb)\n",
    "    axes[0, 0].set_title('Original 1')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(img1_highlighted_rgb)\n",
    "    axes[0, 1].set_title('Highlighted 1')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(img1_mask, cmap='gray')\n",
    "    axes[0, 2].set_title('Mask 1')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Second image\n",
    "    axes[1, 0].imshow(img2_orig_rgb)\n",
    "    axes[1, 0].set_title('Original 2')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(img2_highlighted_rgb)\n",
    "    axes[1, 1].set_title('Highlighted 2')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(img2_mask, cmap='gray')\n",
    "    axes[1, 2].set_title('Mask 2')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return (img1_orig, img1_highlighted, img1_mask), (img2_orig, img2_highlighted, img2_mask)\n",
    "\n",
    "# Example usage:\n",
    "# 1. Process sample images\n",
    "# process_sample_images('image1.jpg', 'image2.jpg', threshold=0.15)\n",
    "\n",
    "# Generate a complete dataset from video\n",
    "generator = generate_reflection_dataset(\n",
    "    'video.mp4',\n",
    "    threshold=0.08,               # Adjust sensitivity\n",
    "    highlight_color=(0, 255, 0),  # Green highlighting\n",
    "    sample_rate=5,                # Process every 5th frame\n",
    "    max_frames=5000,              # Limit number of frames (optional)\n",
    "    debug=True                    # Save debug visualizations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f190b-7e52-4ee8-a617-9d961949b114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
