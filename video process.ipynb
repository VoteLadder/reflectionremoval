{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a580a-58f9-42cd-8491-ebfdee9a9a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: video.mp4\n",
      "Output will be saved to the 'reflection_processing' directory\n",
      "Maximum brightness threshold: 220\n",
      "GPU Available: True\n",
      "Using GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   1%|          | 10/1200 [00:01<03:22,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 222121/921600 pixels (24.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   2%|▏         | 20/1200 [00:03<03:47,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 225519/921600 pixels (24.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   2%|▎         | 30/1200 [00:05<03:52,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 224416/921600 pixels (24.35%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   3%|▎         | 40/1200 [00:07<03:52,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 225801/921600 pixels (24.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   4%|▍         | 50/1200 [00:09<03:58,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 226716/921600 pixels (24.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   5%|▌         | 60/1200 [00:11<03:53,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 225482/921600 pixels (24.47%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   6%|▌         | 70/1200 [00:13<03:49,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 226943/921600 pixels (24.62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   7%|▋         | 80/1200 [00:15<03:45,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 225442/921600 pixels (24.46%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   8%|▊         | 90/1200 [00:17<03:36,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 219770/921600 pixels (23.85%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   8%|▊         | 100/1200 [00:19<03:31,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 216465/921600 pixels (23.49%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   9%|▉         | 110/1200 [00:21<03:30,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 222304/921600 pixels (24.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  10%|█         | 120/1200 [00:23<03:35,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 226686/921600 pixels (24.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  11%|█         | 130/1200 [00:25<03:26,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 229379/921600 pixels (24.89%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  12%|█▏        | 140/1200 [00:27<03:26,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 232038/921600 pixels (25.18%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  12%|█▎        | 150/1200 [00:29<03:25,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository coverage: 234470/921600 pixels (25.44%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  13%|█▎        | 160/1200 [00:30<03:26,  5.03it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Specular Reflection Processor for Endoscopic Video\n",
    "\n",
    "Just change the videoname variable at the top of this file and run the entire script.\n",
    "The program will process the video and save samples to their respective folders.\n",
    "\"\"\"\n",
    "\n",
    "# ============= CONFIGURATION (CHANGE THIS) =============\n",
    "videoname = \"video.mp4\"  # Set this to your video filename\n",
    "\n",
    "# ============= PROCESSING PARAMETERS =============\n",
    "# You can adjust these parameters as needed\n",
    "threshold = 0.06        # Threshold for specular reflection detection (0.05-0.20)\n",
    "sample_rate = 0         # Save a frame every N frames (0 to disable saving individual frames)\n",
    "max_frames = 1200       # Set to a number to limit frames processed, or None for all\n",
    "highlight_color = (0, 255, 0)  # Green color for highlighting reflections\n",
    "use_debug = True        # Whether to save debug visualizations\n",
    "use_gpu = True          # Whether to use GPU for processing if available\n",
    "fill_radius = 10        # Radius (in pixels) to average for filling when no data is available\n",
    "\n",
    "# Color deviation filter parameters\n",
    "max_color_deviation = 150     # Maximum allowed color difference for repository updates (0-255)\n",
    "min_initial_frames = 20      # Minimum frames to build initial repository before applying deviation filter\n",
    "strict_update_factor = 0.5   # Factor to make threshold stricter for previously reflective areas (0.0-1.0)\n",
    "max_brightness = 220         # Maximum pixel brightness to add to repository (0-255)\n",
    "\n",
    "# ============= IMPORTS =============\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm  # Using tqdm instead of tqdm.notebook for broader compatibility\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# ============= MAIN CLASS =============\n",
    "class SpecularReflectionProcessor:\n",
    "    def __init__(self, \n",
    "                 wavelet='db4',         \n",
    "                 threshold=0.15,        \n",
    "                 level=3,               \n",
    "                 use_gpu=True,          \n",
    "                 highlight_color=(0, 255, 0),  \n",
    "                 debug=True):           \n",
    "        \n",
    "        self.wavelet = wavelet\n",
    "        self.threshold = threshold\n",
    "        self.level = level\n",
    "        self.use_gpu = use_gpu and torch.cuda.is_available()\n",
    "        self.highlight_color = highlight_color\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Create output directories\n",
    "        self.output_dir = \"reflection_processing\"\n",
    "        self.original_dir = os.path.join(self.output_dir, \"original\")\n",
    "        self.highlighted_dir = os.path.join(self.output_dir, \"highlighted\")\n",
    "        self.processed_dir = os.path.join(self.output_dir, \"processed\")  \n",
    "        self.mask_dir = os.path.join(self.output_dir, \"masks\")\n",
    "        self.debug_dir = os.path.join(self.output_dir, \"debug\")\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        for directory in [self.output_dir, self.original_dir, self.highlighted_dir, \n",
    "                          self.processed_dir, self.mask_dir, self.debug_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "        print(f\"Using GPU: {self.use_gpu}\")\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            \n",
    "        # Initialize repository of non-reflective pixels\n",
    "        self.baseline_repository = None\n",
    "        self.last_valid_frame = None\n",
    "        self.processed_frame_count = 0\n",
    "    \n",
    "    def _calculate_wavelet_features(self, frame):\n",
    "        \"\"\"Calculate wavelet decomposition features from the frame.\"\"\"\n",
    "        # Convert to grayscale for wavelet analysis\n",
    "        if len(frame.shape) == 3:\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = frame.copy()\n",
    "            \n",
    "        # Apply wavelet transform\n",
    "        coeffs = pywt.wavedec2(gray, self.wavelet, level=self.level)\n",
    "        \n",
    "        # Extract high-frequency components (details)\n",
    "        details = []\n",
    "        for detail_coeffs in coeffs[1:]:\n",
    "            details.extend([np.abs(detail_coeffs[i]) for i in range(3)])\n",
    "        \n",
    "        # Normalize and stack details\n",
    "        detail_features = np.stack([cv2.resize(d, (gray.shape[1], gray.shape[0])) for d in details])\n",
    "        detail_features = np.max(detail_features, axis=0)\n",
    "        \n",
    "        return detail_features\n",
    "    \n",
    "    def _detect_specular_regions(self, frame):\n",
    "        \"\"\"Detect specular reflection regions using wavelets and HSV analysis.\"\"\"\n",
    "        # Calculate wavelet features\n",
    "        features = self._calculate_wavelet_features(frame)\n",
    "        \n",
    "        # Convert to HSV for better highlight detection\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        \n",
    "        # High Value and low Saturation indicates specular reflection\n",
    "        high_v = v > 200\n",
    "        low_s = s < 40\n",
    "        \n",
    "        # Combine with wavelet features\n",
    "        normalized_features = (features - np.min(features)) / (np.max(features) - np.min(features) + 1e-6)\n",
    "        wavelet_mask = normalized_features > self.threshold\n",
    "        \n",
    "        # Combine masks\n",
    "        combined_mask = np.logical_or(np.logical_and(high_v, low_s), wavelet_mask)\n",
    "        \n",
    "        # Clean up the mask\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        mask = cv2.morphologyEx(combined_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # Further refine the mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        refined_mask = np.zeros_like(mask)\n",
    "        \n",
    "        # Only keep contours of meaningful size\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 10:  # Minimum area threshold\n",
    "                cv2.drawContours(refined_mask, [contour], -1, 1, -1)\n",
    "        \n",
    "        return refined_mask\n",
    "    \n",
    "    def _create_highlighted_image(self, frame, mask):\n",
    "        \"\"\"Create version of frame with reflections highlighted in green.\"\"\"\n",
    "        highlighted = frame.copy()\n",
    "        mask_bool = mask.astype(bool)\n",
    "        highlighted[mask_bool] = self.highlight_color\n",
    "        return highlighted\n",
    "    \n",
    "    def _update_baseline_repository(self, frame, mask, frame_count=0):\n",
    "        \"\"\"\n",
    "        Update repository of non-reflective pixels with color deviation and brightness filtering.\n",
    "        \n",
    "        Args:\n",
    "            frame: The input frame\n",
    "            mask: Binary mask of reflective areas (1=reflection, 0=valid tissue)\n",
    "            frame_count: Current frame count for initial repository building\n",
    "        \"\"\"\n",
    "        # Always use the specular mask to identify reflective regions\n",
    "        reflective_areas = mask.astype(bool)\n",
    "        non_reflective_areas = ~reflective_areas\n",
    "        \n",
    "        # Calculate brightness for brightness filtering\n",
    "        # Convert to grayscale as a simple brightness measure\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Create a mask for areas that are too bright (potentially reflective)\n",
    "        too_bright = gray > max_brightness\n",
    "        \n",
    "        # Combined safety mask: exclude reflective areas AND too bright areas\n",
    "        safe_areas = non_reflective_areas & ~too_bright\n",
    "        \n",
    "        if self.baseline_repository is None:\n",
    "            # Initialize the repository with the current frame\n",
    "            self.baseline_repository = frame.copy()\n",
    "            # Set reflective areas and too bright areas to zeros\n",
    "            self.baseline_repository[reflective_areas | too_bright] = 0\n",
    "            self.last_valid_frame = frame.copy()\n",
    "            # Initialize frame counter for color deviation filtering\n",
    "            self.processed_frame_count = 1\n",
    "        else:\n",
    "            # Update strategy depends on how many frames we've processed\n",
    "            if self.processed_frame_count < min_initial_frames:\n",
    "                # During initial repository building, update ONLY safe areas\n",
    "                self.baseline_repository[safe_areas] = frame[safe_areas]\n",
    "            else:\n",
    "                # After initial phase, apply color deviation filter\n",
    "                # But ONLY for areas that are safe (non-reflective and not too bright)\n",
    "                # Calculate color deviation between current frame and repository\n",
    "                color_diff = np.abs(frame.astype(np.float32) - self.baseline_repository.astype(np.float32))\n",
    "                # Sum differences across color channels\n",
    "                total_diff = np.sum(color_diff, axis=2)\n",
    "                # Identify areas with acceptable color deviation\n",
    "                acceptable_deviation = total_diff < max_color_deviation\n",
    "                \n",
    "                # Only consider safe areas for updates\n",
    "                valid_updates = safe_areas & acceptable_deviation\n",
    "                \n",
    "                # We still need to handle previously reflective areas with special care\n",
    "                valid_repository = (self.baseline_repository != 0).all(axis=2)\n",
    "                previously_reflective = ~valid_repository\n",
    "                \n",
    "                # For areas that were previously reflective but are now safe,\n",
    "                # we apply a stricter threshold\n",
    "                if np.any(previously_reflective & valid_updates):\n",
    "                    # Apply stricter threshold for previously reflective areas\n",
    "                    strictest_threshold = max_color_deviation * strict_update_factor\n",
    "                    stricter_acceptable = total_diff < strictest_threshold\n",
    "                    \n",
    "                    # Only update previously reflective areas if they meet the stricter threshold\n",
    "                    # AND are currently safe\n",
    "                    valid_updates = valid_updates & (~previously_reflective | (stricter_acceptable & safe_areas))\n",
    "                \n",
    "                # Final safety check: NEVER update unsafe areas\n",
    "                valid_updates = valid_updates & safe_areas\n",
    "                \n",
    "                # Apply the validated updates\n",
    "                self.baseline_repository[valid_updates] = frame[valid_updates]\n",
    "            \n",
    "            # Keep track of the most recent valid frame\n",
    "            self.last_valid_frame = frame.copy()\n",
    "            self.processed_frame_count += 1\n",
    "            \n",
    "            # For debugging: count how many pixels are in the repository\n",
    "            if self.debug and self.processed_frame_count % 10 == 0:\n",
    "                valid_pixels = (self.baseline_repository != 0).all(axis=2).sum()\n",
    "                total_pixels = self.baseline_repository.shape[0] * self.baseline_repository.shape[1]\n",
    "                print(f\"Repository coverage: {valid_pixels}/{total_pixels} pixels ({valid_pixels/total_pixels*100:.2f}%)\")\n",
    "    \n",
    "    def _inpaint_reflections(self, frame, mask, radius=10):\n",
    "        \"\"\"\n",
    "        Replace reflective areas using baseline repository only. \n",
    "        If no repository data exists, keep original pixels.\n",
    "        \n",
    "        Args:\n",
    "            frame: The input frame\n",
    "            mask: Binary mask of reflective areas\n",
    "            radius: Unused parameter (kept for compatibility)\n",
    "            \n",
    "        Returns:\n",
    "            Processed frame with reflections removed where repository data exists\n",
    "        \"\"\"\n",
    "        if self.baseline_repository is None:\n",
    "            # If no repository exists yet, just return the original frame\n",
    "            return frame.copy()\n",
    "        \n",
    "        # Create a processed frame\n",
    "        processed = frame.copy()\n",
    "        \n",
    "        # Get reflective areas\n",
    "        reflective_areas = mask.astype(bool)\n",
    "        \n",
    "        # Use the baseline repository where valid data exists\n",
    "        valid_repository = (self.baseline_repository != 0).all(axis=2) if len(self.baseline_repository.shape) == 3 else (self.baseline_repository != 0)\n",
    "        usable_repository = valid_repository & reflective_areas\n",
    "        \n",
    "        if np.any(usable_repository):\n",
    "            # Use repository data where available\n",
    "            processed[usable_repository] = self.baseline_repository[usable_repository]\n",
    "        \n",
    "        # For areas not in repository, keep original pixels\n",
    "        # (This happens automatically since we're using a copy of the original frame)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def process_video(self, input_path, output_prefix, sample_rate=10, max_frames=None, fill_radius=10):\n",
    "        \"\"\"\n",
    "        Process video to detect, highlight and remove specular reflections.\n",
    "        Creates a multi-view output video showing original, processed, and highlighted views.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to input video file\n",
    "            output_prefix: Prefix for output filenames\n",
    "            sample_rate: Save a frame every N frames (0 to disable)\n",
    "            max_frames: Maximum number of frames to process\n",
    "            fill_radius: Radius parameter (unused but kept for compatibility)\n",
    "        \"\"\"\n",
    "        # Open the video file\n",
    "        video = cv2.VideoCapture(input_path)\n",
    "        if not video.isOpened():\n",
    "            raise ValueError(f\"Could not open video file: {input_path}\")\n",
    "        \n",
    "        # Get video properties\n",
    "        frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        \n",
    "        # Create composite video dimensions\n",
    "        # Layout: Top row: Original and Processed side by side\n",
    "        #         Bottom row: Highlighted reflections\n",
    "        comp_width = width * 2\n",
    "        comp_height = height * 2\n",
    "        \n",
    "        # For creating output video\n",
    "        output_video_path = os.path.join(self.output_dir, f\"{output_prefix}_tri_view.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (comp_width, comp_height))\n",
    "        \n",
    "        # Limit frames if specified\n",
    "        if max_frames is not None:\n",
    "            frame_count = min(frame_count, max_frames)\n",
    "        \n",
    "        # Process each frame\n",
    "        processed_frames = 0\n",
    "        saved_frames = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        processing_times = []  # Store processing times for FPS calculation\n",
    "        \n",
    "        self.baseline_repository = None  # Reset repository\n",
    "        self.processed_frame_count = 0  # Reset frame counter for color deviation filtering\n",
    "        \n",
    "        for i in tqdm(range(frame_count), desc=\"Processing video\"):\n",
    "            frame_start_time = time.time()\n",
    "            \n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Process every frame for repository updates\n",
    "            # Detect specular reflections\n",
    "            specular_mask = self._detect_specular_regions(frame)\n",
    "            \n",
    "            # Update the baseline repository with non-reflective areas\n",
    "            self._update_baseline_repository(frame, specular_mask, i)\n",
    "            \n",
    "            # Create highlighted version\n",
    "            highlighted_frame = self._create_highlighted_image(frame, specular_mask)\n",
    "            \n",
    "            # Create processed version (with reflections removed)\n",
    "            processed_frame = self._inpaint_reflections(frame, specular_mask)\n",
    "            \n",
    "            # Calculate processing time for this frame\n",
    "            frame_processing_time = time.time() - frame_start_time\n",
    "            processing_times.append(frame_processing_time)\n",
    "            \n",
    "            # Calculate FPS based on recent frames\n",
    "            recent_times = processing_times[-20:] if len(processing_times) > 20 else processing_times\n",
    "            avg_time = sum(recent_times) / len(recent_times)\n",
    "            fps_text = f\"Processing: {1.0/avg_time:.2f} frames/sec\"\n",
    "            \n",
    "            # Create the composite frame\n",
    "            # Create a black canvas\n",
    "            composite = np.zeros((comp_height, comp_width, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Add original frame (top left)\n",
    "            composite[0:height, 0:width] = frame\n",
    "            \n",
    "            # Add processed frame (top right)\n",
    "            composite[0:height, width:width*2] = processed_frame\n",
    "            \n",
    "            # Add highlighted frame (bottom, centered)\n",
    "            composite[height:height*2, width//2:width//2+width] = highlighted_frame\n",
    "            \n",
    "            # Add labels to each view\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.8\n",
    "            font_color = (255, 255, 255)\n",
    "            line_thickness = 2\n",
    "            \n",
    "            cv2.putText(composite, \"Original\", (10, 30), font, font_scale, font_color, line_thickness)\n",
    "            cv2.putText(composite, \"Processed\", (width + 10, 30), font, font_scale, font_color, line_thickness)\n",
    "            cv2.putText(composite, \"Highlighted\", (width//2 + 10, height + 30), font, font_scale, font_color, line_thickness)\n",
    "            \n",
    "            # Add FPS counter and frame info\n",
    "            cv2.putText(composite, fps_text, (10, comp_height - 20), font, font_scale, font_color, line_thickness)\n",
    "            frame_info = f\"Frame: {i+1}/{frame_count} | Filters: {'On' if self.processed_frame_count >= min_initial_frames else 'Off'} | Max bright: {max_brightness}\"\n",
    "            cv2.putText(composite, frame_info, (comp_width - 500, comp_height - 20), font, font_scale, font_color, line_thickness)\n",
    "            \n",
    "            # Write to video\n",
    "            out.write(composite)\n",
    "            \n",
    "            # Save individual frames if sample_rate is greater than 0\n",
    "            if sample_rate > 0 and i % sample_rate == 0:\n",
    "                frame_filename = f\"{output_prefix}_{saved_frames:04d}\"\n",
    "                \n",
    "                cv2.imwrite(os.path.join(self.original_dir, f\"{frame_filename}.jpg\"), frame)\n",
    "                cv2.imwrite(os.path.join(self.highlighted_dir, f\"{frame_filename}.jpg\"), highlighted_frame)\n",
    "                cv2.imwrite(os.path.join(self.processed_dir, f\"{frame_filename}.jpg\"), processed_frame)\n",
    "                cv2.imwrite(os.path.join(self.mask_dir, f\"{frame_filename}.png\"), specular_mask * 255)\n",
    "                \n",
    "                # Save the composite view\n",
    "                cv2.imwrite(os.path.join(self.debug_dir, f\"{frame_filename}_composite.jpg\"), composite)\n",
    "                \n",
    "                saved_frames += 1\n",
    "            \n",
    "            processed_frames += 1\n",
    "        \n",
    "        # Clean up\n",
    "        video.release()\n",
    "        out.release()\n",
    "        \n",
    "        # Calculate overall processing stats\n",
    "        end_time = time.time()\n",
    "        total_processing_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Processed {processed_frames} frames in {total_processing_time:.2f} seconds\")\n",
    "        print(f\"Average processing speed: {processed_frames/total_processing_time:.2f} frames/second\")\n",
    "        print(f\"Output composite video saved to: {output_video_path}\")\n",
    "        \n",
    "        return saved_frames\n",
    "    \n",
    "    def display_sample_results(self, num_samples=3):\n",
    "        \"\"\"Display sample results using matplotlib.\"\"\"\n",
    "        original_files = sorted(glob.glob(os.path.join(self.original_dir, \"*.jpg\")))\n",
    "        if not original_files:\n",
    "            print(\"No processed files found.\")\n",
    "            return\n",
    "            \n",
    "        # Select a few samples (every 10th saved frame)\n",
    "        samples = []\n",
    "        for i, path in enumerate(original_files):\n",
    "            if i % 10 == 0 and len(samples) < num_samples:\n",
    "                samples.append(path)\n",
    "        \n",
    "        if not samples:\n",
    "            samples = original_files[:num_samples]\n",
    "            \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(len(samples), 3, figsize=(15, 5 * len(samples)))\n",
    "        if len(samples) == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for i, orig_path in enumerate(samples):\n",
    "            base_name = os.path.basename(orig_path)\n",
    "            \n",
    "            # Load images\n",
    "            original = cv2.imread(orig_path)\n",
    "            highlighted = cv2.imread(os.path.join(self.highlighted_dir, base_name))\n",
    "            processed = cv2.imread(os.path.join(self.processed_dir, base_name))\n",
    "            \n",
    "            # Convert to RGB for display\n",
    "            original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "            highlighted = cv2.cvtColor(highlighted, cv2.COLOR_BGR2RGB)\n",
    "            processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Display\n",
    "            axes[i][0].imshow(original)\n",
    "            axes[i][0].set_title('Original')\n",
    "            axes[i][0].axis('off')\n",
    "            \n",
    "            axes[i][1].imshow(highlighted)\n",
    "            axes[i][1].set_title('Reflections Highlighted')\n",
    "            axes[i][1].axis('off')\n",
    "            \n",
    "            axes[i][2].imshow(processed)\n",
    "            axes[i][2].set_title('Processed (Reflections Removed)')\n",
    "            axes[i][2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"sample_results.png\"))\n",
    "        plt.show()\n",
    "\n",
    "# ============= MAIN FUNCTION =============\n",
    "if __name__ == \"__main__\":\n",
    "    # Extract video name without extension\n",
    "    video_base_name = os.path.splitext(os.path.basename(videoname))[0]\n",
    "    \n",
    "    print(f\"Processing video: {videoname}\")\n",
    "    print(f\"Output will be saved to the 'reflection_processing' directory\")\n",
    "    print(f\"Maximum brightness threshold: {max_brightness}\")\n",
    "    \n",
    "    # Create processor with specified parameters\n",
    "    processor = SpecularReflectionProcessor(\n",
    "        threshold=threshold,\n",
    "        highlight_color=highlight_color,\n",
    "        use_gpu=use_gpu,\n",
    "        debug=use_debug\n",
    "    )\n",
    "    \n",
    "    # Process the video\n",
    "    processor.process_video(\n",
    "        videoname,\n",
    "        video_base_name,\n",
    "        sample_rate=sample_rate,\n",
    "        max_frames=max_frames,\n",
    "        fill_radius=fill_radius\n",
    "    )\n",
    "    \n",
    "    # Display sample results if any samples were saved\n",
    "    if sample_rate > 0:\n",
    "        print(\"\\nDisplaying sample results...\")\n",
    "        processor.display_sample_results(num_samples=3)\n",
    "    \n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(\"Results saved in the following directories:\")\n",
    "    print(f\"  - Original frames: {processor.original_dir}\")\n",
    "    print(f\"  - Highlighted frames: {processor.highlighted_dir}\")\n",
    "    print(f\"  - Processed frames: {processor.processed_dir}\")\n",
    "    print(f\"  - Masks: {processor.mask_dir}\")\n",
    "    if use_debug:\n",
    "        print(f\"  - Debug visualizations: {processor.debug_dir}\")\n",
    "    print(f\"  - Processed video: {os.path.join(processor.output_dir, f'{video_base_name}_tri_view.mp4')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583aada0-2290-481d-bd80-bb79921a8d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
