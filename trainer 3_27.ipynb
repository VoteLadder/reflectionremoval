{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01db55c-737a-4b37-aabb-24833d8718c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_wavelets\n",
      "  Downloading pytorch_wavelets-1.3.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch_wavelets) (1.26.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pytorch_wavelets) (1.16.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch_wavelets) (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_wavelets) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch_wavelets) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->pytorch_wavelets) (1.3.0)\n",
      "Downloading pytorch_wavelets-1.3.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch_wavelets\n",
      "Successfully installed pytorch_wavelets-1.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085f1111-abae-43d8-a4d5-46f83c1bb69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear image directory: /notebooks/output_3_18/smoke/clear\n",
      "Blurry image directory: /notebooks/output_3_18/smoke/blurry\n",
      "Output directory: /notebooks/output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2\n",
      "Using device: cuda\n",
      "Limiting dataset from 10212 to 1000 samples.\n",
      "Using 4 dataloader workers.\n",
      "Loading test image pair with index: 5\n",
      "Training dataset size: 1000\n",
      "Test image shapes: Blurry=torch.Size([1, 3, 512, 512]), WaveletInput=torch.Size([1, 12, 256, 256]), Clear=torch.Size([1, 3, 512, 512])\n",
      "Saved initial prediction sample.\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 250/250 [01:57<00:00,  2.12it/s, avg_loss=0.019581, loss=0.009483, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 finished. Average Training Loss: 0.019581\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_001.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 250/250 [02:01<00:00,  2.06it/s, avg_loss=0.010446, loss=0.008062, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 finished. Average Training Loss: 0.010446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 250/250 [02:02<00:00,  2.05it/s, avg_loss=0.008094, loss=0.008627, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 finished. Average Training Loss: 0.008094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 250/250 [02:01<00:00,  2.06it/s, avg_loss=0.006549, loss=0.006559, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 finished. Average Training Loss: 0.006549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 250/250 [02:01<00:00,  2.06it/s, avg_loss=0.006011, loss=0.005653, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 finished. Average Training Loss: 0.006011\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_005.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 250/250 [01:59<00:00,  2.10it/s, avg_loss=0.005432, loss=0.004233, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 finished. Average Training Loss: 0.005432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 250/250 [01:58<00:00,  2.10it/s, avg_loss=0.005180, loss=0.005491, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 finished. Average Training Loss: 0.005180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 250/250 [01:59<00:00,  2.10it/s, avg_loss=0.004965, loss=0.004451, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 finished. Average Training Loss: 0.004965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 250/250 [01:58<00:00,  2.10it/s, avg_loss=0.004914, loss=0.005630, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 finished. Average Training Loss: 0.004914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.004649, loss=0.004817, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 finished. Average Training Loss: 0.004649\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_010.png\n",
      "Saved checkpoint to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/checkpoints/wavelet_unet_bn_epoch_010.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.004300, loss=0.004293, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 finished. Average Training Loss: 0.004300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.004061, loss=0.004935, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 finished. Average Training Loss: 0.004061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.003948, loss=0.003790, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 finished. Average Training Loss: 0.003948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.003842, loss=0.003186, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 finished. Average Training Loss: 0.003842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.003973, loss=0.004707, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 finished. Average Training Loss: 0.003973\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 250/250 [01:57<00:00,  2.12it/s, avg_loss=0.003702, loss=0.003009, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 finished. Average Training Loss: 0.003702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.003696, loss=0.004715, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 finished. Average Training Loss: 0.003696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.003518, loss=0.003050, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 finished. Average Training Loss: 0.003518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 250/250 [01:58<00:00,  2.11it/s, avg_loss=0.003615, loss=0.002831, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 finished. Average Training Loss: 0.003615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.003446, loss=0.002454, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 finished. Average Training Loss: 0.003446\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_020.png\n",
      "Saved checkpoint to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/checkpoints/wavelet_unet_bn_epoch_020.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 250/250 [01:57<00:00,  2.12it/s, avg_loss=0.003486, loss=0.003828, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 finished. Average Training Loss: 0.003486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 250/250 [01:57<00:00,  2.13it/s, avg_loss=0.003453, loss=0.003284, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 finished. Average Training Loss: 0.003453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 250/250 [01:57<00:00,  2.12it/s, avg_loss=0.003373, loss=0.002503, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 finished. Average Training Loss: 0.003373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 250/250 [01:58<00:00,  2.10it/s, avg_loss=0.003238, loss=0.003157, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 finished. Average Training Loss: 0.003238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.003289, loss=0.002895, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 finished. Average Training Loss: 0.003289\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_025.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 250/250 [01:59<00:00,  2.09it/s, avg_loss=0.003127, loss=0.002864, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 finished. Average Training Loss: 0.003127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 250/250 [01:59<00:00,  2.10it/s, avg_loss=0.003090, loss=0.002898, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 finished. Average Training Loss: 0.003090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 250/250 [01:59<00:00,  2.09it/s, avg_loss=0.003062, loss=0.003061, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 finished. Average Training Loss: 0.003062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 250/250 [01:58<00:00,  2.10it/s, avg_loss=0.003025, loss=0.003193, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 finished. Average Training Loss: 0.003025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 250/250 [01:59<00:00,  2.10it/s, avg_loss=0.003522, loss=0.004363, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 finished. Average Training Loss: 0.003522\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_030.png\n",
      "Saved checkpoint to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/checkpoints/wavelet_unet_bn_epoch_030.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 250/250 [02:00<00:00,  2.07it/s, avg_loss=0.003613, loss=0.002887, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 finished. Average Training Loss: 0.003613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 250/250 [02:00<00:00,  2.08it/s, avg_loss=0.003177, loss=0.002924, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 finished. Average Training Loss: 0.003177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 250/250 [02:00<00:00,  2.07it/s, avg_loss=0.003027, loss=0.002817, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 finished. Average Training Loss: 0.003027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 250/250 [02:00<00:00,  2.08it/s, avg_loss=0.003042, loss=0.002904, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 finished. Average Training Loss: 0.003042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 250/250 [02:00<00:00,  2.08it/s, avg_loss=0.002830, loss=0.002987, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 finished. Average Training Loss: 0.002830\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_035.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 250/250 [02:01<00:00,  2.06it/s, avg_loss=0.002902, loss=0.002818, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 finished. Average Training Loss: 0.002902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 250/250 [02:01<00:00,  2.06it/s, avg_loss=0.002792, loss=0.002122, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 finished. Average Training Loss: 0.002792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 250/250 [02:01<00:00,  2.06it/s, avg_loss=0.002800, loss=0.002710, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 finished. Average Training Loss: 0.002800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 250/250 [02:01<00:00,  2.06it/s, avg_loss=0.002751, loss=0.002690, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50 finished. Average Training Loss: 0.002751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 250/250 [02:00<00:00,  2.07it/s, avg_loss=0.002713, loss=0.002182, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50 finished. Average Training Loss: 0.002713\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_040.png\n",
      "Saved checkpoint to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/checkpoints/wavelet_unet_bn_epoch_040.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.002675, loss=0.002594, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50 finished. Average Training Loss: 0.002675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 250/250 [01:57<00:00,  2.13it/s, avg_loss=0.002702, loss=0.002138, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50 finished. Average Training Loss: 0.002702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.002675, loss=0.003826, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50 finished. Average Training Loss: 0.002675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.002613, loss=0.002997, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50 finished. Average Training Loss: 0.002613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.002622, loss=0.002590, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50 finished. Average Training Loss: 0.002622\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_045.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 250/250 [01:57<00:00,  2.12it/s, avg_loss=0.002538, loss=0.002249, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50 finished. Average Training Loss: 0.002538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 250/250 [01:58<00:00,  2.12it/s, avg_loss=0.002514, loss=0.003646, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50 finished. Average Training Loss: 0.002514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 250/250 [01:57<00:00,  2.12it/s, avg_loss=0.002588, loss=0.003038, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50 finished. Average Training Loss: 0.002588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 250/250 [01:57<00:00,  2.13it/s, avg_loss=0.002575, loss=0.002375, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50 finished. Average Training Loss: 0.002575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 250/250 [01:56<00:00,  2.14it/s, avg_loss=0.002510, loss=0.002309, lr=5.0e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50 finished. Average Training Loss: 0.002510\n",
      "Saved prediction sample to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/samples/pred_epoch_050.png\n",
      "Saved checkpoint to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/checkpoints/wavelet_unet_bn_epoch_050.pth\n",
      "Training finished.\n",
      "Saved final model to output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2/wavelet_unet_bn_final.pth\n",
      "Training process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pywt\n",
    "from tqdm import tqdm\n",
    "from pytorch_wavelets import DWTForward\n",
    "import traceback\n",
    "\n",
    "# Custom Dataset for Paired Smoky/Clear Images\n",
    "class ColonoscopyDataset(Dataset):\n",
    "    def __init__(self, clear_dir, blurry_dir, wavelet='db1', max_samples=None):\n",
    "        try:\n",
    "            self.clear_files = sorted([f for f in os.listdir(clear_dir) if os.path.isfile(os.path.join(clear_dir, f))])\n",
    "            self.blurry_files = sorted([f for f in os.listdir(blurry_dir) if os.path.isfile(os.path.join(blurry_dir, f))])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error accessing directories: {e}\")\n",
    "            raise e\n",
    "\n",
    "        if not self.clear_files or not self.blurry_files:\n",
    "            raise ValueError(f\"No files found in directories: Clear={clear_dir}, Blurry={blurry_dir}\")\n",
    "\n",
    "        min_len = min(len(self.clear_files), len(self.blurry_files))\n",
    "        if len(self.clear_files) != len(self.blurry_files):\n",
    "            print(f\"Warning: Mismatched file counts. Using {min_len} pairs.\")\n",
    "            self.clear_files = self.clear_files[:min_len]\n",
    "            self.blurry_files = self.blurry_files[:min_len]\n",
    "\n",
    "        if max_samples is not None and max_samples > 0 and min_len > max_samples:\n",
    "            print(f\"Limiting dataset from {min_len} to {max_samples} samples.\")\n",
    "            self.clear_files = self.clear_files[:max_samples]\n",
    "            self.blurry_files = self.blurry_files[:max_samples]\n",
    "\n",
    "        self.clear_dir = clear_dir\n",
    "        self.blurry_dir = blurry_dir\n",
    "        self.wavelet = wavelet\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clear_files)\n",
    "\n",
    "    def _load_and_preprocess_image(self, img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            base, ext = os.path.splitext(img_path)\n",
    "            if not ext:\n",
    "                for try_ext in [\".png\", \".jpg\"]:\n",
    "                    img = cv2.imread(img_path + try_ext)\n",
    "                    if img is not None:\n",
    "                        break\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "        if len(img.shape) == 2:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "        elif img.shape[2] == 4:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        elif img.shape[2] != 3:\n",
    "            raise ValueError(f\"Image {img_path} has unexpected shape {img.shape}\")\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            if idx >= len(self.clear_files):\n",
    "                raise IndexError(f\"Index {idx} out of bounds: {len(self.clear_files)}\")\n",
    "\n",
    "            clear_img_path = os.path.join(self.clear_dir, self.clear_files[idx])\n",
    "            blurry_img_path = os.path.join(self.blurry_dir, self.blurry_files[idx])\n",
    "\n",
    "            clear_img_np = self._load_and_preprocess_image(clear_img_path)\n",
    "            blurry_img_np = self._load_and_preprocess_image(blurry_img_path)\n",
    "\n",
    "            if clear_img_np.shape[:2] != blurry_img_np.shape[:2]:\n",
    "                target_h, target_w = clear_img_np.shape[:2]\n",
    "                blurry_img_np = cv2.resize(blurry_img_np, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            # print(f\"Idx {idx}: clear_img range [{clear_img_np.min():.3f}, {clear_img_np.max():.3f}], shape {clear_img_np.shape}\")\n",
    "            # print(f\"Idx {idx}: blurry_img range [{blurry_img_np.min():.3f}, {blurry_img_np.max():.3f}], shape {blurry_img_np.shape}\")\n",
    "\n",
    "            clear_img_tensor = torch.from_numpy(clear_img_np).permute(2, 0, 1).float()\n",
    "            blurry_img_tensor = torch.from_numpy(blurry_img_np).permute(2, 0, 1).float()\n",
    "\n",
    "            return blurry_img_tensor, clear_img_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item at index {idx}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            if idx > 0:\n",
    "                return self.__getitem__(0)\n",
    "            else:\n",
    "                dummy_tensor = torch.zeros((3, 256, 256), dtype=torch.float32)\n",
    "                return dummy_tensor, dummy_tensor\n",
    "\n",
    "# Wavelet-U-Net Model with BatchNorm\n",
    "class WaveletUNet_BN(nn.Module):\n",
    "    def __init__(self, in_channels=3, wavelet_channels=12):\n",
    "        super().__init__()\n",
    "\n",
    "        def conv_block(in_ch, out_ch):\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            for m in block.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            return block\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "\n",
    "        self.wavelet_enc1 = conv_block(wavelet_channels, 64)\n",
    "        self.wavelet_enc2 = conv_block(64, 128)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dec3 = conv_block(512 + 256 + 128, 256)\n",
    "        self.dec2 = conv_block(256 + 128 + 64, 128)\n",
    "        self.dec1 = conv_block(128 + 64, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.final.weight, mode='fan_out', nonlinearity='linear')\n",
    "        if self.final.bias is not None:\n",
    "            nn.init.constant_(self.final.bias, 0)\n",
    "\n",
    "    def forward(self, x, wavelet):\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool(e1)\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool(e2)\n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool(e3)\n",
    "        e4 = self.enc4(p3)\n",
    "\n",
    "        w_feat2 = self.wavelet_enc1(wavelet)\n",
    "        pw_feat2 = self.pool(w_feat2)\n",
    "        w_feat3 = self.wavelet_enc2(pw_feat2)\n",
    "\n",
    "        up3 = self.up(e4)\n",
    "        cat3 = torch.cat([up3, e3, w_feat3], dim=1)\n",
    "        d3 = self.dec3(cat3)\n",
    "\n",
    "        up2 = self.up(d3)\n",
    "        cat2 = torch.cat([up2, e2, w_feat2], dim=1)\n",
    "        d2 = self.dec2(cat2)\n",
    "\n",
    "        up1 = self.up(d2)\n",
    "        cat1 = torch.cat([up1, e1], dim=1)\n",
    "        d1 = self.dec1(cat1)\n",
    "\n",
    "        out = self.final(d1)\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Combined Loss with Differentiable DWT (Fixed)\n",
    "class CombinedLossDWT(nn.Module):\n",
    "    def __init__(self, alpha=0.85, wavelet='db1', device='cpu'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.dwt = DWTForward(J=1, wave=wavelet, mode='symmetric').to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def _compute_dwt(self, x):\n",
    "        x = x.to(self.device)\n",
    "        LL, H_coeffs = self.dwt(x)\n",
    "\n",
    "        # The H_coeffs[0] has shape [B, C, 3, H, W] where 3 is for the three detail coefficients\n",
    "        # We need to reshape it to match LL's dimensions for concatenation\n",
    "        detail_coeffs = H_coeffs[0]  # Shape: [B, C, 3, H, W]\n",
    "        B, C, _, H, W = detail_coeffs.shape\n",
    "\n",
    "        # Reshape and concatenate the detail coefficients along the channel dimension\n",
    "        # Option 1: Flatten the 3 detail coefficient types into the channel dimension\n",
    "        detail_coeffs_reshaped = detail_coeffs.reshape(B, C*3, H, W)\n",
    "\n",
    "        # Concatenate the approximation (LL) and detail coefficients\n",
    "        coeffs_combined = torch.cat([LL, detail_coeffs_reshaped], dim=1)\n",
    "\n",
    "        return coeffs_combined\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred, target = pred.float(), target.float()\n",
    "        spatial_loss = self.l1_loss(pred, target)\n",
    "        pred_wavelet = self._compute_dwt(pred)\n",
    "        target_wavelet = self._compute_dwt(target)\n",
    "        freq_loss = self.l1_loss(pred_wavelet, target_wavelet)\n",
    "        total_loss = self.alpha * spatial_loss + (1 - self.alpha) * freq_loss\n",
    "\n",
    "        if torch.isnan(total_loss):\n",
    "            print(f\"NaN loss! Spatial: {spatial_loss.item()}, Freq: {freq_loss.item()}\")\n",
    "            return torch.tensor(1000.0, device=self.device, requires_grad=True)\n",
    "        return total_loss\n",
    "\n",
    "# Helper function for wavelet input\n",
    "def get_wavelet_input(img_tensor, wavelet='db1', device='cpu'):\n",
    "    img_np = img_tensor.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    batch_wavelets = []\n",
    "    for i in range(img_np.shape[0]):\n",
    "        coeffs = pywt.dwt2(img_np[i], wavelet, mode='symmetric', axes=(-3, -2))\n",
    "        cA, (cH, cV, cD) = coeffs\n",
    "        wavelet_np = np.concatenate([cA, cH, cV, cD], axis=2).astype(np.float32)\n",
    "        for ch in range(wavelet_np.shape[2]):\n",
    "            wavelet_np[:, :, ch] = (wavelet_np[:, :, ch] - wavelet_np[:, :, ch].mean()) / (wavelet_np[:, :, ch].std() + 1e-8)\n",
    "        batch_wavelets.append(wavelet_np)\n",
    "\n",
    "    wavelet_tensor = torch.from_numpy(np.stack(batch_wavelets)).permute(0, 3, 1, 2).float().to(device)\n",
    "    return wavelet_tensor\n",
    "\n",
    "# Load Test Image Pair\n",
    "def load_test_image_pair(clear_dir, blurry_dir, idx=0, device='cpu'):\n",
    "    temp_dataset = ColonoscopyDataset(clear_dir, blurry_dir)\n",
    "    effective_idx = min(idx, len(temp_dataset) - 1)\n",
    "    if idx != effective_idx:\n",
    "        print(f\"Adjusted test idx from {idx} to {effective_idx}\")\n",
    "    blurry_img_tensor, clear_img_tensor = temp_dataset[effective_idx]\n",
    "    wavelet_input = get_wavelet_input(blurry_img_tensor.unsqueeze(0), device=device)\n",
    "    return blurry_img_tensor.unsqueeze(0).to(device), wavelet_input, clear_img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "# Training Function\n",
    "def train_model(clear_dir, blurry_dir, output_dir, epochs=50, batch_size=4, lr=0.0005,\n",
    "                wavelet='db1', alpha=0.85, weight_decay=1e-5, max_samples_train=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = WaveletUNet_BN().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = CombinedLossDWT(alpha=alpha, wavelet=wavelet, device=device)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    try:\n",
    "        dataset = ColonoscopyDataset(clear_dir, blurry_dir, wavelet=wavelet, max_samples=max_samples_train)\n",
    "        if len(dataset) == 0:\n",
    "            print(\"ERROR: Dataset is empty!\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    num_w = min(4, os.cpu_count() // 2 if os.cpu_count() > 1 else 1)\n",
    "    print(f\"Using {num_w} dataloader workers.\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_w,\n",
    "                            pin_memory=True if device.type == 'cuda' else False, drop_last=True)\n",
    "\n",
    "    samples_dir = os.path.join(output_dir, \"samples\")\n",
    "    checkpoints_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "    os.makedirs(samples_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "    test_idx = min(5, len(dataset) - 1)\n",
    "    print(f\"Loading test image pair with index: {test_idx}\")\n",
    "    try:\n",
    "        test_blurry, test_wavelet_input, test_clear = load_test_image_pair(clear_dir, blurry_dir, idx=test_idx, device=device)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading test image: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Training dataset size: {len(dataset)}\")\n",
    "    print(f\"Test image shapes: Blurry={test_blurry.shape}, WaveletInput={test_wavelet_input.shape}, Clear={test_clear.shape}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_pred = model(test_blurry, test_wavelet_input)\n",
    "        initial_pred_np = initial_pred.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "        initial_pred_img = (np.clip(initial_pred_np, 0, 1) * 255).astype(np.uint8)\n",
    "        initial_pred_img = cv2.cvtColor(initial_pred_img, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(os.path.join(samples_dir, \"initial_pred_test.png\"), initial_pred_img)\n",
    "        print(\"Saved initial prediction sample.\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
    "        for i, batch in enumerate(progress_bar):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            try:\n",
    "                blurry_img, clear_img = batch\n",
    "            except Exception as e:\n",
    "                print(f\"Error unpacking batch {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "            blurry_img = blurry_img.to(device)\n",
    "            clear_img = clear_img.to(device)\n",
    "\n",
    "            if not isinstance(blurry_img, torch.Tensor) or not isinstance(clear_img, torch.Tensor):\n",
    "                continue\n",
    "            if blurry_img.ndim != 4 or clear_img.ndim != 4:\n",
    "                continue\n",
    "\n",
    "            diff = torch.mean(torch.abs(blurry_img - clear_img)).item()\n",
    "            # print(f\"Batch {i}: Mean abs diff between blurry and clear: {diff:.4f}\")\n",
    "\n",
    "            wavelet_input = get_wavelet_input(blurry_img, wavelet=wavelet, device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(blurry_img, wavelet_input)\n",
    "            # print(f\"Epoch {epoch+1}, Batch {i}: pred range [{pred.min():.3f}, {pred.max():.3f}]\")\n",
    "\n",
    "            loss = criterion(pred, clear_img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            running_loss += current_loss\n",
    "            progress_bar.set_postfix(loss=f\"{current_loss:.6f}\", avg_loss=f\"{running_loss/(i+1):.6f}\", lr=f\"{optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader) if len(dataloader) > 0 else 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs} finished. Average Training Loss: {avg_loss:.6f}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == epochs - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_test = model(test_blurry, test_wavelet_input)\n",
    "                pred_test_np = pred_test.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "                pred_test_img = (np.clip(pred_test_np, 0, 1) * 255).astype(np.uint8)\n",
    "                pred_test_img = cv2.cvtColor(pred_test_img, cv2.COLOR_RGB2BGR)\n",
    "                sample_path = os.path.join(samples_dir, f\"pred_epoch_{epoch+1:03d}.png\")\n",
    "                cv2.imwrite(sample_path, pred_test_img)\n",
    "                print(f\"Saved prediction sample to {sample_path}\")\n",
    "\n",
    "                clear_test_np = test_clear.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "                clear_test_img = (np.clip(clear_test_np, 0, 1) * 255).astype(np.uint8)\n",
    "                clear_test_img = cv2.cvtColor(clear_test_img, cv2.COLOR_RGB2BGR)\n",
    "                blurry_test_np = test_blurry.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "                blurry_test_img = (np.clip(blurry_test_np, 0, 1) * 255).astype(np.uint8)\n",
    "                blurry_test_img = cv2.cvtColor(blurry_test_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                h, w = pred_test_img.shape[:2]\n",
    "                if blurry_test_img.shape[:2] != (h, w):\n",
    "                    blurry_test_img = cv2.resize(blurry_test_img, (w, h))\n",
    "                if clear_test_img.shape[:2] != (h, w):\n",
    "                    clear_test_img = cv2.resize(clear_test_img, (w, h))\n",
    "\n",
    "                comparison_img = np.concatenate((blurry_test_img, pred_test_img, clear_test_img), axis=1)\n",
    "                comp_path = os.path.join(samples_dir, f\"comparison_epoch_{epoch+1:03d}.png\")\n",
    "                cv2.imwrite(comp_path, comparison_img)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            checkpoint_path = os.path.join(checkpoints_dir, f\"wavelet_unet_bn_epoch_{epoch+1:03d}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    final_model_path = os.path.join(output_dir, \"wavelet_unet_bn_final.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Saved final model to {final_model_path}\")\n",
    "    return model\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    clear_dir = \"output_3_18/smoke/clear\"\n",
    "    blurry_dir = \"output_3_18/smoke/blurry\"\n",
    "    output_dir = \"output_3_18/smoke/output_wavelet_bn_diffdwt_1k_limit_v2\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Clear image directory: {os.path.abspath(clear_dir)}\")\n",
    "    print(f\"Blurry image directory: {os.path.abspath(blurry_dir)}\")\n",
    "    print(f\"Output directory: {os.path.abspath(output_dir)}\")\n",
    "\n",
    "    if not os.path.isdir(clear_dir) or not os.path.isdir(blurry_dir):\n",
    "        print(f\"Error: Directory not found - Clear: {clear_dir}, Blurry: {blurry_dir}\")\n",
    "    else:\n",
    "        try:\n",
    "            model = train_model(\n",
    "                clear_dir, blurry_dir, output_dir,\n",
    "                epochs=50,\n",
    "                batch_size=4,\n",
    "                lr=0.0005,\n",
    "                wavelet='db1',\n",
    "                alpha=0.85,\n",
    "                weight_decay=1e-5,\n",
    "                max_samples_train=1000\n",
    "            )\n",
    "            if model:\n",
    "                print(\"Training process completed.\")\n",
    "            else:\n",
    "                print(\"Training process failed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdda3ca-b933-43bc-afea-925382f5b520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
